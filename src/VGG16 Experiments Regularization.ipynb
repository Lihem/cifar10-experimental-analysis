{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and Biases related imports\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (50000, 32, 32, 3)\n",
      "Train Labels Shape: (50000, 10)\n",
      "Test Data Shape: (10000, 32, 32, 3)\n",
      "Test Labels Shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def load_cifar10_batch(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        batch = pickle.load(file, encoding='bytes')\n",
    "    return batch\n",
    "\n",
    "def load_cifar10_data(folder_path):\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        batch_file = f\"{folder_path}/data_batch_{i}\"\n",
    "        batch = load_cifar10_batch(batch_file)\n",
    "        train_data.append(batch[b'data'])\n",
    "        train_labels.extend(batch[b'labels'])\n",
    "\n",
    "    test_batch_file = f\"{folder_path}/test_batch\"\n",
    "    test_batch = load_cifar10_batch(test_batch_file)\n",
    "    test_data = test_batch[b'data']\n",
    "    test_labels = test_batch[b'labels']\n",
    "\n",
    "    train_data = np.vstack(train_data)\n",
    "    train_labels = np.array(train_labels)\n",
    "    test_labels = np.array(test_labels)\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "def preprocess_data(train_data, train_labels, test_data, test_labels):\n",
    "    train_data = train_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "    test_data = test_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "\n",
    "    train_labels_onehot = to_categorical(train_labels)\n",
    "    test_labels_onehot = to_categorical(test_labels)\n",
    "\n",
    "    return train_data, train_labels_onehot, test_data, test_labels_onehot\n",
    "\n",
    "cifar10_folder = 'cifar-10-batches-py'\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_cifar10_data(cifar10_folder)\n",
    "\n",
    "x_train, y_train, x_test, y_test = preprocess_data(\n",
    "    train_data, train_labels, test_data, test_labels\n",
    ")\n",
    "\n",
    "print(\"Train Data Shape:\", x_train.shape)\n",
    "print(\"Train Labels Shape:\", y_train.shape)\n",
    "print(\"Test Data Shape:\", x_test.shape)\n",
    "print(\"Test Labels Shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlihem\u001b[0m (\u001b[33mtakim\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.environ['WANDB_NOTEBOOK_NAME'] = 'RUN_1'\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'val_loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    'reg_alpha': {\n",
    "        'values': [0] + np.logspace(-5, -2, num=7).tolist()\n",
    "        },\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "parameters_dict.update({\n",
    "    'earlystopping_patience': {\n",
    "        'value': 10},\n",
    "    'epochs': {\n",
    "        'value': 100},\n",
    "    'learning_rate': {\n",
    "        'value': 0.00025118864\n",
    "        },\n",
    "    'batch_size': {\n",
    "          'value': 64\n",
    "        },\n",
    "    'kernel_size': {\n",
    "        'value': (3, 3)\n",
    "        },\n",
    "    'dropout': {\n",
    "          'value': True\n",
    "        },\n",
    "    'pooling': {\n",
    "          'value': 'max'\n",
    "        },\n",
    "    'batchnorm': {\n",
    "          'value': True\n",
    "        },\n",
    "    'a_layers': {\n",
    "          'value': 16\n",
    "        },\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'grid',\n",
      " 'metric': {'goal': 'minimize', 'name': 'val_loss'},\n",
      " 'parameters': {'a_layers': {'value': 16},\n",
      "                'batch_size': {'value': 64},\n",
      "                'batchnorm': {'value': True},\n",
      "                'dropout': {'value': True},\n",
      "                'earlystopping_patience': {'value': 10},\n",
      "                'epochs': {'value': 100},\n",
      "                'kernel_size': {'value': (3, 3)},\n",
      "                'learning_rate': {'value': 0.00025118864},\n",
      "                'pooling': {'value': 'max'},\n",
      "                'reg_alpha': {'values': [0,\n",
      "                                         1e-05,\n",
      "                                         3.1622776601683795e-05,\n",
      "                                         0.0001,\n",
      "                                         0.00031622776601683794,\n",
      "                                         0.001,\n",
      "                                         0.0031622776601683794,\n",
      "                                         0.01]}}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: nj765c0f\n",
      "Sweep URL: https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"CIFAR-10_Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_model(kernel_size, dropout, pooling, batchnorm, n_layers, reg_alpha):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size, activation='relu', padding='same', input_shape=(32, 32, 3), kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(256, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(256, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(256, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    if n_layers == 19:\n",
    "        model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "        model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "        model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "        if batchnorm:\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=4096, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if dropout:\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=4096, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if dropout:\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def train(config = None):\n",
    "    with wandb.init(config=config):\n",
    "\n",
    "        config = wandb.config\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        model = create_model(config[\"kernel_size\"], config[\"dropout\"], config[\"pooling\"], config[\"batchnorm\"], config[\"a_layers\"], config[\"reg_alpha\"])\n",
    "        model.compile(\n",
    "            optimizer = Adam(learning_rate=config[\"learning_rate\"]),\n",
    "            loss = \"categorical_crossentropy\",\n",
    "            metrics = [\"accuracy\", tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top@3_accuracy')]\n",
    "        )\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                    patience=config[\"earlystopping_patience\"],\n",
    "                                    restore_best_weights=True)\n",
    "\n",
    "        history = model.fit(x_train, y_train,\n",
    "                                    epochs=config[\"epochs\"],\n",
    "                                    batch_size=config[\"batch_size\"],\n",
    "                                    validation_split=0.1,\n",
    "                                    callbacks=[\n",
    "                                        WandbMetricsLogger(log_freq='epoch'),\n",
    "                                        early_stopping\n",
    "                                    ], verbose=1\n",
    "                                    )\n",
    "        \n",
    "        test_stats = model.evaluate(x_test, y_test)\n",
    "        wandb.log({\"test_loss\": test_stats[0]})\n",
    "        wandb.log({\"test_acc\": test_stats[1]})\n",
    "\n",
    "        val_loss_history = history.history['val_loss']\n",
    "        val_acc_history = history.history['val_accuracy']\n",
    "\n",
    "        best_epoch_num = -1 if (len(val_loss_history) == 100 or len(val_loss_history) <= 10) else (len(val_loss_history) - 11)\n",
    "\n",
    "        wandb.log({\"best_val_loss\": val_loss_history[best_epoch_num]})\n",
    "        wandb.log({\"best_val_acc\": val_acc_history[best_epoch_num]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: miggc7tq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ta_layers: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearlystopping_patience: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: [3, 3]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00025118864\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpooling: max\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treg_alpha: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\NEURAL PROJE\\wandb\\run-20240102_132918-miggc7tq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/miggc7tq' target=\"_blank\">pleasant-sweep-1</a></strong> to <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/miggc7tq' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/miggc7tq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  5/704 [..............................] - ETA: 22s - loss: 3.0088 - accuracy: 0.1187 - top@3_accuracy: 0.3063WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0126s vs `on_train_batch_end` time: 0.0170s). Check your callbacks.\n",
      "704/704 [==============================] - 27s 34ms/step - loss: 1.5844 - accuracy: 0.4291 - top@3_accuracy: 0.7637 - val_loss: 1.4624 - val_accuracy: 0.4896 - val_top@3_accuracy: 0.8248\n",
      "Epoch 2/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 1.0759 - accuracy: 0.6273 - top@3_accuracy: 0.8837 - val_loss: 0.9832 - val_accuracy: 0.6568 - val_top@3_accuracy: 0.9074\n",
      "Epoch 3/100\n",
      "704/704 [==============================] - 22s 32ms/step - loss: 0.8354 - accuracy: 0.7210 - top@3_accuracy: 0.9233 - val_loss: 0.9616 - val_accuracy: 0.6794 - val_top@3_accuracy: 0.8876\n",
      "Epoch 4/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.6712 - accuracy: 0.7781 - top@3_accuracy: 0.9449 - val_loss: 0.6988 - val_accuracy: 0.7608 - val_top@3_accuracy: 0.9404\n",
      "Epoch 5/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.5319 - accuracy: 0.8264 - top@3_accuracy: 0.9609 - val_loss: 0.7996 - val_accuracy: 0.7396 - val_top@3_accuracy: 0.9250\n",
      "Epoch 6/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.4725 - accuracy: 0.8485 - top@3_accuracy: 0.9677 - val_loss: 0.6998 - val_accuracy: 0.7882 - val_top@3_accuracy: 0.9400\n",
      "Epoch 7/100\n",
      "704/704 [==============================] - 21s 30ms/step - loss: 0.3787 - accuracy: 0.8786 - top@3_accuracy: 0.9773 - val_loss: 0.7885 - val_accuracy: 0.7718 - val_top@3_accuracy: 0.9458\n",
      "Epoch 8/100\n",
      "704/704 [==============================] - 21s 30ms/step - loss: 0.3297 - accuracy: 0.8965 - top@3_accuracy: 0.9828 - val_loss: 0.7663 - val_accuracy: 0.7680 - val_top@3_accuracy: 0.9364\n",
      "Epoch 9/100\n",
      "704/704 [==============================] - 21s 30ms/step - loss: 0.3085 - accuracy: 0.9038 - top@3_accuracy: 0.9847 - val_loss: 0.8278 - val_accuracy: 0.7872 - val_top@3_accuracy: 0.9402\n",
      "Epoch 10/100\n",
      "704/704 [==============================] - 21s 30ms/step - loss: 0.2488 - accuracy: 0.9228 - top@3_accuracy: 0.9892 - val_loss: 0.6776 - val_accuracy: 0.8102 - val_top@3_accuracy: 0.9492\n",
      "Epoch 11/100\n",
      "704/704 [==============================] - 21s 30ms/step - loss: 0.1893 - accuracy: 0.9408 - top@3_accuracy: 0.9932 - val_loss: 0.7419 - val_accuracy: 0.8112 - val_top@3_accuracy: 0.9518\n",
      "Epoch 12/100\n",
      "704/704 [==============================] - 21s 30ms/step - loss: 0.2647 - accuracy: 0.9230 - top@3_accuracy: 0.9870 - val_loss: 1.0422 - val_accuracy: 0.6592 - val_top@3_accuracy: 0.8760\n",
      "Epoch 13/100\n",
      "704/704 [==============================] - 21s 30ms/step - loss: 0.1826 - accuracy: 0.9413 - top@3_accuracy: 0.9936 - val_loss: 0.6834 - val_accuracy: 0.8264 - val_top@3_accuracy: 0.9578\n",
      "Epoch 14/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.1218 - accuracy: 0.9620 - top@3_accuracy: 0.9969 - val_loss: 0.7614 - val_accuracy: 0.8198 - val_top@3_accuracy: 0.9538\n",
      "Epoch 15/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.1004 - accuracy: 0.9698 - top@3_accuracy: 0.9977 - val_loss: 0.7393 - val_accuracy: 0.8296 - val_top@3_accuracy: 0.9524\n",
      "Epoch 16/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.0717 - accuracy: 0.9779 - top@3_accuracy: 0.9986 - val_loss: 0.8851 - val_accuracy: 0.7978 - val_top@3_accuracy: 0.9424\n",
      "Epoch 17/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.2273 - accuracy: 0.9351 - top@3_accuracy: 0.9912 - val_loss: 0.7245 - val_accuracy: 0.8084 - val_top@3_accuracy: 0.9456\n",
      "Epoch 18/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.0803 - accuracy: 0.9746 - top@3_accuracy: 0.9982 - val_loss: 0.9058 - val_accuracy: 0.8210 - val_top@3_accuracy: 0.9542\n",
      "Epoch 19/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.2525 - accuracy: 0.9396 - top@3_accuracy: 0.9900 - val_loss: 0.8187 - val_accuracy: 0.7248 - val_top@3_accuracy: 0.9314\n",
      "Epoch 20/100\n",
      "704/704 [==============================] - 21s 30ms/step - loss: 0.1081 - accuracy: 0.9669 - top@3_accuracy: 0.9977 - val_loss: 0.7605 - val_accuracy: 0.8320 - val_top@3_accuracy: 0.9580\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.7005 - accuracy: 0.8059 - top@3_accuracy: 0.9489\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62da5cce3d04cac8c7b4bd5cb3c24a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁▄▅▅▆▆▇▇▇▇█▇████▇███</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▆▅▄▃▃▂▂▂▂▂▂▂▁▁▁▂▁▂▁</td></tr><tr><td>epoch/top@3_accuracy</td><td>▁▅▆▆▇▇▇█████████████</td></tr><tr><td>epoch/val_accuracy</td><td>▁▄▅▇▆▇▇▇▇██▄███▇██▆█</td></tr><tr><td>epoch/val_loss</td><td>█▄▄▁▂▁▂▂▂▁▂▄▁▂▂▃▁▃▂▂</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>▁▅▄▇▆▇▇▇▇██▄███▇▇█▇█</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.8102</td></tr><tr><td>best_val_loss</td><td>0.67756</td></tr><tr><td>epoch/accuracy</td><td>0.96689</td></tr><tr><td>epoch/epoch</td><td>19</td></tr><tr><td>epoch/learning_rate</td><td>0.00025</td></tr><tr><td>epoch/loss</td><td>0.10813</td></tr><tr><td>epoch/top@3_accuracy</td><td>0.99771</td></tr><tr><td>epoch/val_accuracy</td><td>0.832</td></tr><tr><td>epoch/val_loss</td><td>0.76045</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>0.958</td></tr><tr><td>test_acc</td><td>0.8059</td></tr><tr><td>test_loss</td><td>0.70055</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pleasant-sweep-1</strong> at: <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/miggc7tq' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/miggc7tq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_132918-miggc7tq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: em6t41zg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ta_layers: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearlystopping_patience: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: [3, 3]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00025118864\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpooling: max\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treg_alpha: 1e-05\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\NEURAL PROJE\\wandb\\run-20240102_133650-em6t41zg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/em6t41zg' target=\"_blank\">summer-sweep-2</a></strong> to <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/em6t41zg' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/em6t41zg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  5/704 [..............................] - ETA: 23s - loss: 3.1007 - accuracy: 0.1125 - top@3_accuracy: 0.3344WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0142s vs `on_train_batch_end` time: 0.0465s). Check your callbacks.\n",
      "704/704 [==============================] - 25s 34ms/step - loss: 1.6804 - accuracy: 0.4277 - top@3_accuracy: 0.7630 - val_loss: 1.3944 - val_accuracy: 0.5268 - val_top@3_accuracy: 0.8512\n",
      "Epoch 2/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 1.1394 - accuracy: 0.6344 - top@3_accuracy: 0.8897 - val_loss: 1.1254 - val_accuracy: 0.6476 - val_top@3_accuracy: 0.9110\n",
      "Epoch 3/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.9006 - accuracy: 0.7256 - top@3_accuracy: 0.9265 - val_loss: 1.0966 - val_accuracy: 0.6804 - val_top@3_accuracy: 0.8798\n",
      "Epoch 4/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.7387 - accuracy: 0.7839 - top@3_accuracy: 0.9483 - val_loss: 0.8167 - val_accuracy: 0.7482 - val_top@3_accuracy: 0.9370\n",
      "Epoch 5/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.6153 - accuracy: 0.8276 - top@3_accuracy: 0.9638 - val_loss: 0.7837 - val_accuracy: 0.7728 - val_top@3_accuracy: 0.9376\n",
      "Epoch 6/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.5339 - accuracy: 0.8551 - top@3_accuracy: 0.9702 - val_loss: 0.6877 - val_accuracy: 0.8024 - val_top@3_accuracy: 0.9566\n",
      "Epoch 7/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.4575 - accuracy: 0.8804 - top@3_accuracy: 0.9775 - val_loss: 0.9069 - val_accuracy: 0.7660 - val_top@3_accuracy: 0.9482\n",
      "Epoch 8/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.3829 - accuracy: 0.9040 - top@3_accuracy: 0.9839 - val_loss: 0.8198 - val_accuracy: 0.7906 - val_top@3_accuracy: 0.9458\n",
      "Epoch 9/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.3271 - accuracy: 0.9233 - top@3_accuracy: 0.9887 - val_loss: 0.7633 - val_accuracy: 0.8110 - val_top@3_accuracy: 0.9510\n",
      "Epoch 10/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.3358 - accuracy: 0.9255 - top@3_accuracy: 0.9894 - val_loss: 0.8610 - val_accuracy: 0.7692 - val_top@3_accuracy: 0.9248\n",
      "Epoch 11/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.3513 - accuracy: 0.9244 - top@3_accuracy: 0.9884 - val_loss: 0.9321 - val_accuracy: 0.7538 - val_top@3_accuracy: 0.9158\n",
      "Epoch 12/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.2535 - accuracy: 0.9477 - top@3_accuracy: 0.9938 - val_loss: 0.7658 - val_accuracy: 0.8312 - val_top@3_accuracy: 0.9600\n",
      "Epoch 13/100\n",
      "704/704 [==============================] - 24s 35ms/step - loss: 0.2018 - accuracy: 0.9647 - top@3_accuracy: 0.9960 - val_loss: 0.7778 - val_accuracy: 0.8268 - val_top@3_accuracy: 0.9584\n",
      "Epoch 14/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.1943 - accuracy: 0.9670 - top@3_accuracy: 0.9969 - val_loss: 0.8777 - val_accuracy: 0.8094 - val_top@3_accuracy: 0.9478\n",
      "Epoch 15/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.1777 - accuracy: 0.9708 - top@3_accuracy: 0.9976 - val_loss: 0.9278 - val_accuracy: 0.7998 - val_top@3_accuracy: 0.9458\n",
      "Epoch 16/100\n",
      "704/704 [==============================] - 24s 35ms/step - loss: 0.1728 - accuracy: 0.9726 - top@3_accuracy: 0.9979 - val_loss: 0.8169 - val_accuracy: 0.8206 - val_top@3_accuracy: 0.9534\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.7161 - accuracy: 0.7903 - top@3_accuracy: 0.9521\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3bdebcdac14c3bb5434eaf2d26d312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁▄▅▆▆▆▇▇▇▇▇█████</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▅▄▄▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>epoch/top@3_accuracy</td><td>▁▅▆▇▇▇▇█████████</td></tr><tr><td>epoch/val_accuracy</td><td>▁▄▅▆▇▇▇▇█▇▆██▇▇█</td></tr><tr><td>epoch/val_loss</td><td>█▅▅▂▂▁▃▂▂▃▃▂▂▃▃▂</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>▁▅▃▇▇█▇▇▇▆▅██▇▇█</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.8024</td></tr><tr><td>best_val_loss</td><td>0.68766</td></tr><tr><td>epoch/accuracy</td><td>0.97258</td></tr><tr><td>epoch/epoch</td><td>15</td></tr><tr><td>epoch/learning_rate</td><td>0.00025</td></tr><tr><td>epoch/loss</td><td>0.17275</td></tr><tr><td>epoch/top@3_accuracy</td><td>0.99789</td></tr><tr><td>epoch/val_accuracy</td><td>0.8206</td></tr><tr><td>epoch/val_loss</td><td>0.81689</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>0.9534</td></tr><tr><td>test_acc</td><td>0.7903</td></tr><tr><td>test_loss</td><td>0.71609</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">summer-sweep-2</strong> at: <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/em6t41zg' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/em6t41zg</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_133650-em6t41zg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nr86om0a with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ta_layers: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearlystopping_patience: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: [3, 3]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00025118864\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpooling: max\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treg_alpha: 3.1622776601683795e-05\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\NEURAL PROJE\\wandb\\run-20240102_134335-nr86om0a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/nr86om0a' target=\"_blank\">twilight-sweep-3</a></strong> to <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/nr86om0a' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/nr86om0a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  5/704 [..............................] - ETA: 24s - loss: 3.4629 - accuracy: 0.1219 - top@3_accuracy: 0.3562WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0148s vs `on_train_batch_end` time: 0.0193s). Check your callbacks.\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 1.8758 - accuracy: 0.4252 - top@3_accuracy: 0.7574 - val_loss: 2.1122 - val_accuracy: 0.4066 - val_top@3_accuracy: 0.6904\n",
      "Epoch 2/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 1.3062 - accuracy: 0.6354 - top@3_accuracy: 0.8870 - val_loss: 1.1806 - val_accuracy: 0.6758 - val_top@3_accuracy: 0.9056\n",
      "Epoch 3/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 1.0607 - accuracy: 0.7251 - top@3_accuracy: 0.9241 - val_loss: 1.2669 - val_accuracy: 0.6572 - val_top@3_accuracy: 0.8772\n",
      "Epoch 4/100\n",
      "704/704 [==============================] - 24s 35ms/step - loss: 0.8837 - accuracy: 0.7828 - top@3_accuracy: 0.9464 - val_loss: 0.9738 - val_accuracy: 0.7532 - val_top@3_accuracy: 0.9338\n",
      "Epoch 5/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.7556 - accuracy: 0.8221 - top@3_accuracy: 0.9592 - val_loss: 0.9494 - val_accuracy: 0.7660 - val_top@3_accuracy: 0.9332\n",
      "Epoch 6/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.6636 - accuracy: 0.8513 - top@3_accuracy: 0.9683 - val_loss: 0.8774 - val_accuracy: 0.7838 - val_top@3_accuracy: 0.9442\n",
      "Epoch 7/100\n",
      "704/704 [==============================] - 24s 35ms/step - loss: 0.5734 - accuracy: 0.8775 - top@3_accuracy: 0.9758 - val_loss: 0.8619 - val_accuracy: 0.7924 - val_top@3_accuracy: 0.9438\n",
      "Epoch 8/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.4720 - accuracy: 0.9059 - top@3_accuracy: 0.9849 - val_loss: 0.7735 - val_accuracy: 0.8236 - val_top@3_accuracy: 0.9520\n",
      "Epoch 9/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.5235 - accuracy: 0.8998 - top@3_accuracy: 0.9822 - val_loss: 2.8116 - val_accuracy: 0.3516 - val_top@3_accuracy: 0.5656\n",
      "Epoch 10/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.5202 - accuracy: 0.8965 - top@3_accuracy: 0.9809 - val_loss: 0.8520 - val_accuracy: 0.8026 - val_top@3_accuracy: 0.9506\n",
      "Epoch 11/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.3612 - accuracy: 0.9438 - top@3_accuracy: 0.9923 - val_loss: 1.0196 - val_accuracy: 0.7884 - val_top@3_accuracy: 0.9378\n",
      "Epoch 12/100\n",
      "704/704 [==============================] - 24s 33ms/step - loss: 0.5593 - accuracy: 0.8900 - top@3_accuracy: 0.9798 - val_loss: 1.0600 - val_accuracy: 0.7594 - val_top@3_accuracy: 0.9230\n",
      "Epoch 13/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.3421 - accuracy: 0.9494 - top@3_accuracy: 0.9942 - val_loss: 0.8972 - val_accuracy: 0.8138 - val_top@3_accuracy: 0.9498\n",
      "Epoch 14/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.2749 - accuracy: 0.9693 - top@3_accuracy: 0.9969 - val_loss: 1.1194 - val_accuracy: 0.7992 - val_top@3_accuracy: 0.9452\n",
      "Epoch 15/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.3406 - accuracy: 0.9517 - top@3_accuracy: 0.9930 - val_loss: 1.3951 - val_accuracy: 0.7376 - val_top@3_accuracy: 0.9054\n",
      "Epoch 16/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.3773 - accuracy: 0.9439 - top@3_accuracy: 0.9926 - val_loss: 0.9212 - val_accuracy: 0.8210 - val_top@3_accuracy: 0.9574\n",
      "Epoch 17/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.2535 - accuracy: 0.9768 - top@3_accuracy: 0.9984 - val_loss: 0.9383 - val_accuracy: 0.8178 - val_top@3_accuracy: 0.9528\n",
      "Epoch 18/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.2670 - accuracy: 0.9722 - top@3_accuracy: 0.9975 - val_loss: 1.0084 - val_accuracy: 0.8102 - val_top@3_accuracy: 0.9498\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.8032 - accuracy: 0.8151 - top@3_accuracy: 0.9544\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9aa911da6604a92a18e94b94e4ad2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁▄▅▆▆▆▇▇▇▇█▇██████</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▆▄▄▃▃▂▂▂▂▁▂▁▁▁▂▁▁</td></tr><tr><td>epoch/top@3_accuracy</td><td>▁▅▆▆▇▇▇██▇█▇██████</td></tr><tr><td>epoch/val_accuracy</td><td>▂▆▆▇▇▇██▁█▇▇██▇███</td></tr><tr><td>epoch/val_loss</td><td>▆▂▃▂▂▁▁▁█▁▂▂▁▂▃▂▂▂</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>▃▇▇█████▁██▇██▇███</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.8236</td></tr><tr><td>best_val_loss</td><td>0.77351</td></tr><tr><td>epoch/accuracy</td><td>0.9722</td></tr><tr><td>epoch/epoch</td><td>17</td></tr><tr><td>epoch/learning_rate</td><td>0.00025</td></tr><tr><td>epoch/loss</td><td>0.26698</td></tr><tr><td>epoch/top@3_accuracy</td><td>0.99753</td></tr><tr><td>epoch/val_accuracy</td><td>0.8102</td></tr><tr><td>epoch/val_loss</td><td>1.00837</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>0.9498</td></tr><tr><td>test_acc</td><td>0.8151</td></tr><tr><td>test_loss</td><td>0.80319</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">twilight-sweep-3</strong> at: <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/nr86om0a' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/nr86om0a</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_134335-nr86om0a\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: shd1gpdl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ta_layers: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearlystopping_patience: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: [3, 3]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00025118864\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpooling: max\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treg_alpha: 0.0001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\NEURAL PROJE\\wandb\\run-20240102_135101-shd1gpdl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/shd1gpdl' target=\"_blank\">swift-sweep-4</a></strong> to <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/shd1gpdl' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/shd1gpdl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  5/704 [..............................] - ETA: 23s - loss: 3.8598 - accuracy: 0.1469 - top@3_accuracy: 0.3719WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0138s vs `on_train_batch_end` time: 0.0300s). Check your callbacks.\n",
      "704/704 [==============================] - 25s 33ms/step - loss: 2.3960 - accuracy: 0.4273 - top@3_accuracy: 0.7658 - val_loss: 2.4639 - val_accuracy: 0.4214 - val_top@3_accuracy: 0.7428\n",
      "Epoch 2/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 1.7258 - accuracy: 0.6309 - top@3_accuracy: 0.8907 - val_loss: 1.5619 - val_accuracy: 0.6740 - val_top@3_accuracy: 0.9062\n",
      "Epoch 3/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 1.3849 - accuracy: 0.7232 - top@3_accuracy: 0.9255 - val_loss: 1.3721 - val_accuracy: 0.7154 - val_top@3_accuracy: 0.9092\n",
      "Epoch 4/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 1.1485 - accuracy: 0.7796 - top@3_accuracy: 0.9437 - val_loss: 1.1168 - val_accuracy: 0.7772 - val_top@3_accuracy: 0.9424\n",
      "Epoch 5/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.9755 - accuracy: 0.8157 - top@3_accuracy: 0.9566 - val_loss: 1.1161 - val_accuracy: 0.7590 - val_top@3_accuracy: 0.9424\n",
      "Epoch 6/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.8427 - accuracy: 0.8450 - top@3_accuracy: 0.9670 - val_loss: 1.0507 - val_accuracy: 0.7692 - val_top@3_accuracy: 0.9426\n",
      "Epoch 7/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.7163 - accuracy: 0.8748 - top@3_accuracy: 0.9756 - val_loss: 1.2270 - val_accuracy: 0.7442 - val_top@3_accuracy: 0.9114\n",
      "Epoch 8/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.6640 - accuracy: 0.8902 - top@3_accuracy: 0.9789 - val_loss: 0.9234 - val_accuracy: 0.8024 - val_top@3_accuracy: 0.9522\n",
      "Epoch 9/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.6041 - accuracy: 0.9059 - top@3_accuracy: 0.9834 - val_loss: 1.0268 - val_accuracy: 0.7960 - val_top@3_accuracy: 0.9438\n",
      "Epoch 10/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.5535 - accuracy: 0.9202 - top@3_accuracy: 0.9856 - val_loss: 1.0818 - val_accuracy: 0.7968 - val_top@3_accuracy: 0.9458\n",
      "Epoch 11/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.4944 - accuracy: 0.9380 - top@3_accuracy: 0.9901 - val_loss: 1.3112 - val_accuracy: 0.7446 - val_top@3_accuracy: 0.9046\n",
      "Epoch 12/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.5710 - accuracy: 0.9189 - top@3_accuracy: 0.9868 - val_loss: 1.0307 - val_accuracy: 0.8178 - val_top@3_accuracy: 0.9504\n",
      "Epoch 13/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.4591 - accuracy: 0.9493 - top@3_accuracy: 0.9936 - val_loss: 1.1459 - val_accuracy: 0.7978 - val_top@3_accuracy: 0.9422\n",
      "Epoch 14/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.5397 - accuracy: 0.9336 - top@3_accuracy: 0.9883 - val_loss: 0.9708 - val_accuracy: 0.8068 - val_top@3_accuracy: 0.9462\n",
      "Epoch 15/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.4864 - accuracy: 0.9491 - top@3_accuracy: 0.9932 - val_loss: 0.9661 - val_accuracy: 0.8338 - val_top@3_accuracy: 0.9620\n",
      "Epoch 16/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.4174 - accuracy: 0.9665 - top@3_accuracy: 0.9969 - val_loss: 1.0554 - val_accuracy: 0.8218 - val_top@3_accuracy: 0.9496\n",
      "Epoch 17/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.4554 - accuracy: 0.9575 - top@3_accuracy: 0.9943 - val_loss: 1.0763 - val_accuracy: 0.8198 - val_top@3_accuracy: 0.9492\n",
      "Epoch 18/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.4024 - accuracy: 0.9705 - top@3_accuracy: 0.9967 - val_loss: 1.1716 - val_accuracy: 0.7972 - val_top@3_accuracy: 0.9346\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.9434 - accuracy: 0.8057 - top@3_accuracy: 0.9514\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f346aaab46456ab437cae30d180fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.010 MB uploaded\\r'), FloatProgress(value=0.1111002731174405, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁▄▅▆▆▆▇▇▇▇█▇██████</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▆▄▄▃▃▂▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>epoch/top@3_accuracy</td><td>▁▅▆▆▇▇▇▇██████████</td></tr><tr><td>epoch/val_accuracy</td><td>▁▅▆▇▇▇▆▇▇▇▆█▇████▇</td></tr><tr><td>epoch/val_loss</td><td>█▄▃▂▂▂▂▁▁▂▃▁▂▁▁▂▂▂</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>▁▆▆▇▇▇▆█▇▇▆█▇▇███▇</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.8024</td></tr><tr><td>best_val_loss</td><td>0.92339</td></tr><tr><td>epoch/accuracy</td><td>0.97047</td></tr><tr><td>epoch/epoch</td><td>17</td></tr><tr><td>epoch/learning_rate</td><td>0.00025</td></tr><tr><td>epoch/loss</td><td>0.40241</td></tr><tr><td>epoch/top@3_accuracy</td><td>0.99669</td></tr><tr><td>epoch/val_accuracy</td><td>0.7972</td></tr><tr><td>epoch/val_loss</td><td>1.17161</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>0.9346</td></tr><tr><td>test_acc</td><td>0.8057</td></tr><tr><td>test_loss</td><td>0.94341</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">swift-sweep-4</strong> at: <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/shd1gpdl' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/shd1gpdl</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_135101-shd1gpdl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9fypfafo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ta_layers: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearlystopping_patience: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: [3, 3]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00025118864\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpooling: max\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treg_alpha: 0.00031622776601683794\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\NEURAL PROJE\\wandb\\run-20240102_135818-9fypfafo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/9fypfafo' target=\"_blank\">leafy-sweep-5</a></strong> to <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/9fypfafo' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/9fypfafo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  5/704 [..............................] - ETA: 23s - loss: 5.8232 - accuracy: 0.1250 - top@3_accuracy: 0.3281WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0140s vs `on_train_batch_end` time: 0.0337s). Check your callbacks.\n",
      "704/704 [==============================] - 25s 34ms/step - loss: 3.8685 - accuracy: 0.4152 - top@3_accuracy: 0.7574 - val_loss: 3.1362 - val_accuracy: 0.5138 - val_top@3_accuracy: 0.8246\n",
      "Epoch 2/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 2.5203 - accuracy: 0.6332 - top@3_accuracy: 0.8893 - val_loss: 2.2489 - val_accuracy: 0.6402 - val_top@3_accuracy: 0.8734\n",
      "Epoch 3/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 1.8465 - accuracy: 0.7184 - top@3_accuracy: 0.9214 - val_loss: 1.8259 - val_accuracy: 0.6588 - val_top@3_accuracy: 0.8954\n",
      "Epoch 4/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 1.4433 - accuracy: 0.7688 - top@3_accuracy: 0.9392 - val_loss: 1.4272 - val_accuracy: 0.7476 - val_top@3_accuracy: 0.9260\n",
      "Epoch 5/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 1.1749 - accuracy: 0.8086 - top@3_accuracy: 0.9546 - val_loss: 1.2136 - val_accuracy: 0.7768 - val_top@3_accuracy: 0.9410\n",
      "Epoch 6/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 1.0064 - accuracy: 0.8374 - top@3_accuracy: 0.9612 - val_loss: 1.1902 - val_accuracy: 0.7630 - val_top@3_accuracy: 0.9286\n",
      "Epoch 7/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.9027 - accuracy: 0.8604 - top@3_accuracy: 0.9697 - val_loss: 1.0666 - val_accuracy: 0.7992 - val_top@3_accuracy: 0.9476\n",
      "Epoch 8/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.7965 - accuracy: 0.8833 - top@3_accuracy: 0.9770 - val_loss: 1.0801 - val_accuracy: 0.7952 - val_top@3_accuracy: 0.9442\n",
      "Epoch 9/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.7707 - accuracy: 0.8932 - top@3_accuracy: 0.9787 - val_loss: 1.0627 - val_accuracy: 0.8068 - val_top@3_accuracy: 0.9468\n",
      "Epoch 10/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.7258 - accuracy: 0.9057 - top@3_accuracy: 0.9823 - val_loss: 1.0402 - val_accuracy: 0.8066 - val_top@3_accuracy: 0.9542\n",
      "Epoch 11/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.6451 - accuracy: 0.9260 - top@3_accuracy: 0.9873 - val_loss: 1.3058 - val_accuracy: 0.7734 - val_top@3_accuracy: 0.9280\n",
      "Epoch 12/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.6296 - accuracy: 0.9326 - top@3_accuracy: 0.9884 - val_loss: 1.2233 - val_accuracy: 0.7770 - val_top@3_accuracy: 0.9286\n",
      "Epoch 13/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.7143 - accuracy: 0.9225 - top@3_accuracy: 0.9853 - val_loss: 1.0408 - val_accuracy: 0.8262 - val_top@3_accuracy: 0.9562\n",
      "Epoch 14/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.5783 - accuracy: 0.9534 - top@3_accuracy: 0.9927 - val_loss: 1.2760 - val_accuracy: 0.8010 - val_top@3_accuracy: 0.9416\n",
      "Epoch 15/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.5658 - accuracy: 0.9543 - top@3_accuracy: 0.9936 - val_loss: 1.1729 - val_accuracy: 0.8092 - val_top@3_accuracy: 0.9388\n",
      "Epoch 16/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.5489 - accuracy: 0.9575 - top@3_accuracy: 0.9944 - val_loss: 1.3157 - val_accuracy: 0.7824 - val_top@3_accuracy: 0.9266\n",
      "Epoch 17/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.6428 - accuracy: 0.9383 - top@3_accuracy: 0.9888 - val_loss: 1.0506 - val_accuracy: 0.8380 - val_top@3_accuracy: 0.9574\n",
      "Epoch 18/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.5388 - accuracy: 0.9629 - top@3_accuracy: 0.9956 - val_loss: 1.1392 - val_accuracy: 0.8156 - val_top@3_accuracy: 0.9448\n",
      "Epoch 19/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.5244 - accuracy: 0.9657 - top@3_accuracy: 0.9960 - val_loss: 1.2526 - val_accuracy: 0.8080 - val_top@3_accuracy: 0.9486\n",
      "Epoch 20/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.5297 - accuracy: 0.9642 - top@3_accuracy: 0.9961 - val_loss: 1.0841 - val_accuracy: 0.8338 - val_top@3_accuracy: 0.9478\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 1.0776 - accuracy: 0.7985 - top@3_accuracy: 0.9472\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33fb5866547b4838b3d9b43058cd4e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.025 MB uploaded\\r'), FloatProgress(value=0.04341849009050292, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁▄▅▅▆▆▇▇▇▇▇█▇███████</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▅▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/top@3_accuracy</td><td>▁▅▆▆▇▇▇▇▇███████████</td></tr><tr><td>epoch/val_accuracy</td><td>▁▄▄▆▇▆▇▇▇▇▇▇█▇▇▇██▇█</td></tr><tr><td>epoch/val_loss</td><td>█▅▄▂▂▂▁▁▁▁▂▂▁▂▁▂▁▁▂▁</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>▁▄▅▆▇▆▇▇▇█▆▆█▇▇▆█▇█▇</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.8066</td></tr><tr><td>best_val_loss</td><td>1.0402</td></tr><tr><td>epoch/accuracy</td><td>0.96424</td></tr><tr><td>epoch/epoch</td><td>19</td></tr><tr><td>epoch/learning_rate</td><td>0.00025</td></tr><tr><td>epoch/loss</td><td>0.52967</td></tr><tr><td>epoch/top@3_accuracy</td><td>0.99609</td></tr><tr><td>epoch/val_accuracy</td><td>0.8338</td></tr><tr><td>epoch/val_loss</td><td>1.08414</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>0.9478</td></tr><tr><td>test_acc</td><td>0.7985</td></tr><tr><td>test_loss</td><td>1.07764</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">leafy-sweep-5</strong> at: <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/9fypfafo' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/9fypfafo</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_135818-9fypfafo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nmv1ug54 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ta_layers: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearlystopping_patience: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: [3, 3]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00025118864\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpooling: max\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treg_alpha: 0.001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\NEURAL PROJE\\wandb\\run-20240102_140620-nmv1ug54</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/nmv1ug54' target=\"_blank\">fancy-sweep-6</a></strong> to <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/nmv1ug54' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/nmv1ug54</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  5/704 [..............................] - ETA: 23s - loss: 11.7763 - accuracy: 0.1344 - top@3_accuracy: 0.3031WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0142s vs `on_train_batch_end` time: 0.0190s). Check your callbacks.\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 7.1792 - accuracy: 0.3988 - top@3_accuracy: 0.7444 - val_loss: 5.0226 - val_accuracy: 0.4066 - val_top@3_accuracy: 0.7928\n",
      "Epoch 2/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 3.5365 - accuracy: 0.5986 - top@3_accuracy: 0.8724 - val_loss: 2.8744 - val_accuracy: 0.6036 - val_top@3_accuracy: 0.8858\n",
      "Epoch 3/100\n",
      "704/704 [==============================] - 25s 36ms/step - loss: 2.2161 - accuracy: 0.6938 - top@3_accuracy: 0.9118 - val_loss: 2.0788 - val_accuracy: 0.6560 - val_top@3_accuracy: 0.8772\n",
      "Epoch 4/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 1.6406 - accuracy: 0.7468 - top@3_accuracy: 0.9321 - val_loss: 1.6389 - val_accuracy: 0.7136 - val_top@3_accuracy: 0.9034\n",
      "Epoch 5/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 1.3551 - accuracy: 0.7833 - top@3_accuracy: 0.9433 - val_loss: 1.3314 - val_accuracy: 0.7766 - val_top@3_accuracy: 0.9400\n",
      "Epoch 6/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 1.1905 - accuracy: 0.8084 - top@3_accuracy: 0.9508 - val_loss: 1.3342 - val_accuracy: 0.7574 - val_top@3_accuracy: 0.9322\n",
      "Epoch 7/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 1.0572 - accuracy: 0.8369 - top@3_accuracy: 0.9613 - val_loss: 1.2957 - val_accuracy: 0.7586 - val_top@3_accuracy: 0.9306\n",
      "Epoch 8/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 1.0028 - accuracy: 0.8486 - top@3_accuracy: 0.9644 - val_loss: 1.2688 - val_accuracy: 0.7602 - val_top@3_accuracy: 0.9354\n",
      "Epoch 9/100\n",
      "704/704 [==============================] - 24s 35ms/step - loss: 0.9725 - accuracy: 0.8604 - top@3_accuracy: 0.9692 - val_loss: 1.2613 - val_accuracy: 0.7626 - val_top@3_accuracy: 0.9344\n",
      "Epoch 10/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.9038 - accuracy: 0.8781 - top@3_accuracy: 0.9732 - val_loss: 1.3914 - val_accuracy: 0.7428 - val_top@3_accuracy: 0.9094\n",
      "Epoch 11/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.8275 - accuracy: 0.8964 - top@3_accuracy: 0.9781 - val_loss: 1.0900 - val_accuracy: 0.8242 - val_top@3_accuracy: 0.9530\n",
      "Epoch 12/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.7917 - accuracy: 0.9065 - top@3_accuracy: 0.9816 - val_loss: 1.0942 - val_accuracy: 0.8176 - val_top@3_accuracy: 0.9498\n",
      "Epoch 13/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.7795 - accuracy: 0.9119 - top@3_accuracy: 0.9828 - val_loss: 1.2920 - val_accuracy: 0.7854 - val_top@3_accuracy: 0.9362\n",
      "Epoch 14/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.7364 - accuracy: 0.9222 - top@3_accuracy: 0.9857 - val_loss: 1.2440 - val_accuracy: 0.7920 - val_top@3_accuracy: 0.9402\n",
      "Epoch 15/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.7094 - accuracy: 0.9285 - top@3_accuracy: 0.9872 - val_loss: 1.1778 - val_accuracy: 0.8010 - val_top@3_accuracy: 0.9440\n",
      "Epoch 16/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.7152 - accuracy: 0.9281 - top@3_accuracy: 0.9869 - val_loss: 1.1723 - val_accuracy: 0.8024 - val_top@3_accuracy: 0.9386\n",
      "Epoch 17/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.7056 - accuracy: 0.9336 - top@3_accuracy: 0.9876 - val_loss: 1.1373 - val_accuracy: 0.8092 - val_top@3_accuracy: 0.9500\n",
      "Epoch 18/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.6757 - accuracy: 0.9412 - top@3_accuracy: 0.9901 - val_loss: 1.2934 - val_accuracy: 0.7804 - val_top@3_accuracy: 0.9380\n",
      "Epoch 19/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.6565 - accuracy: 0.9450 - top@3_accuracy: 0.9904 - val_loss: 1.2878 - val_accuracy: 0.8008 - val_top@3_accuracy: 0.9396\n",
      "Epoch 20/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.6535 - accuracy: 0.9466 - top@3_accuracy: 0.9913 - val_loss: 1.1636 - val_accuracy: 0.8062 - val_top@3_accuracy: 0.9446\n",
      "Epoch 21/100\n",
      "704/704 [==============================] - 24s 35ms/step - loss: 0.6357 - accuracy: 0.9495 - top@3_accuracy: 0.9926 - val_loss: 1.2359 - val_accuracy: 0.8028 - val_top@3_accuracy: 0.9396\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 1.1281 - accuracy: 0.8192 - top@3_accuracy: 0.9511\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f154e17455304d52aca8ea2ff1f8bf91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.010 MB uploaded\\r'), FloatProgress(value=0.10456256311392638, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁▄▅▅▆▆▇▇▇▇▇▇█████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/top@3_accuracy</td><td>▁▅▆▆▇▇▇▇▇▇███████████</td></tr><tr><td>epoch/val_accuracy</td><td>▁▄▅▆▇▇▇▇▇▇██▇▇███▇███</td></tr><tr><td>epoch/val_loss</td><td>█▄▃▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>▁▅▅▆▇▇▇▇▇▆██▇▇█▇█▇▇█▇</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.8242</td></tr><tr><td>best_val_loss</td><td>1.08996</td></tr><tr><td>epoch/accuracy</td><td>0.94951</td></tr><tr><td>epoch/epoch</td><td>20</td></tr><tr><td>epoch/learning_rate</td><td>0.00025</td></tr><tr><td>epoch/loss</td><td>0.63567</td></tr><tr><td>epoch/top@3_accuracy</td><td>0.99262</td></tr><tr><td>epoch/val_accuracy</td><td>0.8028</td></tr><tr><td>epoch/val_loss</td><td>1.23586</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>0.9396</td></tr><tr><td>test_acc</td><td>0.8192</td></tr><tr><td>test_loss</td><td>1.12811</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fancy-sweep-6</strong> at: <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/nmv1ug54' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/nmv1ug54</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_140620-nmv1ug54\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xxvils4l with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ta_layers: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearlystopping_patience: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: [3, 3]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00025118864\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpooling: max\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treg_alpha: 0.0031622776601683794\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\NEURAL PROJE\\wandb\\run-20240102_141506-xxvils4l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/xxvils4l' target=\"_blank\">giddy-sweep-7</a></strong> to <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/xxvils4l' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/xxvils4l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  5/704 [..............................] - ETA: 24s - loss: 30.7488 - accuracy: 0.0938 - top@3_accuracy: 0.3031 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0146s vs `on_train_batch_end` time: 0.0329s). Check your callbacks.\n",
      "704/704 [==============================] - 26s 35ms/step - loss: 12.5338 - accuracy: 0.4144 - top@3_accuracy: 0.7571 - val_loss: 5.4692 - val_accuracy: 0.4358 - val_top@3_accuracy: 0.7756\n",
      "Epoch 2/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 3.4524 - accuracy: 0.6014 - top@3_accuracy: 0.8734 - val_loss: 2.7890 - val_accuracy: 0.4968 - val_top@3_accuracy: 0.8226\n",
      "Epoch 3/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 2.0092 - accuracy: 0.6766 - top@3_accuracy: 0.9049 - val_loss: 1.8741 - val_accuracy: 0.6528 - val_top@3_accuracy: 0.8914\n",
      "Epoch 4/100\n",
      "704/704 [==============================] - 24s 35ms/step - loss: 1.5896 - accuracy: 0.7262 - top@3_accuracy: 0.9200 - val_loss: 1.5286 - val_accuracy: 0.7270 - val_top@3_accuracy: 0.9186\n",
      "Epoch 5/100\n",
      "704/704 [==============================] - 25s 36ms/step - loss: 1.3689 - accuracy: 0.7666 - top@3_accuracy: 0.9325 - val_loss: 1.3955 - val_accuracy: 0.7440 - val_top@3_accuracy: 0.9182\n",
      "Epoch 6/100\n",
      "704/704 [==============================] - 25s 36ms/step - loss: 1.2449 - accuracy: 0.7955 - top@3_accuracy: 0.9429 - val_loss: 1.4914 - val_accuracy: 0.7112 - val_top@3_accuracy: 0.8984\n",
      "Epoch 7/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 1.1281 - accuracy: 0.8196 - top@3_accuracy: 0.9538 - val_loss: 1.2411 - val_accuracy: 0.7816 - val_top@3_accuracy: 0.9408\n",
      "Epoch 8/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 1.0659 - accuracy: 0.8345 - top@3_accuracy: 0.9581 - val_loss: 1.4895 - val_accuracy: 0.7114 - val_top@3_accuracy: 0.9020\n",
      "Epoch 9/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 1.0021 - accuracy: 0.8494 - top@3_accuracy: 0.9618 - val_loss: 1.4245 - val_accuracy: 0.7258 - val_top@3_accuracy: 0.9146\n",
      "Epoch 10/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.9632 - accuracy: 0.8607 - top@3_accuracy: 0.9646 - val_loss: 1.3994 - val_accuracy: 0.7404 - val_top@3_accuracy: 0.9050\n",
      "Epoch 11/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.9253 - accuracy: 0.8706 - top@3_accuracy: 0.9694 - val_loss: 1.1835 - val_accuracy: 0.7942 - val_top@3_accuracy: 0.9448\n",
      "Epoch 12/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.8826 - accuracy: 0.8817 - top@3_accuracy: 0.9716 - val_loss: 1.2022 - val_accuracy: 0.7952 - val_top@3_accuracy: 0.9356\n",
      "Epoch 13/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.8503 - accuracy: 0.8888 - top@3_accuracy: 0.9748 - val_loss: 1.0989 - val_accuracy: 0.8038 - val_top@3_accuracy: 0.9492\n",
      "Epoch 14/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.8329 - accuracy: 0.8940 - top@3_accuracy: 0.9757 - val_loss: 1.3892 - val_accuracy: 0.7540 - val_top@3_accuracy: 0.9218\n",
      "Epoch 15/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.8101 - accuracy: 0.8995 - top@3_accuracy: 0.9779 - val_loss: 1.1791 - val_accuracy: 0.8050 - val_top@3_accuracy: 0.9434\n",
      "Epoch 16/100\n",
      "704/704 [==============================] - 26s 36ms/step - loss: 0.7945 - accuracy: 0.9040 - top@3_accuracy: 0.9783 - val_loss: 1.1488 - val_accuracy: 0.8038 - val_top@3_accuracy: 0.9424\n",
      "Epoch 17/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.7633 - accuracy: 0.9123 - top@3_accuracy: 0.9825 - val_loss: 1.1740 - val_accuracy: 0.8034 - val_top@3_accuracy: 0.9394\n",
      "Epoch 18/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.7668 - accuracy: 0.9118 - top@3_accuracy: 0.9814 - val_loss: 1.3924 - val_accuracy: 0.7468 - val_top@3_accuracy: 0.9140\n",
      "Epoch 19/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.7651 - accuracy: 0.9125 - top@3_accuracy: 0.9833 - val_loss: 1.2298 - val_accuracy: 0.7950 - val_top@3_accuracy: 0.9360\n",
      "Epoch 20/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.7368 - accuracy: 0.9211 - top@3_accuracy: 0.9843 - val_loss: 1.0598 - val_accuracy: 0.8310 - val_top@3_accuracy: 0.9558\n",
      "Epoch 21/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.7193 - accuracy: 0.9242 - top@3_accuracy: 0.9858 - val_loss: 1.3819 - val_accuracy: 0.7534 - val_top@3_accuracy: 0.9106\n",
      "Epoch 22/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.7321 - accuracy: 0.9221 - top@3_accuracy: 0.9845 - val_loss: 1.0699 - val_accuracy: 0.8310 - val_top@3_accuracy: 0.9514\n",
      "Epoch 23/100\n",
      "704/704 [==============================] - 24s 35ms/step - loss: 0.7063 - accuracy: 0.9292 - top@3_accuracy: 0.9866 - val_loss: 1.1957 - val_accuracy: 0.8074 - val_top@3_accuracy: 0.9378\n",
      "Epoch 24/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.7074 - accuracy: 0.9289 - top@3_accuracy: 0.9859 - val_loss: 1.2198 - val_accuracy: 0.7914 - val_top@3_accuracy: 0.9340\n",
      "Epoch 25/100\n",
      "704/704 [==============================] - 24s 35ms/step - loss: 0.6963 - accuracy: 0.9320 - top@3_accuracy: 0.9876 - val_loss: 1.2762 - val_accuracy: 0.8030 - val_top@3_accuracy: 0.9428\n",
      "Epoch 26/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.6936 - accuracy: 0.9327 - top@3_accuracy: 0.9874 - val_loss: 1.0931 - val_accuracy: 0.8266 - val_top@3_accuracy: 0.9496\n",
      "Epoch 27/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.6807 - accuracy: 0.9348 - top@3_accuracy: 0.9879 - val_loss: 1.0625 - val_accuracy: 0.8360 - val_top@3_accuracy: 0.9526\n",
      "Epoch 28/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.6938 - accuracy: 0.9318 - top@3_accuracy: 0.9877 - val_loss: 1.1557 - val_accuracy: 0.8100 - val_top@3_accuracy: 0.9514\n",
      "Epoch 29/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.6763 - accuracy: 0.9374 - top@3_accuracy: 0.9882 - val_loss: 1.0805 - val_accuracy: 0.8280 - val_top@3_accuracy: 0.9548\n",
      "Epoch 30/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.6854 - accuracy: 0.9372 - top@3_accuracy: 0.9873 - val_loss: 1.0955 - val_accuracy: 0.8256 - val_top@3_accuracy: 0.9536\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 1.1089 - accuracy: 0.8228 - top@3_accuracy: 0.9510\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62bc0202dbd4d24bb6c834c38ce4a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁▄▅▅▆▆▆▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/top@3_accuracy</td><td>▁▅▅▆▆▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>epoch/val_accuracy</td><td>▁▂▅▆▆▆▇▆▆▆▇▇▇▇▇▇▇▆▇█▇█▇▇▇█████</td></tr><tr><td>epoch/val_loss</td><td>█▄▂▂▂▂▁▂▂▂▁▁▁▂▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>▁▃▅▇▇▆▇▆▆▆█▇█▇█▇▇▆▇█▆█▇▇▇█████</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.831</td></tr><tr><td>best_val_loss</td><td>1.0598</td></tr><tr><td>epoch/accuracy</td><td>0.93722</td></tr><tr><td>epoch/epoch</td><td>29</td></tr><tr><td>epoch/learning_rate</td><td>0.00025</td></tr><tr><td>epoch/loss</td><td>0.6854</td></tr><tr><td>epoch/top@3_accuracy</td><td>0.98731</td></tr><tr><td>epoch/val_accuracy</td><td>0.8256</td></tr><tr><td>epoch/val_loss</td><td>1.09552</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>0.9536</td></tr><tr><td>test_acc</td><td>0.8228</td></tr><tr><td>test_loss</td><td>1.10894</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">giddy-sweep-7</strong> at: <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/xxvils4l' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/xxvils4l</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_141506-xxvils4l\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1nkpl2t6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ta_layers: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearlystopping_patience: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: [3, 3]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00025118864\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpooling: max\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treg_alpha: 0.01\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\NEURAL PROJE\\wandb\\run-20240102_142750-1nkpl2t6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/1nkpl2t6' target=\"_blank\">sweepy-sweep-8</a></strong> to <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/nj765c0f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/1nkpl2t6' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/1nkpl2t6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  5/704 [..............................] - ETA: 23s - loss: 90.4477 - accuracy: 0.1250 - top@3_accuracy: 0.3187 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0144s vs `on_train_batch_end` time: 0.0195s). Check your callbacks.\n",
      "704/704 [==============================] - 26s 35ms/step - loss: 20.0187 - accuracy: 0.4239 - top@3_accuracy: 0.7689 - val_loss: 4.1994 - val_accuracy: 0.4656 - val_top@3_accuracy: 0.7860\n",
      "Epoch 2/100\n",
      "704/704 [==============================] - 24s 35ms/step - loss: 2.6934 - accuracy: 0.5838 - top@3_accuracy: 0.8677 - val_loss: 2.2131 - val_accuracy: 0.5434 - val_top@3_accuracy: 0.8372\n",
      "Epoch 3/100\n",
      "704/704 [==============================] - 24s 35ms/step - loss: 1.8092 - accuracy: 0.6518 - top@3_accuracy: 0.8955 - val_loss: 1.9341 - val_accuracy: 0.5594 - val_top@3_accuracy: 0.8606\n",
      "Epoch 4/100\n",
      "704/704 [==============================] - 24s 35ms/step - loss: 1.5165 - accuracy: 0.6993 - top@3_accuracy: 0.9152 - val_loss: 2.3337 - val_accuracy: 0.4758 - val_top@3_accuracy: 0.7524\n",
      "Epoch 5/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 1.3716 - accuracy: 0.7343 - top@3_accuracy: 0.9252 - val_loss: 1.7463 - val_accuracy: 0.6432 - val_top@3_accuracy: 0.8658\n",
      "Epoch 6/100\n",
      "704/704 [==============================] - 24s 35ms/step - loss: 1.2643 - accuracy: 0.7619 - top@3_accuracy: 0.9335 - val_loss: 1.5054 - val_accuracy: 0.6766 - val_top@3_accuracy: 0.9000\n",
      "Epoch 7/100\n",
      "704/704 [==============================] - 24s 35ms/step - loss: 1.1875 - accuracy: 0.7847 - top@3_accuracy: 0.9402 - val_loss: 1.5291 - val_accuracy: 0.6846 - val_top@3_accuracy: 0.9024\n",
      "Epoch 8/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 1.1393 - accuracy: 0.7990 - top@3_accuracy: 0.9437 - val_loss: 1.3491 - val_accuracy: 0.7238 - val_top@3_accuracy: 0.9170\n",
      "Epoch 9/100\n",
      "704/704 [==============================] - 24s 35ms/step - loss: 1.0848 - accuracy: 0.8157 - top@3_accuracy: 0.9488 - val_loss: 1.2722 - val_accuracy: 0.7622 - val_top@3_accuracy: 0.9244\n",
      "Epoch 10/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 1.0410 - accuracy: 0.8270 - top@3_accuracy: 0.9535 - val_loss: 1.1118 - val_accuracy: 0.7896 - val_top@3_accuracy: 0.9498\n",
      "Epoch 11/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 1.0131 - accuracy: 0.8317 - top@3_accuracy: 0.9574 - val_loss: 1.1148 - val_accuracy: 0.7992 - val_top@3_accuracy: 0.9364\n",
      "Epoch 12/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.9851 - accuracy: 0.8410 - top@3_accuracy: 0.9593 - val_loss: 1.3556 - val_accuracy: 0.7368 - val_top@3_accuracy: 0.9154\n",
      "Epoch 13/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.9513 - accuracy: 0.8511 - top@3_accuracy: 0.9636 - val_loss: 1.3597 - val_accuracy: 0.7274 - val_top@3_accuracy: 0.8986\n",
      "Epoch 14/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.9314 - accuracy: 0.8557 - top@3_accuracy: 0.9638 - val_loss: 1.1724 - val_accuracy: 0.7768 - val_top@3_accuracy: 0.9402\n",
      "Epoch 15/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.9339 - accuracy: 0.8540 - top@3_accuracy: 0.9638 - val_loss: 1.1173 - val_accuracy: 0.8022 - val_top@3_accuracy: 0.9406\n",
      "Epoch 16/100\n",
      "704/704 [==============================] - 24s 35ms/step - loss: 0.9024 - accuracy: 0.8632 - top@3_accuracy: 0.9671 - val_loss: 1.2089 - val_accuracy: 0.7650 - val_top@3_accuracy: 0.9388\n",
      "Epoch 17/100\n",
      "704/704 [==============================] - 24s 35ms/step - loss: 0.8897 - accuracy: 0.8687 - top@3_accuracy: 0.9675 - val_loss: 1.2513 - val_accuracy: 0.7724 - val_top@3_accuracy: 0.9232\n",
      "Epoch 18/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.8866 - accuracy: 0.8680 - top@3_accuracy: 0.9694 - val_loss: 1.1065 - val_accuracy: 0.8046 - val_top@3_accuracy: 0.9366\n",
      "Epoch 19/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.8714 - accuracy: 0.8743 - top@3_accuracy: 0.9695 - val_loss: 1.1482 - val_accuracy: 0.7892 - val_top@3_accuracy: 0.9330\n",
      "Epoch 20/100\n",
      "704/704 [==============================] - 24s 35ms/step - loss: 0.8580 - accuracy: 0.8766 - top@3_accuracy: 0.9707 - val_loss: 1.1172 - val_accuracy: 0.8028 - val_top@3_accuracy: 0.9396\n",
      "Epoch 21/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.8569 - accuracy: 0.8790 - top@3_accuracy: 0.9709 - val_loss: 1.0632 - val_accuracy: 0.8230 - val_top@3_accuracy: 0.9474\n",
      "Epoch 22/100\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.8358 - accuracy: 0.8828 - top@3_accuracy: 0.9733 - val_loss: 1.0450 - val_accuracy: 0.8204 - val_top@3_accuracy: 0.9476\n",
      "Epoch 23/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.8378 - accuracy: 0.8830 - top@3_accuracy: 0.9728 - val_loss: 1.2579 - val_accuracy: 0.7792 - val_top@3_accuracy: 0.9252\n",
      "Epoch 24/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.8348 - accuracy: 0.8838 - top@3_accuracy: 0.9741 - val_loss: 1.1745 - val_accuracy: 0.7922 - val_top@3_accuracy: 0.9436\n",
      "Epoch 25/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.8115 - accuracy: 0.8914 - top@3_accuracy: 0.9753 - val_loss: 1.1327 - val_accuracy: 0.8018 - val_top@3_accuracy: 0.9418\n",
      "Epoch 26/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.8258 - accuracy: 0.8878 - top@3_accuracy: 0.9737 - val_loss: 1.1742 - val_accuracy: 0.7966 - val_top@3_accuracy: 0.9352\n",
      "Epoch 27/100\n",
      "704/704 [==============================] - 26s 37ms/step - loss: 0.8118 - accuracy: 0.8920 - top@3_accuracy: 0.9752 - val_loss: 1.5614 - val_accuracy: 0.7306 - val_top@3_accuracy: 0.9056\n",
      "Epoch 28/100\n",
      "704/704 [==============================] - 26s 37ms/step - loss: 0.8123 - accuracy: 0.8930 - top@3_accuracy: 0.9757 - val_loss: 1.2386 - val_accuracy: 0.7810 - val_top@3_accuracy: 0.9396\n",
      "Epoch 29/100\n",
      "704/704 [==============================] - 26s 37ms/step - loss: 0.7956 - accuracy: 0.8964 - top@3_accuracy: 0.9781 - val_loss: 1.0436 - val_accuracy: 0.8246 - val_top@3_accuracy: 0.9548\n",
      "Epoch 30/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.7834 - accuracy: 0.8994 - top@3_accuracy: 0.9778 - val_loss: 1.0908 - val_accuracy: 0.8172 - val_top@3_accuracy: 0.9480\n",
      "Epoch 31/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.7875 - accuracy: 0.8970 - top@3_accuracy: 0.9776 - val_loss: 1.1073 - val_accuracy: 0.8062 - val_top@3_accuracy: 0.9430\n",
      "Epoch 32/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.7873 - accuracy: 0.8992 - top@3_accuracy: 0.9774 - val_loss: 1.1240 - val_accuracy: 0.8070 - val_top@3_accuracy: 0.9392\n",
      "Epoch 33/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.7770 - accuracy: 0.9015 - top@3_accuracy: 0.9788 - val_loss: 1.1045 - val_accuracy: 0.8132 - val_top@3_accuracy: 0.9416\n",
      "Epoch 34/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.7723 - accuracy: 0.9004 - top@3_accuracy: 0.9800 - val_loss: 1.2413 - val_accuracy: 0.7834 - val_top@3_accuracy: 0.9248\n",
      "Epoch 35/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.7753 - accuracy: 0.9014 - top@3_accuracy: 0.9789 - val_loss: 1.3395 - val_accuracy: 0.7648 - val_top@3_accuracy: 0.9168\n",
      "Epoch 36/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.7712 - accuracy: 0.9023 - top@3_accuracy: 0.9800 - val_loss: 1.1007 - val_accuracy: 0.8090 - val_top@3_accuracy: 0.9382\n",
      "Epoch 37/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.7581 - accuracy: 0.9055 - top@3_accuracy: 0.9808 - val_loss: 1.3537 - val_accuracy: 0.7628 - val_top@3_accuracy: 0.9130\n",
      "Epoch 38/100\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.7606 - accuracy: 0.9066 - top@3_accuracy: 0.9805 - val_loss: 1.2312 - val_accuracy: 0.7722 - val_top@3_accuracy: 0.9268\n",
      "Epoch 39/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.7543 - accuracy: 0.9079 - top@3_accuracy: 0.9812 - val_loss: 1.1798 - val_accuracy: 0.7920 - val_top@3_accuracy: 0.9380\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.0652 - accuracy: 0.8135 - top@3_accuracy: 0.9510\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c72d104513c4982a6af44a7af9553c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁▃▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/top@3_accuracy</td><td>▁▄▅▆▆▆▇▇▇▇▇▇▇▇▇████████████████████████</td></tr><tr><td>epoch/val_accuracy</td><td>▁▃▃▁▄▅▅▆▇▇█▆▆▇█▇▇█▇███▇▇█▇▆▇█████▇▇█▇▇▇</td></tr><tr><td>epoch/val_loss</td><td>█▄▃▄▃▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▂▁▂▁▁</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>▂▄▅▁▅▆▆▇▇█▇▇▆▇█▇▇▇▇▇██▇██▇▆▇███▇█▇▇▇▇▇▇</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.8246</td></tr><tr><td>best_val_loss</td><td>1.04359</td></tr><tr><td>epoch/accuracy</td><td>0.90789</td></tr><tr><td>epoch/epoch</td><td>38</td></tr><tr><td>epoch/learning_rate</td><td>0.00025</td></tr><tr><td>epoch/loss</td><td>0.75435</td></tr><tr><td>epoch/top@3_accuracy</td><td>0.98124</td></tr><tr><td>epoch/val_accuracy</td><td>0.792</td></tr><tr><td>epoch/val_loss</td><td>1.17982</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>0.938</td></tr><tr><td>test_acc</td><td>0.8135</td></tr><tr><td>test_loss</td><td>1.0652</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sweepy-sweep-8</strong> at: <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/1nkpl2t6' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/1nkpl2t6</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_142750-1nkpl2t6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
