{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and Biases related imports\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (50000, 32, 32, 3)\n",
      "Train Labels Shape: (50000, 10)\n",
      "Test Data Shape: (10000, 32, 32, 3)\n",
      "Test Labels Shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def load_cifar10_batch(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        batch = pickle.load(file, encoding='bytes')\n",
    "    return batch\n",
    "\n",
    "def load_cifar10_data(folder_path):\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        batch_file = f\"{folder_path}/data_batch_{i}\"\n",
    "        batch = load_cifar10_batch(batch_file)\n",
    "        train_data.append(batch[b'data'])\n",
    "        train_labels.extend(batch[b'labels'])\n",
    "\n",
    "    test_batch_file = f\"{folder_path}/test_batch\"\n",
    "    test_batch = load_cifar10_batch(test_batch_file)\n",
    "    test_data = test_batch[b'data']\n",
    "    test_labels = test_batch[b'labels']\n",
    "\n",
    "    train_data = np.vstack(train_data)\n",
    "    train_labels = np.array(train_labels)\n",
    "    test_labels = np.array(test_labels)\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "def preprocess_data(train_data, train_labels, test_data, test_labels):\n",
    "    train_data = train_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "    test_data = test_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "\n",
    "    train_labels_onehot = to_categorical(train_labels)\n",
    "    test_labels_onehot = to_categorical(test_labels)\n",
    "\n",
    "    return train_data, train_labels_onehot, test_data, test_labels_onehot\n",
    "\n",
    "cifar10_folder = 'cifar-10-batches-py'\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_cifar10_data(cifar10_folder)\n",
    "\n",
    "x_train, y_train, x_test, y_test = preprocess_data(\n",
    "    train_data, train_labels, test_data, test_labels\n",
    ")\n",
    "\n",
    "print(\"Train Data Shape:\", x_train.shape)\n",
    "print(\"Train Labels Shape:\", y_train.shape)\n",
    "print(\"Test Data Shape:\", x_test.shape)\n",
    "print(\"Test Labels Shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlihem\u001b[0m (\u001b[33mtakim\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.environ['WANDB_NOTEBOOK_NAME'] = 'RUN_1'\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'val_loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    'super_res': {\n",
    "          'values': [True, False]\n",
    "        }\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "parameters_dict.update({\n",
    "    'earlystopping_patience': {\n",
    "        'value': 10},\n",
    "    'epochs': {\n",
    "        'value': 100},\n",
    "    'learning_rate': {\n",
    "        'value': 0.00025118864\n",
    "        },\n",
    "    'batch_size': {\n",
    "          'value': 64\n",
    "        },\n",
    "    'kernel_size': {\n",
    "        'value': (3, 3)\n",
    "        },\n",
    "    'dropout': {\n",
    "          'value': True\n",
    "        },\n",
    "    'pooling': {\n",
    "          'value': 'max'\n",
    "        },\n",
    "    'batchnorm': {\n",
    "          'value': True\n",
    "        },\n",
    "    'a_layers': {\n",
    "          'value': 16\n",
    "        },\n",
    "    'reg_alpha': {\n",
    "        'value': 0\n",
    "        },\n",
    "    'normalization': {\n",
    "        'value': False},\n",
    "    'augmentation': {\n",
    "        'value': 'light'\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'grid',\n",
      " 'metric': {'goal': 'minimize', 'name': 'val_loss'},\n",
      " 'parameters': {'a_layers': {'value': 16},\n",
      "                'augmentation': {'value': 'light'},\n",
      "                'batch_size': {'value': 64},\n",
      "                'batchnorm': {'value': True},\n",
      "                'dropout': {'value': True},\n",
      "                'earlystopping_patience': {'value': 10},\n",
      "                'epochs': {'value': 100},\n",
      "                'kernel_size': {'value': (3, 3)},\n",
      "                'learning_rate': {'value': 0.00025118864},\n",
      "                'normalization': {'value': False},\n",
      "                'pooling': {'value': 'max'},\n",
      "                'reg_alpha': {'value': 0},\n",
      "                'super_res': {'values': [True, False]}}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ddw49e7t\n",
      "Sweep URL: https://wandb.ai/takim/CIFAR-10_Classification/sweeps/ddw49e7t\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"CIFAR-10_Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_model(kernel_size, dropout, pooling, batchnorm, n_layers, reg_alpha, input_shape):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size, activation='relu', padding='same', input_shape=input_shape, kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(256, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(256, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(256, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    if n_layers == 19:\n",
    "        model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "        model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "        model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "        if batchnorm:\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=4096, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if dropout:\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=4096, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if dropout:\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train(config = None):\n",
    "    with wandb.init(config=config):\n",
    "\n",
    "        config = wandb.config\n",
    "\n",
    "        do_normalization = config['normalization']\n",
    "        do_augmentation = config['augmentation'] != 'none'\n",
    "        do_superres = config['super_res']\n",
    "\n",
    "        x_train_to_use = (x_train.astype('float32') / 255) if do_normalization else x_train\n",
    "        x_test_to_use = (x_test.astype('float32') / 255) if do_normalization else x_test\n",
    "\n",
    "        if do_superres:\n",
    "            x_train_superres = np.load('x_train_superres.npy')\n",
    "            x_test_superres = np.load('x_test_superres.npy')\n",
    "            x_train_to_use = x_train_superres\n",
    "            x_test_to_use = x_test_superres\n",
    "            \n",
    "        shape = (128, 128, 3) if do_superres else (32, 32, 3)\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        model = create_model(config[\"kernel_size\"], config[\"dropout\"], config[\"pooling\"], config[\"batchnorm\"], config[\"a_layers\"], config[\"reg_alpha\"], shape)\n",
    "        model.compile(\n",
    "            optimizer = Adam(learning_rate=config[\"learning_rate\"]),\n",
    "            loss = \"categorical_crossentropy\",\n",
    "            metrics = [\"accuracy\", tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top@3_accuracy')]\n",
    "        )\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                    patience=config[\"earlystopping_patience\"],\n",
    "                                    restore_best_weights=True)\n",
    "\n",
    "        if not do_augmentation:\n",
    "            history = model.fit(x_train_to_use, y_train,\n",
    "                                epochs=config[\"epochs\"],\n",
    "                                batch_size=config[\"batch_size\"],\n",
    "                                validation_split=0.1,\n",
    "                                callbacks=[\n",
    "                                    WandbMetricsLogger(log_freq='epoch'),\n",
    "                                    early_stopping\n",
    "                                ], verbose=1\n",
    "                                )\n",
    "        else:\n",
    "            if config['augmentation'] == 'light':\n",
    "                datagen = ImageDataGenerator(\n",
    "                    rotation_range=20,\n",
    "                    horizontal_flip=True,\n",
    "                    width_shift_range=0.1,\n",
    "                    height_shift_range=0.1,\n",
    "                    fill_mode='nearest'\n",
    "                )\n",
    "            else:\n",
    "                datagen = ImageDataGenerator(\n",
    "                    rotation_range=40,\n",
    "                    horizontal_flip=True,\n",
    "                    width_shift_range=0.2,\n",
    "                    height_shift_range=0.2,\n",
    "                    shear_range=0.1,\n",
    "                    zoom_range=0.1,\n",
    "                    fill_mode='nearest'\n",
    "                )\n",
    "\n",
    "            x_tr, x_vl, y_tr, y_vl = train_test_split(x_train_to_use, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "            train_datagen = datagen.flow(x_tr, y_tr, batch_size=config[\"batch_size\"])\n",
    "            history = model.fit(train_datagen,\n",
    "                                epochs=config[\"epochs\"],\n",
    "                                batch_size=config[\"batch_size\"],\n",
    "                                validation_data=(x_vl, y_vl),\n",
    "                                callbacks=[\n",
    "                                    WandbMetricsLogger(log_freq='epoch'),\n",
    "                                    early_stopping\n",
    "                                ], verbose=1\n",
    "                                )\n",
    "            \n",
    "        \n",
    "        test_stats = model.evaluate(x_test_to_use, y_test)\n",
    "        wandb.log({\"test_loss\": test_stats[0]})\n",
    "        wandb.log({\"test_acc\": test_stats[1]})\n",
    "\n",
    "        val_loss_history = history.history['val_loss']\n",
    "        val_acc_history = history.history['val_accuracy']\n",
    "\n",
    "        best_epoch_num = -1 if (len(val_loss_history) == 100 or len(val_loss_history) <= 10) else (len(val_loss_history) - 11)\n",
    "\n",
    "        wandb.log({\"best_val_loss\": val_loss_history[best_epoch_num]})\n",
    "        wandb.log({\"best_val_acc\": val_acc_history[best_epoch_num]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: h52wdk1y with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ta_layers: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation: light\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearlystopping_patience: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: [3, 3]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00025118864\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnormalization: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpooling: max\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treg_alpha: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuper_res: True\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\NEURAL PROJE\\wandb\\run-20240104_031115-h52wdk1y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/h52wdk1y' target=\"_blank\">quiet-sweep-1</a></strong> to <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/ddw49e7t' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/ddw49e7t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/ddw49e7t' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/ddw49e7t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/h52wdk1y' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/h52wdk1y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  6/704 [..............................] - ETA: 3:35 - loss: 5.6930 - accuracy: 0.1302 - top@3_accuracy: 0.3047WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0928s vs `on_train_batch_end` time: 0.1809s). Check your callbacks.\n",
      "704/704 [==============================] - 235s 292ms/step - loss: 2.1155 - accuracy: 0.3211 - top@3_accuracy: 0.6609 - val_loss: 1.5988 - val_accuracy: 0.4238 - val_top@3_accuracy: 0.7508\n",
      "Epoch 2/100\n",
      "704/704 [==============================] - 203s 288ms/step - loss: 1.3725 - accuracy: 0.5100 - top@3_accuracy: 0.8214 - val_loss: 1.6123 - val_accuracy: 0.4624 - val_top@3_accuracy: 0.7382\n",
      "Epoch 3/100\n",
      "704/704 [==============================] - 201s 286ms/step - loss: 1.0986 - accuracy: 0.6112 - top@3_accuracy: 0.8812 - val_loss: 1.1125 - val_accuracy: 0.6082 - val_top@3_accuracy: 0.8692\n",
      "Epoch 4/100\n",
      "704/704 [==============================] - 196s 278ms/step - loss: 0.9367 - accuracy: 0.6739 - top@3_accuracy: 0.9087 - val_loss: 0.8081 - val_accuracy: 0.7134 - val_top@3_accuracy: 0.9300\n",
      "Epoch 5/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.8240 - accuracy: 0.7186 - top@3_accuracy: 0.9253 - val_loss: 0.8534 - val_accuracy: 0.7030 - val_top@3_accuracy: 0.9190\n",
      "Epoch 6/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.7303 - accuracy: 0.7547 - top@3_accuracy: 0.9380 - val_loss: 0.8494 - val_accuracy: 0.7106 - val_top@3_accuracy: 0.9212\n",
      "Epoch 7/100\n",
      "704/704 [==============================] - 199s 283ms/step - loss: 0.6615 - accuracy: 0.7802 - top@3_accuracy: 0.9470 - val_loss: 0.6074 - val_accuracy: 0.7922 - val_top@3_accuracy: 0.9536\n",
      "Epoch 8/100\n",
      "704/704 [==============================] - 202s 287ms/step - loss: 0.6046 - accuracy: 0.7998 - top@3_accuracy: 0.9555 - val_loss: 0.5316 - val_accuracy: 0.8184 - val_top@3_accuracy: 0.9652\n",
      "Epoch 9/100\n",
      "704/704 [==============================] - 202s 287ms/step - loss: 0.5625 - accuracy: 0.8140 - top@3_accuracy: 0.9591 - val_loss: 0.6081 - val_accuracy: 0.7962 - val_top@3_accuracy: 0.9506\n",
      "Epoch 10/100\n",
      "704/704 [==============================] - 198s 281ms/step - loss: 0.5115 - accuracy: 0.8292 - top@3_accuracy: 0.9647 - val_loss: 0.5276 - val_accuracy: 0.8256 - val_top@3_accuracy: 0.9670\n",
      "Epoch 11/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.4859 - accuracy: 0.8392 - top@3_accuracy: 0.9681 - val_loss: 0.4618 - val_accuracy: 0.8438 - val_top@3_accuracy: 0.9708\n",
      "Epoch 12/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.4441 - accuracy: 0.8579 - top@3_accuracy: 0.9713 - val_loss: 0.5334 - val_accuracy: 0.8184 - val_top@3_accuracy: 0.9640\n",
      "Epoch 13/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.4166 - accuracy: 0.8665 - top@3_accuracy: 0.9748 - val_loss: 0.4107 - val_accuracy: 0.8708 - val_top@3_accuracy: 0.9756\n",
      "Epoch 14/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.3918 - accuracy: 0.8729 - top@3_accuracy: 0.9770 - val_loss: 0.4270 - val_accuracy: 0.8600 - val_top@3_accuracy: 0.9776\n",
      "Epoch 15/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.3544 - accuracy: 0.8829 - top@3_accuracy: 0.9790 - val_loss: 0.4254 - val_accuracy: 0.8638 - val_top@3_accuracy: 0.9728\n",
      "Epoch 16/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.3476 - accuracy: 0.8884 - top@3_accuracy: 0.9804 - val_loss: 0.3596 - val_accuracy: 0.8806 - val_top@3_accuracy: 0.9790\n",
      "Epoch 17/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.3112 - accuracy: 0.8983 - top@3_accuracy: 0.9828 - val_loss: 0.4648 - val_accuracy: 0.8570 - val_top@3_accuracy: 0.9742\n",
      "Epoch 18/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.3849 - accuracy: 0.8776 - top@3_accuracy: 0.9782 - val_loss: 0.3262 - val_accuracy: 0.8924 - val_top@3_accuracy: 0.9808\n",
      "Epoch 19/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.2990 - accuracy: 0.9024 - top@3_accuracy: 0.9846 - val_loss: 0.3839 - val_accuracy: 0.8760 - val_top@3_accuracy: 0.9786\n",
      "Epoch 20/100\n",
      "704/704 [==============================] - 196s 279ms/step - loss: 0.2868 - accuracy: 0.9063 - top@3_accuracy: 0.9852 - val_loss: 0.3506 - val_accuracy: 0.8898 - val_top@3_accuracy: 0.9782\n",
      "Epoch 21/100\n",
      "704/704 [==============================] - 202s 287ms/step - loss: 0.2654 - accuracy: 0.9128 - top@3_accuracy: 0.9872 - val_loss: 0.3299 - val_accuracy: 0.8972 - val_top@3_accuracy: 0.9840\n",
      "Epoch 22/100\n",
      "704/704 [==============================] - 202s 287ms/step - loss: 0.2488 - accuracy: 0.9190 - top@3_accuracy: 0.9887 - val_loss: 0.3235 - val_accuracy: 0.8980 - val_top@3_accuracy: 0.9832\n",
      "Epoch 23/100\n",
      "704/704 [==============================] - 202s 286ms/step - loss: 0.2325 - accuracy: 0.9233 - top@3_accuracy: 0.9905 - val_loss: 0.3754 - val_accuracy: 0.8862 - val_top@3_accuracy: 0.9812\n",
      "Epoch 24/100\n",
      "704/704 [==============================] - 202s 287ms/step - loss: 0.2208 - accuracy: 0.9284 - top@3_accuracy: 0.9898 - val_loss: 0.3146 - val_accuracy: 0.9094 - val_top@3_accuracy: 0.9882\n",
      "Epoch 25/100\n",
      "704/704 [==============================] - 202s 287ms/step - loss: 0.2162 - accuracy: 0.9289 - top@3_accuracy: 0.9910 - val_loss: 0.3114 - val_accuracy: 0.9016 - val_top@3_accuracy: 0.9808\n",
      "Epoch 26/100\n",
      "704/704 [==============================] - 196s 279ms/step - loss: 0.2669 - accuracy: 0.9182 - top@3_accuracy: 0.9882 - val_loss: 0.3568 - val_accuracy: 0.8864 - val_top@3_accuracy: 0.9808\n",
      "Epoch 27/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.2233 - accuracy: 0.9291 - top@3_accuracy: 0.9910 - val_loss: 0.2728 - val_accuracy: 0.9190 - val_top@3_accuracy: 0.9896\n",
      "Epoch 28/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.1802 - accuracy: 0.9406 - top@3_accuracy: 0.9932 - val_loss: 0.2957 - val_accuracy: 0.9118 - val_top@3_accuracy: 0.9870\n",
      "Epoch 29/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.1730 - accuracy: 0.9445 - top@3_accuracy: 0.9938 - val_loss: 0.2916 - val_accuracy: 0.9092 - val_top@3_accuracy: 0.9874\n",
      "Epoch 30/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.2013 - accuracy: 0.9368 - top@3_accuracy: 0.9922 - val_loss: 0.2781 - val_accuracy: 0.9130 - val_top@3_accuracy: 0.9836\n",
      "Epoch 31/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.2161 - accuracy: 0.9324 - top@3_accuracy: 0.9913 - val_loss: 0.3162 - val_accuracy: 0.9064 - val_top@3_accuracy: 0.9862\n",
      "Epoch 32/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.1702 - accuracy: 0.9452 - top@3_accuracy: 0.9942 - val_loss: 0.2660 - val_accuracy: 0.9206 - val_top@3_accuracy: 0.9890\n",
      "Epoch 33/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.1451 - accuracy: 0.9536 - top@3_accuracy: 0.9952 - val_loss: 0.2798 - val_accuracy: 0.9212 - val_top@3_accuracy: 0.9872\n",
      "Epoch 34/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.1467 - accuracy: 0.9540 - top@3_accuracy: 0.9946 - val_loss: 0.3172 - val_accuracy: 0.9134 - val_top@3_accuracy: 0.9874\n",
      "Epoch 35/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.1561 - accuracy: 0.9510 - top@3_accuracy: 0.9944 - val_loss: 0.2572 - val_accuracy: 0.9164 - val_top@3_accuracy: 0.9876\n",
      "Epoch 36/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.1616 - accuracy: 0.9503 - top@3_accuracy: 0.9942 - val_loss: 0.2773 - val_accuracy: 0.9154 - val_top@3_accuracy: 0.9854\n",
      "Epoch 37/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.1437 - accuracy: 0.9543 - top@3_accuracy: 0.9947 - val_loss: 0.2589 - val_accuracy: 0.9236 - val_top@3_accuracy: 0.9848\n",
      "Epoch 38/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.1610 - accuracy: 0.9513 - top@3_accuracy: 0.9946 - val_loss: 0.6319 - val_accuracy: 0.8098 - val_top@3_accuracy: 0.9490\n",
      "Epoch 39/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.2039 - accuracy: 0.9387 - top@3_accuracy: 0.9921 - val_loss: 0.2805 - val_accuracy: 0.9128 - val_top@3_accuracy: 0.9864\n",
      "Epoch 40/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.1251 - accuracy: 0.9602 - top@3_accuracy: 0.9964 - val_loss: 0.3483 - val_accuracy: 0.9256 - val_top@3_accuracy: 0.9900\n",
      "Epoch 41/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.1275 - accuracy: 0.9590 - top@3_accuracy: 0.9959 - val_loss: 0.2644 - val_accuracy: 0.9246 - val_top@3_accuracy: 0.9854\n",
      "Epoch 42/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.1767 - accuracy: 0.9464 - top@3_accuracy: 0.9937 - val_loss: 0.2927 - val_accuracy: 0.9164 - val_top@3_accuracy: 0.9852\n",
      "Epoch 43/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.1125 - accuracy: 0.9642 - top@3_accuracy: 0.9967 - val_loss: 0.2792 - val_accuracy: 0.9214 - val_top@3_accuracy: 0.9890\n",
      "Epoch 44/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.1106 - accuracy: 0.9663 - top@3_accuracy: 0.9971 - val_loss: 0.2704 - val_accuracy: 0.9222 - val_top@3_accuracy: 0.9868\n",
      "Epoch 45/100\n",
      "704/704 [==============================] - 195s 277ms/step - loss: 0.1103 - accuracy: 0.9658 - top@3_accuracy: 0.9969 - val_loss: 0.3141 - val_accuracy: 0.9162 - val_top@3_accuracy: 0.9856\n",
      "313/313 [==============================] - 14s 39ms/step - loss: 0.2842 - accuracy: 0.9123 - top@3_accuracy: 0.9857\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ddcd0d4495546fb82df660d091860b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁▃▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇███▇████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/top@3_accuracy</td><td>▁▄▆▆▇▇▇▇▇▇▇█████████████████████████████</td></tr><tr><td>epoch/val_accuracy</td><td>▁▂▄▅▅▅▆▇▇▇▇▇▇▇▇▇▇███▇██▇█████████▆██████</td></tr><tr><td>epoch/val_loss</td><td>██▅▄▄▄▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>▁▁▅▆▆▆▇▇▇▇▇██████████████████████▇██████</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.9164</td></tr><tr><td>best_val_loss</td><td>0.25723</td></tr><tr><td>epoch/accuracy</td><td>0.96576</td></tr><tr><td>epoch/epoch</td><td>44</td></tr><tr><td>epoch/learning_rate</td><td>0.00025</td></tr><tr><td>epoch/loss</td><td>0.1103</td></tr><tr><td>epoch/top@3_accuracy</td><td>0.99693</td></tr><tr><td>epoch/val_accuracy</td><td>0.9162</td></tr><tr><td>epoch/val_loss</td><td>0.31414</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>0.9856</td></tr><tr><td>test_acc</td><td>0.9123</td></tr><tr><td>test_loss</td><td>0.28425</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">quiet-sweep-1</strong> at: <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/h52wdk1y' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/h52wdk1y</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_031115-h52wdk1y\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5ox2zohb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ta_layers: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation: light\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearlystopping_patience: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: [3, 3]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00025118864\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnormalization: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpooling: max\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treg_alpha: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuper_res: False\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\NEURAL PROJE\\wandb\\run-20240104_054007-5ox2zohb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/5ox2zohb' target=\"_blank\">colorful-sweep-2</a></strong> to <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/ddw49e7t' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/ddw49e7t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/ddw49e7t' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/ddw49e7t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/5ox2zohb' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/5ox2zohb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  5/704 [..............................] - ETA: 21s - loss: 2.7950 - accuracy: 0.1344 - top@3_accuracy: 0.3375WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0118s vs `on_train_batch_end` time: 0.0156s). Check your callbacks.\n",
      "704/704 [==============================] - 24s 32ms/step - loss: 1.6829 - accuracy: 0.3807 - top@3_accuracy: 0.7314 - val_loss: 2.3872 - val_accuracy: 0.3038 - val_top@3_accuracy: 0.6254\n",
      "Epoch 2/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 1.2455 - accuracy: 0.5580 - top@3_accuracy: 0.8517 - val_loss: 1.0526 - val_accuracy: 0.6256 - val_top@3_accuracy: 0.8822\n",
      "Epoch 3/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 1.0552 - accuracy: 0.6374 - top@3_accuracy: 0.8867 - val_loss: 2.1285 - val_accuracy: 0.4628 - val_top@3_accuracy: 0.8006\n",
      "Epoch 4/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.9255 - accuracy: 0.6857 - top@3_accuracy: 0.9090 - val_loss: 0.9422 - val_accuracy: 0.6860 - val_top@3_accuracy: 0.8962\n",
      "Epoch 5/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.8212 - accuracy: 0.7278 - top@3_accuracy: 0.9244 - val_loss: 0.8396 - val_accuracy: 0.7172 - val_top@3_accuracy: 0.9016\n",
      "Epoch 6/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.7513 - accuracy: 0.7517 - top@3_accuracy: 0.9331 - val_loss: 0.6963 - val_accuracy: 0.7546 - val_top@3_accuracy: 0.9380\n",
      "Epoch 7/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.6969 - accuracy: 0.7710 - top@3_accuracy: 0.9409 - val_loss: 0.8471 - val_accuracy: 0.7296 - val_top@3_accuracy: 0.9166\n",
      "Epoch 8/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.6539 - accuracy: 0.7834 - top@3_accuracy: 0.9492 - val_loss: 0.7000 - val_accuracy: 0.7664 - val_top@3_accuracy: 0.9420\n",
      "Epoch 9/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.6214 - accuracy: 0.7971 - top@3_accuracy: 0.9530 - val_loss: 0.8663 - val_accuracy: 0.7150 - val_top@3_accuracy: 0.9092\n",
      "Epoch 10/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.6500 - accuracy: 0.7911 - top@3_accuracy: 0.9484 - val_loss: 2.0832 - val_accuracy: 0.4626 - val_top@3_accuracy: 0.7352\n",
      "Epoch 11/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.5930 - accuracy: 0.8058 - top@3_accuracy: 0.9556 - val_loss: 0.5811 - val_accuracy: 0.8158 - val_top@3_accuracy: 0.9528\n",
      "Epoch 12/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.5413 - accuracy: 0.8255 - top@3_accuracy: 0.9614 - val_loss: 0.5085 - val_accuracy: 0.8310 - val_top@3_accuracy: 0.9648\n",
      "Epoch 13/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.5932 - accuracy: 0.8073 - top@3_accuracy: 0.9552 - val_loss: 0.5431 - val_accuracy: 0.8176 - val_top@3_accuracy: 0.9630\n",
      "Epoch 14/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.4784 - accuracy: 0.8426 - top@3_accuracy: 0.9665 - val_loss: 0.5055 - val_accuracy: 0.8284 - val_top@3_accuracy: 0.9600\n",
      "Epoch 15/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.5181 - accuracy: 0.8318 - top@3_accuracy: 0.9640 - val_loss: 0.5978 - val_accuracy: 0.8082 - val_top@3_accuracy: 0.9538\n",
      "Epoch 16/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.5104 - accuracy: 0.8366 - top@3_accuracy: 0.9657 - val_loss: 0.4783 - val_accuracy: 0.8408 - val_top@3_accuracy: 0.9626\n",
      "Epoch 17/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.4723 - accuracy: 0.8440 - top@3_accuracy: 0.9687 - val_loss: 0.4773 - val_accuracy: 0.8392 - val_top@3_accuracy: 0.9634\n",
      "Epoch 18/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.4677 - accuracy: 0.8499 - top@3_accuracy: 0.9697 - val_loss: 0.4661 - val_accuracy: 0.8448 - val_top@3_accuracy: 0.9652\n",
      "Epoch 19/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.4316 - accuracy: 0.8590 - top@3_accuracy: 0.9734 - val_loss: 0.4491 - val_accuracy: 0.8510 - val_top@3_accuracy: 0.9702\n",
      "Epoch 20/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.5326 - accuracy: 0.8320 - top@3_accuracy: 0.9640 - val_loss: 0.4984 - val_accuracy: 0.8348 - val_top@3_accuracy: 0.9640\n",
      "Epoch 21/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.3960 - accuracy: 0.8704 - top@3_accuracy: 0.9771 - val_loss: 0.4328 - val_accuracy: 0.8508 - val_top@3_accuracy: 0.9716\n",
      "Epoch 22/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.3639 - accuracy: 0.8798 - top@3_accuracy: 0.9791 - val_loss: 0.4477 - val_accuracy: 0.8562 - val_top@3_accuracy: 0.9670\n",
      "Epoch 23/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.3501 - accuracy: 0.8855 - top@3_accuracy: 0.9803 - val_loss: 0.4597 - val_accuracy: 0.8504 - val_top@3_accuracy: 0.9702\n",
      "Epoch 24/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.3891 - accuracy: 0.8740 - top@3_accuracy: 0.9787 - val_loss: 0.7166 - val_accuracy: 0.7494 - val_top@3_accuracy: 0.9462\n",
      "Epoch 25/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.4920 - accuracy: 0.8501 - top@3_accuracy: 0.9708 - val_loss: 0.6371 - val_accuracy: 0.7946 - val_top@3_accuracy: 0.9430\n",
      "Epoch 26/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.5341 - accuracy: 0.8354 - top@3_accuracy: 0.9651 - val_loss: 0.4206 - val_accuracy: 0.8570 - val_top@3_accuracy: 0.9700\n",
      "Epoch 27/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.3885 - accuracy: 0.8740 - top@3_accuracy: 0.9781 - val_loss: 0.5824 - val_accuracy: 0.8158 - val_top@3_accuracy: 0.9508\n",
      "Epoch 28/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.3254 - accuracy: 0.8941 - top@3_accuracy: 0.9822 - val_loss: 0.3949 - val_accuracy: 0.8728 - val_top@3_accuracy: 0.9730\n",
      "Epoch 29/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.3422 - accuracy: 0.8908 - top@3_accuracy: 0.9819 - val_loss: 0.3839 - val_accuracy: 0.8696 - val_top@3_accuracy: 0.9786\n",
      "Epoch 30/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.3112 - accuracy: 0.8992 - top@3_accuracy: 0.9849 - val_loss: 0.4107 - val_accuracy: 0.8652 - val_top@3_accuracy: 0.9720\n",
      "Epoch 31/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.4253 - accuracy: 0.8648 - top@3_accuracy: 0.9770 - val_loss: 0.4306 - val_accuracy: 0.8586 - val_top@3_accuracy: 0.9732\n",
      "Epoch 32/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.3486 - accuracy: 0.8880 - top@3_accuracy: 0.9831 - val_loss: 0.5919 - val_accuracy: 0.8168 - val_top@3_accuracy: 0.9510\n",
      "Epoch 33/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.4604 - accuracy: 0.8559 - top@3_accuracy: 0.9740 - val_loss: 0.4732 - val_accuracy: 0.8364 - val_top@3_accuracy: 0.9696\n",
      "Epoch 34/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.3057 - accuracy: 0.8997 - top@3_accuracy: 0.9854 - val_loss: 0.3684 - val_accuracy: 0.8790 - val_top@3_accuracy: 0.9774\n",
      "Epoch 35/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.2615 - accuracy: 0.9126 - top@3_accuracy: 0.9881 - val_loss: 0.3399 - val_accuracy: 0.8874 - val_top@3_accuracy: 0.9810\n",
      "Epoch 36/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.4086 - accuracy: 0.8727 - top@3_accuracy: 0.9764 - val_loss: 0.4813 - val_accuracy: 0.8424 - val_top@3_accuracy: 0.9676\n",
      "Epoch 37/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.3109 - accuracy: 0.9010 - top@3_accuracy: 0.9863 - val_loss: 0.5228 - val_accuracy: 0.8466 - val_top@3_accuracy: 0.9626\n",
      "Epoch 38/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.3293 - accuracy: 0.8968 - top@3_accuracy: 0.9849 - val_loss: 0.4035 - val_accuracy: 0.8670 - val_top@3_accuracy: 0.9740\n",
      "Epoch 39/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.2609 - accuracy: 0.9157 - top@3_accuracy: 0.9885 - val_loss: 0.4156 - val_accuracy: 0.8680 - val_top@3_accuracy: 0.9732\n",
      "Epoch 40/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.4297 - accuracy: 0.8708 - top@3_accuracy: 0.9753 - val_loss: 0.4330 - val_accuracy: 0.8564 - val_top@3_accuracy: 0.9704\n",
      "Epoch 41/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.2948 - accuracy: 0.9033 - top@3_accuracy: 0.9865 - val_loss: 0.3906 - val_accuracy: 0.8782 - val_top@3_accuracy: 0.9798\n",
      "Epoch 42/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.2927 - accuracy: 0.9051 - top@3_accuracy: 0.9878 - val_loss: 0.5096 - val_accuracy: 0.8374 - val_top@3_accuracy: 0.9614\n",
      "Epoch 43/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.2472 - accuracy: 0.9179 - top@3_accuracy: 0.9912 - val_loss: 0.4316 - val_accuracy: 0.8666 - val_top@3_accuracy: 0.9758\n",
      "Epoch 44/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.3039 - accuracy: 0.9062 - top@3_accuracy: 0.9877 - val_loss: 0.4683 - val_accuracy: 0.8580 - val_top@3_accuracy: 0.9726\n",
      "Epoch 45/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.2331 - accuracy: 0.9222 - top@3_accuracy: 0.9914 - val_loss: 0.3979 - val_accuracy: 0.8818 - val_top@3_accuracy: 0.9728\n",
      "313/313 [==============================] - 3s 7ms/step - loss: 0.3746 - accuracy: 0.8829 - top@3_accuracy: 0.9787\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68b1b00b7b246f3be7df6d70d28be2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁▃▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇█▇▇▇███▇█▇█████▇████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▆▅▄▄▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▂▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>epoch/top@3_accuracy</td><td>▁▄▅▆▆▆▇▇▇▇▇▇▇▇▇▇█▇████▇▇████████████████</td></tr><tr><td>epoch/val_accuracy</td><td>▁▅▃▆▆▆▆▇▃▇▇▇▇▇▇▇█▇███▆▇█████▇▇███████▇██</td></tr><tr><td>epoch/val_loss</td><td>█▃▇▃▃▂▃▂▇▂▂▂▂▂▁▁▁▂▁▁▁▂▂▁▁▁▁▁▂▁▁▁▂▁▁▁▁▂▁▁</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>▁▆▄▆▆▇▇▇▃▇███▇███████▇▇█████▇███████████</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.8874</td></tr><tr><td>best_val_loss</td><td>0.33993</td></tr><tr><td>epoch/accuracy</td><td>0.92218</td></tr><tr><td>epoch/epoch</td><td>44</td></tr><tr><td>epoch/learning_rate</td><td>0.00025</td></tr><tr><td>epoch/loss</td><td>0.23315</td></tr><tr><td>epoch/top@3_accuracy</td><td>0.99142</td></tr><tr><td>epoch/val_accuracy</td><td>0.8818</td></tr><tr><td>epoch/val_loss</td><td>0.39789</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>0.9728</td></tr><tr><td>test_acc</td><td>0.8829</td></tr><tr><td>test_loss</td><td>0.37458</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">colorful-sweep-2</strong> at: <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/5ox2zohb' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/5ox2zohb</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_054007-5ox2zohb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
