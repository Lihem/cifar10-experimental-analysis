{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Weights and Biases related imports\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (50000, 32, 32, 3)\n",
      "Train Labels Shape: (50000, 10)\n",
      "Test Data Shape: (10000, 32, 32, 3)\n",
      "Test Labels Shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def load_cifar10_batch(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        batch = pickle.load(file, encoding='bytes')\n",
    "    return batch\n",
    "\n",
    "def load_cifar10_data(folder_path):\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        batch_file = f\"{folder_path}/data_batch_{i}\"\n",
    "        batch = load_cifar10_batch(batch_file)\n",
    "        train_data.append(batch[b'data'])\n",
    "        train_labels.extend(batch[b'labels'])\n",
    "\n",
    "    test_batch_file = f\"{folder_path}/test_batch\"\n",
    "    test_batch = load_cifar10_batch(test_batch_file)\n",
    "    test_data = test_batch[b'data']\n",
    "    test_labels = test_batch[b'labels']\n",
    "\n",
    "    train_data = np.vstack(train_data)\n",
    "    train_labels = np.array(train_labels)\n",
    "    test_labels = np.array(test_labels)\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "def preprocess_data(train_data, train_labels, test_data, test_labels):\n",
    "    train_data = train_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "    test_data = test_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "\n",
    "    train_labels_onehot = to_categorical(train_labels)\n",
    "    test_labels_onehot = to_categorical(test_labels)\n",
    "\n",
    "    return train_data, train_labels_onehot, test_data, test_labels_onehot\n",
    "\n",
    "cifar10_folder = 'cifar-10-batches-py'\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_cifar10_data(cifar10_folder)\n",
    "\n",
    "x_train, y_train, x_test, y_test = preprocess_data(\n",
    "    train_data, train_labels, test_data, test_labels\n",
    ")\n",
    "\n",
    "print(\"Train Data Shape:\", x_train.shape)\n",
    "print(\"Train Labels Shape:\", y_train.shape)\n",
    "print(\"Test Data Shape:\", x_test.shape)\n",
    "print(\"Test Labels Shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msta314\u001b[0m (\u001b[33mtakim\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'val_loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    'normalization': {\n",
    "          'values': [False, True]\n",
    "        }\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "parameters_dict.update({\n",
    "    'earlystopping_patience': {\n",
    "        'value': 10},\n",
    "    'epochs': {\n",
    "        'value': 100},\n",
    "    'learning_rate': {\n",
    "        'value': 0.001},\n",
    "    'batch_size': {\n",
    "        'value': 64},\n",
    "    'kernel_size': {\n",
    "          'value': '5x5'},\n",
    "    'net_filter_size': {\n",
    "          'value': 32},\n",
    "    'net_n': {\n",
    "          'value': 3},\n",
    "    'reg_alpha': {\n",
    "          'value': 0.0001}\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'grid',\n",
      " 'metric': {'goal': 'minimize', 'name': 'val_loss'},\n",
      " 'parameters': {'batch_size': {'value': 64},\n",
      "                'earlystopping_patience': {'value': 10},\n",
      "                'epochs': {'value': 100},\n",
      "                'kernel_size': {'value': '5x5'},\n",
      "                'learning_rate': {'value': 0.001},\n",
      "                'net_filter_size': {'value': 32},\n",
      "                'net_n': {'value': 3},\n",
      "                'normalization': {'values': [False, True]},\n",
      "                'reg_alpha': {'value': 0.0001}}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: zia6uq63\n",
      "Sweep URL: https://wandb.ai/takim/CIFAR-10_Classification/sweeps/zia6uq63\n"
     ]
    }
   ],
   "source": [
    "# sweep_id = wandb.sweep(sweep_config, project=\"zayif-test\")\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"CIFAR-10_Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, Lambda, Add, Input, GlobalAveragePooling2D, Flatten, Dense, Softmax\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def ResidualBlock(x, filter_size, is_switch_block, kernel_size, reg_alpha):\n",
    "\n",
    "    # note that if is_switch_block true, it means that output will not be the same as the input\n",
    "    # so while merging the residual connection, we need to adapt to it\n",
    "    # this adaptation could be with a conv layer, or a simple downsampling + padding is enough.\n",
    "\n",
    "    x_skip = x # save original input to the block\n",
    "\n",
    "    if not is_switch_block:\n",
    "        x = Conv2D(filter_size, kernel_size=kernel_size, strides=(1, 1), padding='same', kernel_regularizer=l2(reg_alpha))(x)\n",
    "    else:\n",
    "        x = Conv2D(filter_size, kernel_size=kernel_size, strides=(2, 2), padding='same', kernel_regularizer=l2(reg_alpha))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = Conv2D(filter_size, kernel_size=kernel_size, strides=(1, 1), padding='same', kernel_regularizer=l2(reg_alpha))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    if is_switch_block: # takes every second element to half(v) spatial dimension and then adds padding to each side for matching filter (last) dimension\n",
    "        x_skip = Lambda(lambda x: tf.pad(x[:, ::2, ::2, :], tf.constant([[0, 0,], [0, 0], [0, 0], [filter_size//4, filter_size//4]]), mode=\"CONSTANT\"))(x_skip)\n",
    "\n",
    "    x = Add()([x, x_skip])\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def ResidualBlocks(x, filter_size, n, kernel_size, reg_alpha):\n",
    "    for group in range(3): # a stack of 6n layers, 3×3 convolutions, feature maps of sizes {4fs, 2fs, fs}, 2n layers for each size\n",
    "        for block in range(n):\n",
    "            if group > 0 and block == 0: # double filter size\n",
    "                filter_size *= 2\n",
    "                is_switch_block = True\n",
    "            else:\n",
    "                is_switch_block = False\n",
    "                \n",
    "            x = ResidualBlock(x, filter_size, is_switch_block, kernel_size, reg_alpha)\n",
    "\n",
    "    return x\n",
    "\n",
    "def create_model(config):\n",
    "\n",
    "    filter_size = config['net_filter_size']\n",
    "    n = config['net_n']\n",
    "    kernel_size = (3, 3) if config['kernel_size'] == '3x3' else (5, 5)\n",
    "\n",
    "    reg_alpha = config['reg_alpha']\n",
    "\n",
    "    inputs = Input(shape=(32, 32, 3))\n",
    "    x = Conv2D(filter_size, kernel_size=kernel_size, strides=(1, 1), padding='same', kernel_regularizer=l2(reg_alpha))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = ResidualBlocks(x, filter_size, n, kernel_size, reg_alpha)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(10)(x)\n",
    "    outputs = Softmax()(x)\n",
    "\n",
    "    model = Model(inputs, outputs, name=f\"ResNet-{n*6+2}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def train(config = None):\n",
    "    with wandb.init(config=config):\n",
    "\n",
    "        config = wandb.config\n",
    "\n",
    "        x_train_to_use = (x_train.astype('float32') / 255) if config['normalization'] else x_train\n",
    "        x_test_to_use = (x_test.astype('float32') / 255) if config['normalization'] else x_test\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        model = create_model(config)\n",
    "        model.compile(\n",
    "            optimizer = Adam(learning_rate=config[\"learning_rate\"]),\n",
    "            loss = \"categorical_crossentropy\",\n",
    "            metrics = [\"accuracy\", tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top@3_accuracy')]\n",
    "        )\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                    patience=config[\"earlystopping_patience\"],\n",
    "                                    restore_best_weights=True)\n",
    "\n",
    "        history = model.fit(x_train_to_use, y_train,\n",
    "                                    epochs=config[\"epochs\"],\n",
    "                                    batch_size=config[\"batch_size\"],\n",
    "                                    validation_split=0.1,\n",
    "                                    callbacks=[\n",
    "                                        WandbMetricsLogger(log_freq='epoch'),\n",
    "                                        early_stopping\n",
    "                                    ], verbose=1\n",
    "                                    )\n",
    "        \n",
    "        test_stats = model.evaluate(x_test_to_use, y_test)\n",
    "        wandb.log({\"test_loss\": test_stats[0]})\n",
    "        wandb.log({\"test_acc\": test_stats[1]})\n",
    "\n",
    "        val_loss_history = history.history['val_loss']\n",
    "        val_acc_history = history.history['val_accuracy']\n",
    "\n",
    "        best_epoch_num = -1 if (len(val_loss_history) == 100 or len(val_loss_history) <= 10) else (len(val_loss_history) - 11)\n",
    "\n",
    "        wandb.log({\"best_val_loss\": val_loss_history[best_epoch_num]})\n",
    "        wandb.log({\"best_val_acc\": val_acc_history[best_epoch_num]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kzmi7ixf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearlystopping_patience: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: 5x5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_filter_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_n: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnormalization: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treg_alpha: 0.0001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Stamina\\Desktop\\543Project\\wandb\\run-20240103_010633-kzmi7ixf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/kzmi7ixf' target=\"_blank\">prime-sweep-1</a></strong> to <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/zia6uq63' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/zia6uq63</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/zia6uq63' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/zia6uq63</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/kzmi7ixf' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/kzmi7ixf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "704/704 [==============================] - 43s 55ms/step - loss: 1.5146 - accuracy: 0.5039 - top@3_accuracy: 0.8132 - val_loss: 2.2335 - val_accuracy: 0.3938 - val_top@3_accuracy: 0.6902\n",
      "Epoch 2/100\n",
      "704/704 [==============================] - 41s 59ms/step - loss: 1.0518 - accuracy: 0.6811 - top@3_accuracy: 0.9136 - val_loss: 1.6175 - val_accuracy: 0.5570 - val_top@3_accuracy: 0.8396\n",
      "Epoch 3/100\n",
      "704/704 [==============================] - 45s 64ms/step - loss: 0.8836 - accuracy: 0.7471 - top@3_accuracy: 0.9406 - val_loss: 1.2906 - val_accuracy: 0.6200 - val_top@3_accuracy: 0.8860\n",
      "Epoch 4/100\n",
      "704/704 [==============================] - 49s 69ms/step - loss: 0.8057 - accuracy: 0.7818 - top@3_accuracy: 0.9527 - val_loss: 1.2015 - val_accuracy: 0.6606 - val_top@3_accuracy: 0.9194\n",
      "Epoch 5/100\n",
      "704/704 [==============================] - 51s 73ms/step - loss: 0.7546 - accuracy: 0.8069 - top@3_accuracy: 0.9604 - val_loss: 1.1133 - val_accuracy: 0.7000 - val_top@3_accuracy: 0.9138\n",
      "Epoch 6/100\n",
      "704/704 [==============================] - 53s 76ms/step - loss: 0.7187 - accuracy: 0.8253 - top@3_accuracy: 0.9675 - val_loss: 1.4764 - val_accuracy: 0.6456 - val_top@3_accuracy: 0.8798\n",
      "Epoch 7/100\n",
      "704/704 [==============================] - 54s 77ms/step - loss: 0.6868 - accuracy: 0.8405 - top@3_accuracy: 0.9721 - val_loss: 1.2399 - val_accuracy: 0.6834 - val_top@3_accuracy: 0.9094\n",
      "Epoch 8/100\n",
      "704/704 [==============================] - 55s 78ms/step - loss: 0.6580 - accuracy: 0.8535 - top@3_accuracy: 0.9758 - val_loss: 1.0500 - val_accuracy: 0.7340 - val_top@3_accuracy: 0.9212\n",
      "Epoch 9/100\n",
      "704/704 [==============================] - 56s 79ms/step - loss: 0.6332 - accuracy: 0.8670 - top@3_accuracy: 0.9789 - val_loss: 0.9161 - val_accuracy: 0.7760 - val_top@3_accuracy: 0.9508\n",
      "Epoch 10/100\n",
      "704/704 [==============================] - 54s 77ms/step - loss: 0.6184 - accuracy: 0.8755 - top@3_accuracy: 0.9817 - val_loss: 1.5349 - val_accuracy: 0.6760 - val_top@3_accuracy: 0.8930\n",
      "Epoch 11/100\n",
      "704/704 [==============================] - 54s 76ms/step - loss: 0.5922 - accuracy: 0.8886 - top@3_accuracy: 0.9855 - val_loss: 1.0945 - val_accuracy: 0.7478 - val_top@3_accuracy: 0.9284\n",
      "Epoch 12/100\n",
      "704/704 [==============================] - 54s 77ms/step - loss: 0.5658 - accuracy: 0.8974 - top@3_accuracy: 0.9875 - val_loss: 1.1164 - val_accuracy: 0.7558 - val_top@3_accuracy: 0.9394\n",
      "Epoch 13/100\n",
      "704/704 [==============================] - 55s 79ms/step - loss: 0.5454 - accuracy: 0.9066 - top@3_accuracy: 0.9894 - val_loss: 1.3634 - val_accuracy: 0.7154 - val_top@3_accuracy: 0.9398\n",
      "Epoch 14/100\n",
      "704/704 [==============================] - 55s 79ms/step - loss: 0.5345 - accuracy: 0.9123 - top@3_accuracy: 0.9909 - val_loss: 1.1521 - val_accuracy: 0.7510 - val_top@3_accuracy: 0.9382\n",
      "Epoch 15/100\n",
      "704/704 [==============================] - 57s 81ms/step - loss: 0.5270 - accuracy: 0.9178 - top@3_accuracy: 0.9923 - val_loss: 1.7953 - val_accuracy: 0.6620 - val_top@3_accuracy: 0.9208\n",
      "Epoch 16/100\n",
      "704/704 [==============================] - 57s 81ms/step - loss: 0.5054 - accuracy: 0.9267 - top@3_accuracy: 0.9936 - val_loss: 1.1901 - val_accuracy: 0.7534 - val_top@3_accuracy: 0.9254\n",
      "Epoch 17/100\n",
      "704/704 [==============================] - 57s 81ms/step - loss: 0.4942 - accuracy: 0.9306 - top@3_accuracy: 0.9945 - val_loss: 1.1085 - val_accuracy: 0.7778 - val_top@3_accuracy: 0.9404\n",
      "Epoch 18/100\n",
      "704/704 [==============================] - 58s 82ms/step - loss: 0.4805 - accuracy: 0.9371 - top@3_accuracy: 0.9958 - val_loss: 0.9108 - val_accuracy: 0.8146 - val_top@3_accuracy: 0.9580\n",
      "Epoch 19/100\n",
      "704/704 [==============================] - 57s 81ms/step - loss: 0.4894 - accuracy: 0.9360 - top@3_accuracy: 0.9955 - val_loss: 1.0283 - val_accuracy: 0.7948 - val_top@3_accuracy: 0.9498\n",
      "Epoch 20/100\n",
      "704/704 [==============================] - 58s 82ms/step - loss: 0.4749 - accuracy: 0.9408 - top@3_accuracy: 0.9957 - val_loss: 1.0509 - val_accuracy: 0.7866 - val_top@3_accuracy: 0.9552\n",
      "Epoch 21/100\n",
      "704/704 [==============================] - 58s 82ms/step - loss: 0.4566 - accuracy: 0.9462 - top@3_accuracy: 0.9968 - val_loss: 1.2793 - val_accuracy: 0.7670 - val_top@3_accuracy: 0.9330\n",
      "Epoch 22/100\n",
      "704/704 [==============================] - 56s 80ms/step - loss: 0.4548 - accuracy: 0.9488 - top@3_accuracy: 0.9968 - val_loss: 1.0201 - val_accuracy: 0.8006 - val_top@3_accuracy: 0.9558\n",
      "Epoch 23/100\n",
      "704/704 [==============================] - 56s 80ms/step - loss: 0.4454 - accuracy: 0.9521 - top@3_accuracy: 0.9974 - val_loss: 2.0033 - val_accuracy: 0.6848 - val_top@3_accuracy: 0.8856\n",
      "Epoch 24/100\n",
      "704/704 [==============================] - 56s 80ms/step - loss: 0.4503 - accuracy: 0.9504 - top@3_accuracy: 0.9972 - val_loss: 1.0923 - val_accuracy: 0.7906 - val_top@3_accuracy: 0.9482\n",
      "Epoch 25/100\n",
      "704/704 [==============================] - 56s 80ms/step - loss: 0.4330 - accuracy: 0.9567 - top@3_accuracy: 0.9975 - val_loss: 1.0550 - val_accuracy: 0.7866 - val_top@3_accuracy: 0.9546\n",
      "Epoch 26/100\n",
      "704/704 [==============================] - 56s 80ms/step - loss: 0.4351 - accuracy: 0.9551 - top@3_accuracy: 0.9978 - val_loss: 1.0240 - val_accuracy: 0.8042 - val_top@3_accuracy: 0.9584\n",
      "Epoch 27/100\n",
      "704/704 [==============================] - 56s 80ms/step - loss: 0.4342 - accuracy: 0.9554 - top@3_accuracy: 0.9977 - val_loss: 1.1876 - val_accuracy: 0.7802 - val_top@3_accuracy: 0.9484\n",
      "Epoch 28/100\n",
      "704/704 [==============================] - 56s 80ms/step - loss: 0.4322 - accuracy: 0.9571 - top@3_accuracy: 0.9977 - val_loss: 1.2307 - val_accuracy: 0.7776 - val_top@3_accuracy: 0.9478\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.9186 - accuracy: 0.8148 - top@3_accuracy: 0.9582\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95f68a8efb040fe86693e144f245bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.027 MB uploaded\\r'), FloatProgress(value=0.04130449899172519, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁▄▅▅▆▆▆▆▇▇▇▇▇▇▇█████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▅▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/top@3_accuracy</td><td>▁▅▆▆▇▇▇▇▇▇██████████████████</td></tr><tr><td>epoch/val_accuracy</td><td>▁▄▅▅▆▅▆▇▇▆▇▇▆▇▅▇▇███▇█▆███▇▇</td></tr><tr><td>epoch/val_loss</td><td>█▅▃▃▂▄▃▂▁▄▂▂▃▂▆▂▂▁▂▂▃▂▇▂▂▂▂▃</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>▁▅▆▇▇▆▇▇█▆▇██▇▇▇████▇█▆█████</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.8146</td></tr><tr><td>best_val_loss</td><td>0.91078</td></tr><tr><td>epoch/accuracy</td><td>0.95709</td></tr><tr><td>epoch/epoch</td><td>27</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.43222</td></tr><tr><td>epoch/top@3_accuracy</td><td>0.99769</td></tr><tr><td>epoch/val_accuracy</td><td>0.7776</td></tr><tr><td>epoch/val_loss</td><td>1.23073</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>0.9478</td></tr><tr><td>test_acc</td><td>0.8148</td></tr><tr><td>test_loss</td><td>0.91862</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">prime-sweep-1</strong> at: <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/kzmi7ixf' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/kzmi7ixf</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_010633-kzmi7ixf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d6k90ykh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearlystopping_patience: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: 5x5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_filter_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_n: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnormalization: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treg_alpha: 0.0001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Stamina\\Desktop\\543Project\\wandb\\run-20240103_013206-d6k90ykh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/d6k90ykh' target=\"_blank\">proud-sweep-2</a></strong> to <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/zia6uq63' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/zia6uq63</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/zia6uq63' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/zia6uq63</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/d6k90ykh' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/d6k90ykh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  5/704 [..............................] - ETA: 35s - loss: 4.3648 - accuracy: 0.1063 - top@3_accuracy: 0.3656WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0253s vs `on_train_batch_end` time: 0.0258s). Check your callbacks.\n",
      "704/704 [==============================] - 57s 78ms/step - loss: 1.5845 - accuracy: 0.4740 - top@3_accuracy: 0.7990 - val_loss: 1.6862 - val_accuracy: 0.4486 - val_top@3_accuracy: 0.7876\n",
      "Epoch 2/100\n",
      "704/704 [==============================] - 56s 80ms/step - loss: 1.1001 - accuracy: 0.6572 - top@3_accuracy: 0.9039 - val_loss: 1.2539 - val_accuracy: 0.6010 - val_top@3_accuracy: 0.8834\n",
      "Epoch 3/100\n",
      "704/704 [==============================] - 57s 81ms/step - loss: 0.9113 - accuracy: 0.7316 - top@3_accuracy: 0.9362 - val_loss: 1.4985 - val_accuracy: 0.5502 - val_top@3_accuracy: 0.8290\n",
      "Epoch 4/100\n",
      "704/704 [==============================] - 58s 82ms/step - loss: 0.8147 - accuracy: 0.7745 - top@3_accuracy: 0.9503 - val_loss: 1.0144 - val_accuracy: 0.7020 - val_top@3_accuracy: 0.9196\n",
      "Epoch 5/100\n",
      "704/704 [==============================] - 58s 83ms/step - loss: 0.7482 - accuracy: 0.8034 - top@3_accuracy: 0.9600 - val_loss: 0.9639 - val_accuracy: 0.7380 - val_top@3_accuracy: 0.9328\n",
      "Epoch 6/100\n",
      "704/704 [==============================] - 59s 83ms/step - loss: 0.7062 - accuracy: 0.8205 - top@3_accuracy: 0.9651 - val_loss: 1.2882 - val_accuracy: 0.6572 - val_top@3_accuracy: 0.9120\n",
      "Epoch 7/100\n",
      "704/704 [==============================] - 61s 87ms/step - loss: 0.6706 - accuracy: 0.8372 - top@3_accuracy: 0.9709 - val_loss: 0.9866 - val_accuracy: 0.7334 - val_top@3_accuracy: 0.9402\n",
      "Epoch 8/100\n",
      "704/704 [==============================] - 64s 91ms/step - loss: 0.6449 - accuracy: 0.8480 - top@3_accuracy: 0.9752 - val_loss: 1.2183 - val_accuracy: 0.6842 - val_top@3_accuracy: 0.9258\n",
      "Epoch 9/100\n",
      "704/704 [==============================] - 71s 101ms/step - loss: 0.6142 - accuracy: 0.8620 - top@3_accuracy: 0.9786 - val_loss: 0.9942 - val_accuracy: 0.7486 - val_top@3_accuracy: 0.9252\n",
      "Epoch 10/100\n",
      "704/704 [==============================] - 79s 113ms/step - loss: 0.5869 - accuracy: 0.8738 - top@3_accuracy: 0.9817 - val_loss: 1.2314 - val_accuracy: 0.7002 - val_top@3_accuracy: 0.9372\n",
      "Epoch 11/100\n",
      "704/704 [==============================] - 86s 123ms/step - loss: 0.5649 - accuracy: 0.8848 - top@3_accuracy: 0.9840 - val_loss: 0.8413 - val_accuracy: 0.7982 - val_top@3_accuracy: 0.9524\n",
      "Epoch 12/100\n",
      "704/704 [==============================] - 88s 125ms/step - loss: 0.5516 - accuracy: 0.8917 - top@3_accuracy: 0.9862 - val_loss: 0.8351 - val_accuracy: 0.8000 - val_top@3_accuracy: 0.9588\n",
      "Epoch 13/100\n",
      "704/704 [==============================] - 86s 123ms/step - loss: 0.5268 - accuracy: 0.9000 - top@3_accuracy: 0.9882 - val_loss: 0.9648 - val_accuracy: 0.7648 - val_top@3_accuracy: 0.9460\n",
      "Epoch 14/100\n",
      "704/704 [==============================] - 91s 129ms/step - loss: 0.5129 - accuracy: 0.9081 - top@3_accuracy: 0.9896 - val_loss: 1.5153 - val_accuracy: 0.6642 - val_top@3_accuracy: 0.8910\n",
      "Epoch 15/100\n",
      "704/704 [==============================] - 89s 126ms/step - loss: 0.4961 - accuracy: 0.9148 - top@3_accuracy: 0.9910 - val_loss: 0.9999 - val_accuracy: 0.7694 - val_top@3_accuracy: 0.9448\n",
      "Epoch 16/100\n",
      "704/704 [==============================] - 87s 124ms/step - loss: 0.4835 - accuracy: 0.9193 - top@3_accuracy: 0.9929 - val_loss: 0.9519 - val_accuracy: 0.7802 - val_top@3_accuracy: 0.9574\n",
      "Epoch 17/100\n",
      "704/704 [==============================] - 86s 123ms/step - loss: 0.4674 - accuracy: 0.9264 - top@3_accuracy: 0.9936 - val_loss: 1.0730 - val_accuracy: 0.7718 - val_top@3_accuracy: 0.9454\n",
      "Epoch 18/100\n",
      "704/704 [==============================] - 91s 129ms/step - loss: 0.4694 - accuracy: 0.9299 - top@3_accuracy: 0.9935 - val_loss: 1.1481 - val_accuracy: 0.7744 - val_top@3_accuracy: 0.9406\n",
      "Epoch 19/100\n",
      "704/704 [==============================] - 92s 130ms/step - loss: 0.4461 - accuracy: 0.9367 - top@3_accuracy: 0.9954 - val_loss: 0.9922 - val_accuracy: 0.7938 - val_top@3_accuracy: 0.9560\n",
      "Epoch 20/100\n",
      "704/704 [==============================] - 86s 123ms/step - loss: 0.4329 - accuracy: 0.9410 - top@3_accuracy: 0.9958 - val_loss: 1.2225 - val_accuracy: 0.7700 - val_top@3_accuracy: 0.9440\n",
      "Epoch 21/100\n",
      "704/704 [==============================] - 89s 127ms/step - loss: 0.4411 - accuracy: 0.9407 - top@3_accuracy: 0.9962 - val_loss: 1.1651 - val_accuracy: 0.7886 - val_top@3_accuracy: 0.9496\n",
      "Epoch 22/100\n",
      "704/704 [==============================] - 87s 124ms/step - loss: 0.4270 - accuracy: 0.9464 - top@3_accuracy: 0.9966 - val_loss: 1.3609 - val_accuracy: 0.7176 - val_top@3_accuracy: 0.9320\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.8691 - accuracy: 0.7948 - top@3_accuracy: 0.9594\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14db30e294914a28ad13bd8d0f60c399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁▄▅▅▆▆▆▇▇▇▇▇▇▇████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/top@3_accuracy</td><td>▁▅▆▆▇▇▇▇▇▇████████████</td></tr><tr><td>epoch/val_accuracy</td><td>▁▄▃▆▇▅▇▆▇▆██▇▅▇█▇▇█▇█▆</td></tr><tr><td>epoch/val_loss</td><td>█▄▆▂▂▅▂▄▂▄▁▁▂▇▂▂▃▄▂▄▄▅</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>▁▅▃▆▇▆▇▇▇▇██▇▅▇█▇▇█▇█▇</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.8</td></tr><tr><td>best_val_loss</td><td>0.8351</td></tr><tr><td>epoch/accuracy</td><td>0.94644</td></tr><tr><td>epoch/epoch</td><td>21</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.42696</td></tr><tr><td>epoch/top@3_accuracy</td><td>0.99662</td></tr><tr><td>epoch/val_accuracy</td><td>0.7176</td></tr><tr><td>epoch/val_loss</td><td>1.36094</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>0.932</td></tr><tr><td>test_acc</td><td>0.7948</td></tr><tr><td>test_loss</td><td>0.86912</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">proud-sweep-2</strong> at: <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/d6k90ykh' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/d6k90ykh</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_013206-d6k90ykh\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "# wandb.agent(sweep_id, train, count=1)\n",
    "wandb.agent(sweep_id, train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
