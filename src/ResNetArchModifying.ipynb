{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Weights and Biases related imports\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (50000, 32, 32, 3)\n",
      "Train Labels Shape: (50000, 10)\n",
      "Test Data Shape: (10000, 32, 32, 3)\n",
      "Test Labels Shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def load_cifar10_batch(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        batch = pickle.load(file, encoding='bytes')\n",
    "    return batch\n",
    "\n",
    "def load_cifar10_data(folder_path):\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        batch_file = f\"{folder_path}/data_batch_{i}\"\n",
    "        batch = load_cifar10_batch(batch_file)\n",
    "        train_data.append(batch[b'data'])\n",
    "        train_labels.extend(batch[b'labels'])\n",
    "\n",
    "    test_batch_file = f\"{folder_path}/test_batch\"\n",
    "    test_batch = load_cifar10_batch(test_batch_file)\n",
    "    test_data = test_batch[b'data']\n",
    "    test_labels = test_batch[b'labels']\n",
    "\n",
    "    train_data = np.vstack(train_data)\n",
    "    train_labels = np.array(train_labels)\n",
    "    test_labels = np.array(test_labels)\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "def preprocess_data(train_data, train_labels, test_data, test_labels):\n",
    "    train_data = train_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "    test_data = test_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "\n",
    "    train_labels_onehot = to_categorical(train_labels)\n",
    "    test_labels_onehot = to_categorical(test_labels)\n",
    "\n",
    "    return train_data, train_labels_onehot, test_data, test_labels_onehot\n",
    "\n",
    "cifar10_folder = 'cifar-10-batches-py'\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_cifar10_data(cifar10_folder)\n",
    "\n",
    "x_train, y_train, x_test, y_test = preprocess_data(\n",
    "    train_data, train_labels, test_data, test_labels\n",
    ")\n",
    "\n",
    "print(\"Train Data Shape:\", x_train.shape)\n",
    "print(\"Train Labels Shape:\", y_train.shape)\n",
    "print(\"Test Data Shape:\", x_test.shape)\n",
    "print(\"Test Labels Shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'val_loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    'kernel_size': {\n",
    "          'values': ['3x3', '5x5']\n",
    "        },\n",
    "    'net_filter_size': {\n",
    "          'values': [8, 16, 32]\n",
    "        },\n",
    "    'net_n': {\n",
    "          'values': [3, 5, 8]\n",
    "        },\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "parameters_dict.update({\n",
    "    'earlystopping_patience': {\n",
    "        'value': 10},\n",
    "    'epochs': {\n",
    "        'value': 2},\n",
    "    'learning_rate': {\n",
    "        'value': 0.000004},\n",
    "    'batch_size': {\n",
    "        'value': 64},\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'grid',\n",
      " 'metric': {'goal': 'minimize', 'name': 'val_loss'},\n",
      " 'parameters': {'batch_size': {'value': 64},\n",
      "                'earlystopping_patience': {'value': 10},\n",
      "                'epochs': {'value': 2},\n",
      "                'kernel_size': {'values': ['3x3', '5x5']},\n",
      "                'learning_rate': {'value': 4e-06},\n",
      "                'net_filter_size': {'values': [8, 16, 32]},\n",
      "                'net_n': {'values': [3, 5, 8]}}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: wfks68fe\n",
      "Sweep URL: https://wandb.ai/takim/TestBeforeMain/sweeps/wfks68fe\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"TestBeforeMain\")\n",
    "# sweep_id = wandb.sweep(sweep_config, project=\"CIFAR-10_Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, Lambda, Add, Input, GlobalAveragePooling2D, Flatten, Dense, Softmax\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "def ResidualBlock(x, filter_size, is_switch_block, kernel_size):\n",
    "\n",
    "    # note that if is_switch_block true, it means that output will not be the same as the input\n",
    "    # so while merging the residual connection, we need to adapt to it\n",
    "    # this adaptation could be with a conv layer, or a simple downsampling + padding is enough.\n",
    "\n",
    "    x_skip = x # save original input to the block\n",
    "\n",
    "    if not is_switch_block:\n",
    "        x = Conv2D(filter_size, kernel_size=kernel_size, strides=(1, 1), padding='same')(x)\n",
    "    else:\n",
    "        x = Conv2D(filter_size, kernel_size=kernel_size, strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = Conv2D(filter_size, kernel_size=kernel_size, strides=(1, 1), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    if is_switch_block: # takes every second element to half(v) spatial dimension and then adds padding to each side for matching filter (last) dimension\n",
    "        x_skip = Lambda(lambda x: tf.pad(x[:, ::2, ::2, :], tf.constant([[0, 0,], [0, 0], [0, 0], [filter_size//4, filter_size//4]]), mode=\"CONSTANT\"))(x_skip)\n",
    "\n",
    "    x = Add()([x, x_skip])\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def ResidualBlocks(x, filter_size, n, kernel_size):\n",
    "    for group in range(3): # a stack of 6n layers, 3×3 convolutions, feature maps of sizes {4fs, 2fs, fs}, 2n layers for each size\n",
    "        for block in range(n):\n",
    "            if group > 0 and block == 0: # double filter size\n",
    "                filter_size *= 2\n",
    "                is_switch_block = True\n",
    "            else:\n",
    "                is_switch_block = False\n",
    "                \n",
    "            x = ResidualBlock(x, filter_size, is_switch_block, kernel_size)\n",
    "\n",
    "    return x\n",
    "\n",
    "def create_model(config):\n",
    "\n",
    "    filter_size = config['net_filter_size']\n",
    "    n = config['net_n']\n",
    "    kernel_size = (3, 3) if config['kernel_size'] == '3x3' else (5, 5)\n",
    "\n",
    "    inputs = Input(shape=(32, 32, 3))\n",
    "    x = Conv2D(filter_size, kernel_size=kernel_size, strides=(1, 1), padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = ResidualBlocks(x, filter_size, n, kernel_size)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(10)(x)\n",
    "    outputs = Softmax()(x)\n",
    "\n",
    "    model = Model(inputs, outputs, name=f\"ResNet-{n*6+2}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def train(config = None):\n",
    "    with wandb.init(config=config):\n",
    "\n",
    "        config = wandb.config\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        model = create_model(config)\n",
    "        model.compile(\n",
    "            optimizer = Adam(learning_rate=config[\"learning_rate\"]),\n",
    "            loss = \"categorical_crossentropy\",\n",
    "            metrics = [\"accuracy\", tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top@3_accuracy')]\n",
    "        )\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                    patience=config[\"earlystopping_patience\"],\n",
    "                                    restore_best_weights=True)\n",
    "\n",
    "        history = model.fit(x_train, y_train,\n",
    "                                    epochs=config[\"epochs\"],\n",
    "                                    batch_size=config[\"batch_size\"],\n",
    "                                    validation_split=0.1,\n",
    "                                    callbacks=[\n",
    "                                        WandbMetricsLogger(log_freq='epoch'),\n",
    "                                        early_stopping\n",
    "                                    ], verbose=1\n",
    "                                    )\n",
    "        \n",
    "        test_stats = model.evaluate(x_test, y_test)\n",
    "        wandb.log({\"test_loss\": test_stats[0]})\n",
    "        wandb.log({\"test_acc\": test_stats[1]})\n",
    "\n",
    "        val_loss_history = history.history['val_loss']\n",
    "        val_acc_history = history.history['val_accuracy']\n",
    "\n",
    "        best_epoch_num = -1 if (len(val_loss_history) == 100 or len(val_loss_history) <= 10) else (len(val_loss_history) - 11)\n",
    "\n",
    "        wandb.log({\"best_val_loss\": val_loss_history[best_epoch_num]})\n",
    "        wandb.log({\"best_val_acc\": val_acc_history[best_epoch_num]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tor8qu6a with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearlystopping_patience: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: 3x3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 4e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_filter_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_n: 3\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Stamina\\Desktop\\543Project\\wandb\\run-20240102_015937-tor8qu6a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/takim/TestBeforeMain/runs/tor8qu6a' target=\"_blank\">resilient-sweep-1</a></strong> to <a href='https://wandb.ai/takim/TestBeforeMain' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/takim/TestBeforeMain/sweeps/wfks68fe' target=\"_blank\">https://wandb.ai/takim/TestBeforeMain/sweeps/wfks68fe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/takim/TestBeforeMain' target=\"_blank\">https://wandb.ai/takim/TestBeforeMain</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/takim/TestBeforeMain/sweeps/wfks68fe' target=\"_blank\">https://wandb.ai/takim/TestBeforeMain/sweeps/wfks68fe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/takim/TestBeforeMain/runs/tor8qu6a' target=\"_blank\">https://wandb.ai/takim/TestBeforeMain/runs/tor8qu6a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "704/704 [==============================] - 14s 18ms/step - loss: 4.2435 - accuracy: 0.1006 - top@3_accuracy: 0.3329 - val_loss: 3.6877 - val_accuracy: 0.0950 - val_top@3_accuracy: 0.3422\n",
      "Epoch 2/2\n",
      "704/704 [==============================] - 13s 18ms/step - loss: 3.3644 - accuracy: 0.1060 - top@3_accuracy: 0.3705 - val_loss: 3.1702 - val_accuracy: 0.1054 - val_top@3_accuracy: 0.3738\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 3.1254 - accuracy: 0.1105 - top@3_accuracy: 0.3795\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb224ce8d654510b5e290df2abc1358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁█</td></tr><tr><td>epoch/epoch</td><td>▁█</td></tr><tr><td>epoch/learning_rate</td><td>▁▁</td></tr><tr><td>epoch/loss</td><td>█▁</td></tr><tr><td>epoch/top@3_accuracy</td><td>▁█</td></tr><tr><td>epoch/val_accuracy</td><td>▁█</td></tr><tr><td>epoch/val_loss</td><td>█▁</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>▁█</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.1054</td></tr><tr><td>best_val_loss</td><td>3.17017</td></tr><tr><td>epoch/accuracy</td><td>0.10602</td></tr><tr><td>epoch/epoch</td><td>1</td></tr><tr><td>epoch/learning_rate</td><td>0.0</td></tr><tr><td>epoch/loss</td><td>3.3644</td></tr><tr><td>epoch/top@3_accuracy</td><td>0.37051</td></tr><tr><td>epoch/val_accuracy</td><td>0.1054</td></tr><tr><td>epoch/val_loss</td><td>3.17017</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>0.3738</td></tr><tr><td>test_acc</td><td>0.1105</td></tr><tr><td>test_loss</td><td>3.12543</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">resilient-sweep-1</strong> at: <a href='https://wandb.ai/takim/TestBeforeMain/runs/tor8qu6a' target=\"_blank\">https://wandb.ai/takim/TestBeforeMain/runs/tor8qu6a</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_015937-tor8qu6a\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train, count=1)\n",
    "# wandb.agent(sweep_id, train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
