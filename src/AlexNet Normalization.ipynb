{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and Biases related imports\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (50000, 32, 32, 3)\n",
      "Train Labels Shape: (50000, 10)\n",
      "Test Data Shape: (10000, 32, 32, 3)\n",
      "Test Labels Shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def load_cifar10_batch(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        batch = pickle.load(file, encoding='bytes')\n",
    "    return batch\n",
    "\n",
    "def load_cifar10_data(folder_path):\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        batch_file = f\"{folder_path}/data_batch_{i}\"\n",
    "        batch = load_cifar10_batch(batch_file)\n",
    "        train_data.append(batch[b'data'])\n",
    "        train_labels.extend(batch[b'labels'])\n",
    "\n",
    "    test_batch_file = f\"{folder_path}/test_batch\"\n",
    "    test_batch = load_cifar10_batch(test_batch_file)\n",
    "    test_data = test_batch[b'data']\n",
    "    test_labels = test_batch[b'labels']\n",
    "\n",
    "    train_data = np.vstack(train_data)\n",
    "    train_labels = np.array(train_labels)\n",
    "    test_labels = np.array(test_labels)\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "def preprocess_data(train_data, train_labels, test_data, test_labels):\n",
    "    train_data = train_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "    test_data = test_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "\n",
    "    train_labels_onehot = to_categorical(train_labels)\n",
    "    test_labels_onehot = to_categorical(test_labels)\n",
    "\n",
    "    return train_data, train_labels_onehot, test_data, test_labels_onehot\n",
    "\n",
    "cifar10_folder = 'cifar-10-batches-py'\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_cifar10_data(cifar10_folder)\n",
    "\n",
    "x_train, y_train, x_test, y_test = preprocess_data(\n",
    "    train_data, train_labels, test_data, test_labels\n",
    ")\n",
    "\n",
    "print(\"Train Data Shape:\", x_train.shape)\n",
    "print(\"Train Labels Shape:\", y_train.shape)\n",
    "print(\"Test Data Shape:\", x_test.shape)\n",
    "print(\"Test Labels Shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlihem\u001b[0m (\u001b[33mtakim\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.environ['WANDB_NOTEBOOK_NAME'] = 'RUN_1'\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'val_loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    'normalization': {\n",
    "          'values': [False, True]\n",
    "        }\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "parameters_dict.update({\n",
    "    'earlystopping_patience': {\n",
    "        'value': 10},\n",
    "    'epochs': {\n",
    "        'value': 100},\n",
    "    'learning_rate': {\n",
    "        'value': 0.000063\n",
    "        },\n",
    "    'batch_size': {\n",
    "          'value': 64\n",
    "        },\n",
    "    'dropout': {\n",
    "          'value': True\n",
    "        },\n",
    "    'batchnorm': {\n",
    "          'value': True\n",
    "        },\n",
    "    'regularization': {\n",
    "          'value': False\n",
    "        },\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'grid',\n",
      " 'metric': {'goal': 'minimize', 'name': 'val_loss'},\n",
      " 'parameters': {'batch_size': {'value': 64},\n",
      "                'batchnorm': {'value': True},\n",
      "                'dropout': {'value': True},\n",
      "                'earlystopping_patience': {'value': 10},\n",
      "                'epochs': {'value': 100},\n",
      "                'learning_rate': {'value': 6.3e-05},\n",
      "                'normalization': {'values': [False, True]},\n",
      "                'regularization': {'value': False}}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 0k41xr03\n",
      "Sweep URL: https://wandb.ai/takim/CIFAR-10_Classification/sweeps/0k41xr03\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"CIFAR-10_Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the AlexNet architecture\n",
    "def create_model(dropout, batchnorm, regularization):\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    if regularization:\n",
    "        model.add(tf.keras.layers.Conv2D(filters=96, kernel_size=(11, 11), strides=(2, 2), activation='relu', input_shape=(32, 32, 3), kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Conv2D(filters=96, kernel_size=(11, 11), strides=(2, 2), activation='relu', input_shape=(32, 32, 3)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(1, 1)))\n",
    "\n",
    "\n",
    "    if regularization:\n",
    "        model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding=\"same\"))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(1, 1)))\n",
    "\n",
    "\n",
    "    if regularization:\n",
    "        model.add(tf.keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "        model.add(tf.keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "        model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"same\"))\n",
    "        model.add(tf.keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"same\"))\n",
    "        model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"same\"))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(1, 1)))\n",
    "\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=4096, activation='relu'))\n",
    "    if dropout:\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=4096, activation='relu'))\n",
    "    if dropout:\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def train(config = None):\n",
    "    with wandb.init(config=config):\n",
    "\n",
    "        config = wandb.config\n",
    "\n",
    "        x_train_to_use = (x_train.astype('float32') / 255) if config['normalization'] else x_train\n",
    "        x_test_to_use = (x_test.astype('float32') / 255) if config['normalization'] else x_test\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        model = create_model(config[\"dropout\"], config[\"batchnorm\"], config[\"regularization\"])\n",
    "        model.compile(\n",
    "            optimizer = Adam(learning_rate=config[\"learning_rate\"]),\n",
    "            loss = \"categorical_crossentropy\",\n",
    "            metrics = [\"accuracy\", tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top@3_accuracy')]\n",
    "        )\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                    patience=config[\"earlystopping_patience\"],\n",
    "                                    restore_best_weights=True)\n",
    "\n",
    "        history = model.fit(x_train_to_use, y_train,\n",
    "                                    epochs=config[\"epochs\"],\n",
    "                                    batch_size=config[\"batch_size\"],\n",
    "                                    validation_split=0.1,\n",
    "                                    callbacks=[\n",
    "                                        WandbMetricsLogger(log_freq='epoch'),\n",
    "                                        early_stopping\n",
    "                                    ], verbose=1\n",
    "                                    )\n",
    "        \n",
    "        test_stats = model.evaluate(x_test_to_use, y_test)\n",
    "        wandb.log({\"test_loss\": test_stats[0]})\n",
    "        wandb.log({\"test_acc\": test_stats[1]})\n",
    "\n",
    "        val_loss_history = history.history['val_loss']\n",
    "        val_acc_history = history.history['val_accuracy']\n",
    "\n",
    "        best_epoch_num = -1 if (len(val_loss_history) == 100 or len(val_loss_history) <= 10) else (len(val_loss_history) - 11)\n",
    "\n",
    "        wandb.log({\"best_val_loss\": val_loss_history[best_epoch_num]})\n",
    "        wandb.log({\"best_val_acc\": val_acc_history[best_epoch_num]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: teffd0qn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearlystopping_patience: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 6.3e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnormalization: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tregularization: False\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\NEURAL PROJE\\wandb\\run-20240103_125633-teffd0qn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/teffd0qn' target=\"_blank\">misty-sweep-1</a></strong> to <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/0k41xr03' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/0k41xr03</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/0k41xr03' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/0k41xr03</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/teffd0qn' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/teffd0qn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  4/704 [..............................] - ETA: 15s - loss: 4.0715 - accuracy: 0.1055 - top@3_accuracy: 0.3633  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0070s vs `on_train_batch_end` time: 0.0127s). Check your callbacks.\n",
      "704/704 [==============================] - 19s 23ms/step - loss: 1.9588 - accuracy: 0.3630 - top@3_accuracy: 0.7016 - val_loss: 1.5109 - val_accuracy: 0.4546 - val_top@3_accuracy: 0.7924\n",
      "Epoch 2/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 1.4450 - accuracy: 0.4886 - top@3_accuracy: 0.8099 - val_loss: 1.3964 - val_accuracy: 0.4912 - val_top@3_accuracy: 0.8194\n",
      "Epoch 3/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 1.2895 - accuracy: 0.5439 - top@3_accuracy: 0.8394 - val_loss: 1.5459 - val_accuracy: 0.4430 - val_top@3_accuracy: 0.7764\n",
      "Epoch 4/100\n",
      "704/704 [==============================] - 16s 22ms/step - loss: 1.1735 - accuracy: 0.5851 - top@3_accuracy: 0.8634 - val_loss: 1.2737 - val_accuracy: 0.5454 - val_top@3_accuracy: 0.8510\n",
      "Epoch 5/100\n",
      "704/704 [==============================] - 16s 22ms/step - loss: 1.0767 - accuracy: 0.6236 - top@3_accuracy: 0.8836 - val_loss: 1.2344 - val_accuracy: 0.5540 - val_top@3_accuracy: 0.8662\n",
      "Epoch 6/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.9894 - accuracy: 0.6544 - top@3_accuracy: 0.8972 - val_loss: 1.6100 - val_accuracy: 0.4498 - val_top@3_accuracy: 0.7474\n",
      "Epoch 7/100\n",
      "704/704 [==============================] - 16s 22ms/step - loss: 0.9032 - accuracy: 0.6856 - top@3_accuracy: 0.9112 - val_loss: 1.1417 - val_accuracy: 0.6026 - val_top@3_accuracy: 0.8746\n",
      "Epoch 8/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.8314 - accuracy: 0.7120 - top@3_accuracy: 0.9236 - val_loss: 1.1645 - val_accuracy: 0.5922 - val_top@3_accuracy: 0.8712\n",
      "Epoch 9/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.7584 - accuracy: 0.7368 - top@3_accuracy: 0.9344 - val_loss: 1.2384 - val_accuracy: 0.5930 - val_top@3_accuracy: 0.8502\n",
      "Epoch 10/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.6825 - accuracy: 0.7615 - top@3_accuracy: 0.9461 - val_loss: 1.1671 - val_accuracy: 0.6056 - val_top@3_accuracy: 0.8648\n",
      "Epoch 11/100\n",
      "704/704 [==============================] - 16s 22ms/step - loss: 0.6198 - accuracy: 0.7832 - top@3_accuracy: 0.9531 - val_loss: 1.0214 - val_accuracy: 0.6498 - val_top@3_accuracy: 0.8944\n",
      "Epoch 12/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.5468 - accuracy: 0.8099 - top@3_accuracy: 0.9628 - val_loss: 1.0494 - val_accuracy: 0.6556 - val_top@3_accuracy: 0.8930\n",
      "Epoch 13/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.4832 - accuracy: 0.8279 - top@3_accuracy: 0.9700 - val_loss: 1.1655 - val_accuracy: 0.6296 - val_top@3_accuracy: 0.8786\n",
      "Epoch 14/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.4285 - accuracy: 0.8483 - top@3_accuracy: 0.9754 - val_loss: 1.0743 - val_accuracy: 0.6556 - val_top@3_accuracy: 0.8890\n",
      "Epoch 15/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.3728 - accuracy: 0.8681 - top@3_accuracy: 0.9817 - val_loss: 1.2065 - val_accuracy: 0.6412 - val_top@3_accuracy: 0.8836\n",
      "Epoch 16/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.3310 - accuracy: 0.8834 - top@3_accuracy: 0.9848 - val_loss: 1.1829 - val_accuracy: 0.6588 - val_top@3_accuracy: 0.8944\n",
      "Epoch 17/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.2963 - accuracy: 0.8973 - top@3_accuracy: 0.9884 - val_loss: 1.1113 - val_accuracy: 0.6892 - val_top@3_accuracy: 0.9080\n",
      "Epoch 18/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.2684 - accuracy: 0.9066 - top@3_accuracy: 0.9912 - val_loss: 1.2981 - val_accuracy: 0.6562 - val_top@3_accuracy: 0.8884\n",
      "Epoch 19/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.2364 - accuracy: 0.9190 - top@3_accuracy: 0.9923 - val_loss: 1.3268 - val_accuracy: 0.6604 - val_top@3_accuracy: 0.8912\n",
      "Epoch 20/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.2050 - accuracy: 0.9301 - top@3_accuracy: 0.9941 - val_loss: 1.3275 - val_accuracy: 0.6466 - val_top@3_accuracy: 0.8906\n",
      "Epoch 21/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.1992 - accuracy: 0.9326 - top@3_accuracy: 0.9941 - val_loss: 1.6583 - val_accuracy: 0.6034 - val_top@3_accuracy: 0.8410\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 1.0177 - accuracy: 0.6491 - top@3_accuracy: 0.8915\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2461621f41184aa0859a02b6a9188ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁▃▃▄▄▅▅▅▆▆▆▆▇▇▇▇█████</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▆▅▅▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>epoch/top@3_accuracy</td><td>▁▄▄▅▅▆▆▆▇▇▇▇▇████████</td></tr><tr><td>epoch/val_accuracy</td><td>▁▂▁▄▄▁▆▅▅▆▇▇▆▇▇▇█▇▇▇▆</td></tr><tr><td>epoch/val_loss</td><td>▆▅▇▄▃▇▂▃▃▃▁▁▃▂▃▃▂▄▄▄█</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>▃▄▂▆▆▁▇▆▅▆▇▇▇▇▇▇█▇▇▇▅</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.6498</td></tr><tr><td>best_val_loss</td><td>1.02138</td></tr><tr><td>epoch/accuracy</td><td>0.93258</td></tr><tr><td>epoch/epoch</td><td>20</td></tr><tr><td>epoch/learning_rate</td><td>6e-05</td></tr><tr><td>epoch/loss</td><td>0.19923</td></tr><tr><td>epoch/top@3_accuracy</td><td>0.99413</td></tr><tr><td>epoch/val_accuracy</td><td>0.6034</td></tr><tr><td>epoch/val_loss</td><td>1.65832</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>0.841</td></tr><tr><td>test_acc</td><td>0.6491</td></tr><tr><td>test_loss</td><td>1.01772</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">misty-sweep-1</strong> at: <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/teffd0qn' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/teffd0qn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_125633-teffd0qn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: grj5k8k8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearlystopping_patience: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 6.3e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnormalization: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tregularization: False\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\NEURAL PROJE\\wandb\\run-20240103_130216-grj5k8k8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/grj5k8k8' target=\"_blank\">eager-sweep-2</a></strong> to <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/0k41xr03' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/0k41xr03</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/0k41xr03' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/0k41xr03</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/grj5k8k8' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/grj5k8k8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  4/704 [..............................] - ETA: 16s - loss: 4.3915 - accuracy: 0.0820 - top@3_accuracy: 0.3125 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_train_batch_end` time: 0.0156s). Check your callbacks.\n",
      "704/704 [==============================] - 17s 23ms/step - loss: 1.8187 - accuracy: 0.3835 - top@3_accuracy: 0.7193 - val_loss: 2.0081 - val_accuracy: 0.3136 - val_top@3_accuracy: 0.6922\n",
      "Epoch 2/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 1.3942 - accuracy: 0.5028 - top@3_accuracy: 0.8170 - val_loss: 1.5525 - val_accuracy: 0.4606 - val_top@3_accuracy: 0.7730\n",
      "Epoch 3/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 1.2362 - accuracy: 0.5637 - top@3_accuracy: 0.8519 - val_loss: 1.2886 - val_accuracy: 0.5542 - val_top@3_accuracy: 0.8352\n",
      "Epoch 4/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 1.1467 - accuracy: 0.5956 - top@3_accuracy: 0.8705 - val_loss: 1.2107 - val_accuracy: 0.5766 - val_top@3_accuracy: 0.8608\n",
      "Epoch 5/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 1.0434 - accuracy: 0.6348 - top@3_accuracy: 0.8877 - val_loss: 1.1821 - val_accuracy: 0.5882 - val_top@3_accuracy: 0.8628\n",
      "Epoch 6/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.9672 - accuracy: 0.6644 - top@3_accuracy: 0.8998 - val_loss: 1.1613 - val_accuracy: 0.5942 - val_top@3_accuracy: 0.8588\n",
      "Epoch 7/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.8811 - accuracy: 0.6939 - top@3_accuracy: 0.9155 - val_loss: 1.1858 - val_accuracy: 0.5876 - val_top@3_accuracy: 0.8666\n",
      "Epoch 8/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.8064 - accuracy: 0.7216 - top@3_accuracy: 0.9265 - val_loss: 1.1642 - val_accuracy: 0.5946 - val_top@3_accuracy: 0.8566\n",
      "Epoch 9/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.7390 - accuracy: 0.7441 - top@3_accuracy: 0.9389 - val_loss: 1.1123 - val_accuracy: 0.6102 - val_top@3_accuracy: 0.8732\n",
      "Epoch 10/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.6644 - accuracy: 0.7687 - top@3_accuracy: 0.9473 - val_loss: 1.2895 - val_accuracy: 0.5728 - val_top@3_accuracy: 0.8404\n",
      "Epoch 11/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.5886 - accuracy: 0.7942 - top@3_accuracy: 0.9569 - val_loss: 1.1448 - val_accuracy: 0.6208 - val_top@3_accuracy: 0.8768\n",
      "Epoch 12/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.5326 - accuracy: 0.8160 - top@3_accuracy: 0.9633 - val_loss: 1.1605 - val_accuracy: 0.6256 - val_top@3_accuracy: 0.8668\n",
      "Epoch 13/100\n",
      "704/704 [==============================] - 16s 22ms/step - loss: 0.4745 - accuracy: 0.8356 - top@3_accuracy: 0.9705 - val_loss: 1.0035 - val_accuracy: 0.6594 - val_top@3_accuracy: 0.8982\n",
      "Epoch 14/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.4074 - accuracy: 0.8580 - top@3_accuracy: 0.9770 - val_loss: 1.2282 - val_accuracy: 0.6236 - val_top@3_accuracy: 0.8728\n",
      "Epoch 15/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.3590 - accuracy: 0.8761 - top@3_accuracy: 0.9821 - val_loss: 1.2356 - val_accuracy: 0.6394 - val_top@3_accuracy: 0.8878\n",
      "Epoch 16/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.3185 - accuracy: 0.8902 - top@3_accuracy: 0.9865 - val_loss: 1.1176 - val_accuracy: 0.6502 - val_top@3_accuracy: 0.8956\n",
      "Epoch 17/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.2741 - accuracy: 0.9055 - top@3_accuracy: 0.9895 - val_loss: 1.0878 - val_accuracy: 0.6658 - val_top@3_accuracy: 0.9022\n",
      "Epoch 18/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.2462 - accuracy: 0.9147 - top@3_accuracy: 0.9912 - val_loss: 1.1343 - val_accuracy: 0.6892 - val_top@3_accuracy: 0.9078\n",
      "Epoch 19/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.2141 - accuracy: 0.9269 - top@3_accuracy: 0.9942 - val_loss: 1.2413 - val_accuracy: 0.6666 - val_top@3_accuracy: 0.8914\n",
      "Epoch 20/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.2042 - accuracy: 0.9301 - top@3_accuracy: 0.9941 - val_loss: 1.1672 - val_accuracy: 0.6708 - val_top@3_accuracy: 0.8982\n",
      "Epoch 21/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.1901 - accuracy: 0.9350 - top@3_accuracy: 0.9960 - val_loss: 1.3382 - val_accuracy: 0.6610 - val_top@3_accuracy: 0.8948\n",
      "Epoch 22/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.1704 - accuracy: 0.9433 - top@3_accuracy: 0.9964 - val_loss: 1.4134 - val_accuracy: 0.6534 - val_top@3_accuracy: 0.8858\n",
      "Epoch 23/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 0.1586 - accuracy: 0.9468 - top@3_accuracy: 0.9964 - val_loss: 1.5345 - val_accuracy: 0.6236 - val_top@3_accuracy: 0.8780\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 1.0360 - accuracy: 0.6551 - top@3_accuracy: 0.8889\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bee089c2884ef0821e899520126583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.024 MB uploaded\\r'), FloatProgress(value=0.045783714193756035, max=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁▂▃▄▄▄▅▅▅▆▆▆▇▇▇▇▇██████</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▆▆▅▅▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>epoch/top@3_accuracy</td><td>▁▃▄▅▅▆▆▆▇▇▇▇▇██████████</td></tr><tr><td>epoch/val_accuracy</td><td>▁▄▅▆▆▆▆▆▇▆▇▇▇▇▇▇████▇▇▇</td></tr><tr><td>epoch/val_loss</td><td>█▅▃▂▂▂▂▂▂▃▂▂▁▃▃▂▂▂▃▂▃▄▅</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>▁▄▆▆▇▆▇▆▇▆▇▇█▇▇███▇██▇▇</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.6594</td></tr><tr><td>best_val_loss</td><td>1.00354</td></tr><tr><td>epoch/accuracy</td><td>0.9468</td></tr><tr><td>epoch/epoch</td><td>22</td></tr><tr><td>epoch/learning_rate</td><td>6e-05</td></tr><tr><td>epoch/loss</td><td>0.15862</td></tr><tr><td>epoch/top@3_accuracy</td><td>0.9964</td></tr><tr><td>epoch/val_accuracy</td><td>0.6236</td></tr><tr><td>epoch/val_loss</td><td>1.53453</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>0.878</td></tr><tr><td>test_acc</td><td>0.6551</td></tr><tr><td>test_loss</td><td>1.036</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eager-sweep-2</strong> at: <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/grj5k8k8' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/grj5k8k8</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_130216-grj5k8k8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
