{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and Biases related imports\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (50000, 32, 32, 3)\n",
      "Train Labels Shape: (50000, 10)\n",
      "Test Data Shape: (10000, 32, 32, 3)\n",
      "Test Labels Shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def load_cifar10_batch(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        batch = pickle.load(file, encoding='bytes')\n",
    "    return batch\n",
    "\n",
    "def load_cifar10_data(folder_path):\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        batch_file = f\"{folder_path}/data_batch_{i}\"\n",
    "        batch = load_cifar10_batch(batch_file)\n",
    "        train_data.append(batch[b'data'])\n",
    "        train_labels.extend(batch[b'labels'])\n",
    "\n",
    "    test_batch_file = f\"{folder_path}/test_batch\"\n",
    "    test_batch = load_cifar10_batch(test_batch_file)\n",
    "    test_data = test_batch[b'data']\n",
    "    test_labels = test_batch[b'labels']\n",
    "\n",
    "    train_data = np.vstack(train_data)\n",
    "    train_labels = np.array(train_labels)\n",
    "    test_labels = np.array(test_labels)\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "def preprocess_data(train_data, train_labels, test_data, test_labels):\n",
    "    train_data = train_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "    test_data = test_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "\n",
    "    train_labels_onehot = to_categorical(train_labels)\n",
    "    test_labels_onehot = to_categorical(test_labels)\n",
    "\n",
    "    return train_data, train_labels_onehot, test_data, test_labels_onehot\n",
    "\n",
    "cifar10_folder = 'cifar-10-batches-py'\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_cifar10_data(cifar10_folder)\n",
    "\n",
    "x_train, y_train, x_test, y_test = preprocess_data(\n",
    "    train_data, train_labels, test_data, test_labels\n",
    ")\n",
    "\n",
    "print(\"Train Data Shape:\", x_train.shape)\n",
    "print(\"Train Labels Shape:\", y_train.shape)\n",
    "print(\"Test Data Shape:\", x_test.shape)\n",
    "print(\"Test Labels Shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlihem\u001b[0m (\u001b[33mtakim\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.environ['WANDB_NOTEBOOK_NAME'] = 'RUN_1'\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'val_loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    'augmentation': {\n",
    "          'values': ['none', 'light', 'heavy']\n",
    "        }\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "parameters_dict.update({\n",
    "    'earlystopping_patience': {\n",
    "        'value': 10},\n",
    "    'epochs': {\n",
    "        'value': 100},\n",
    "    'learning_rate': {\n",
    "        'value': 0.00025118864\n",
    "        },\n",
    "    'batch_size': {\n",
    "          'value': 64\n",
    "        },\n",
    "    'kernel_size': {\n",
    "        'value': (3, 3)\n",
    "        },\n",
    "    'dropout': {\n",
    "          'value': True\n",
    "        },\n",
    "    'pooling': {\n",
    "          'value': 'max'\n",
    "        },\n",
    "    'batchnorm': {\n",
    "          'value': True\n",
    "        },\n",
    "    'a_layers': {\n",
    "          'value': 16\n",
    "        },\n",
    "    'reg_alpha': {\n",
    "        'value': 0\n",
    "        },\n",
    "    'normalization': {\n",
    "        'value': False}\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'grid',\n",
      " 'metric': {'goal': 'minimize', 'name': 'val_loss'},\n",
      " 'parameters': {'a_layers': {'value': 16},\n",
      "                'augmentation': {'values': ['none', 'light', 'heavy']},\n",
      "                'batch_size': {'value': 64},\n",
      "                'batchnorm': {'value': True},\n",
      "                'dropout': {'value': True},\n",
      "                'earlystopping_patience': {'value': 10},\n",
      "                'epochs': {'value': 100},\n",
      "                'kernel_size': {'value': (3, 3)},\n",
      "                'learning_rate': {'value': 0.00025118864},\n",
      "                'normalization': {'value': False},\n",
      "                'pooling': {'value': 'max'},\n",
      "                'reg_alpha': {'value': 0}}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: s4w5tx8e\n",
      "Sweep URL: https://wandb.ai/takim/CIFAR-10_Classification/sweeps/s4w5tx8e\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"CIFAR-10_Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_model(kernel_size, dropout, pooling, batchnorm, n_layers, reg_alpha):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size, activation='relu', padding='same', input_shape=(32, 32, 3), kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(256, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(256, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(256, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    if n_layers == 19:\n",
    "        model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "        model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "        model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "        if batchnorm:\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=4096, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if dropout:\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=4096, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if dropout:\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train(config = None):\n",
    "    with wandb.init(config=config):\n",
    "\n",
    "        config = wandb.config\n",
    "\n",
    "        do_normalization = config['normalization']\n",
    "        do_augmentation = config['augmentation'] != 'none'\n",
    "\n",
    "        x_train_to_use = (x_train.astype('float32') / 255) if do_normalization else x_train\n",
    "        x_test_to_use = (x_test.astype('float32') / 255) if do_normalization else x_test\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        model = create_model(config[\"kernel_size\"], config[\"dropout\"], config[\"pooling\"], config[\"batchnorm\"], config[\"a_layers\"], config[\"reg_alpha\"])\n",
    "        model.compile(\n",
    "            optimizer = Adam(learning_rate=config[\"learning_rate\"]),\n",
    "            loss = \"categorical_crossentropy\",\n",
    "            metrics = [\"accuracy\", tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top@3_accuracy')]\n",
    "        )\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                    patience=config[\"earlystopping_patience\"],\n",
    "                                    restore_best_weights=True)\n",
    "\n",
    "        if not do_augmentation:\n",
    "            history = model.fit(x_train_to_use, y_train,\n",
    "                                epochs=config[\"epochs\"],\n",
    "                                batch_size=config[\"batch_size\"],\n",
    "                                validation_split=0.1,\n",
    "                                callbacks=[\n",
    "                                    WandbMetricsLogger(log_freq='epoch'),\n",
    "                                    early_stopping\n",
    "                                ], verbose=1\n",
    "                                )\n",
    "        else:\n",
    "            if config['augmentation'] == 'light':\n",
    "                datagen = ImageDataGenerator(\n",
    "                    rotation_range=20,\n",
    "                    horizontal_flip=True,\n",
    "                    width_shift_range=0.1,\n",
    "                    height_shift_range=0.1,\n",
    "                    fill_mode='nearest'\n",
    "                )\n",
    "            else:\n",
    "                datagen = ImageDataGenerator(\n",
    "                    rotation_range=40,\n",
    "                    horizontal_flip=True,\n",
    "                    width_shift_range=0.2,\n",
    "                    height_shift_range=0.2,\n",
    "                    shear_range=0.1,\n",
    "                    zoom_range=0.1,\n",
    "                    fill_mode='nearest'\n",
    "                )\n",
    "\n",
    "            x_tr, x_vl, y_tr, y_vl = train_test_split(x_train_to_use, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "            train_datagen = datagen.flow(x_tr, y_tr, batch_size=config[\"batch_size\"])\n",
    "            history = model.fit(train_datagen,\n",
    "                                epochs=config[\"epochs\"],\n",
    "                                batch_size=config[\"batch_size\"],\n",
    "                                validation_data=(x_vl, y_vl),\n",
    "                                callbacks=[\n",
    "                                    WandbMetricsLogger(log_freq='epoch'),\n",
    "                                    early_stopping\n",
    "                                ], verbose=1\n",
    "                                )\n",
    "            \n",
    "        \n",
    "        test_stats = model.evaluate(x_test_to_use, y_test)\n",
    "        wandb.log({\"test_loss\": test_stats[0]})\n",
    "        wandb.log({\"test_acc\": test_stats[1]})\n",
    "\n",
    "        val_loss_history = history.history['val_loss']\n",
    "        val_acc_history = history.history['val_accuracy']\n",
    "\n",
    "        best_epoch_num = -1 if (len(val_loss_history) == 100 or len(val_loss_history) <= 10) else (len(val_loss_history) - 11)\n",
    "\n",
    "        wandb.log({\"best_val_loss\": val_loss_history[best_epoch_num]})\n",
    "        wandb.log({\"best_val_acc\": val_acc_history[best_epoch_num]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: njsfs215 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ta_layers: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation: none\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearlystopping_patience: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: [3, 3]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00025118864\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnormalization: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpooling: max\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treg_alpha: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\NEURAL PROJE\\wandb\\run-20240103_115657-njsfs215</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/njsfs215' target=\"_blank\">golden-sweep-1</a></strong> to <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/s4w5tx8e' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/s4w5tx8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/s4w5tx8e' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/s4w5tx8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/njsfs215' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/njsfs215</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  5/704 [..............................] - ETA: 22s - loss: 3.3629 - accuracy: 0.1125 - top@3_accuracy: 0.3344WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0125s vs `on_train_batch_end` time: 0.0169s). Check your callbacks.\n",
      "704/704 [==============================] - 27s 34ms/step - loss: 1.6127 - accuracy: 0.4149 - top@3_accuracy: 0.7568 - val_loss: 1.3176 - val_accuracy: 0.5268 - val_top@3_accuracy: 0.8308\n",
      "Epoch 2/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 1.0666 - accuracy: 0.6299 - top@3_accuracy: 0.8860 - val_loss: 0.9793 - val_accuracy: 0.6734 - val_top@3_accuracy: 0.9024\n",
      "Epoch 3/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.8147 - accuracy: 0.7252 - top@3_accuracy: 0.9259 - val_loss: 1.2230 - val_accuracy: 0.6204 - val_top@3_accuracy: 0.8370\n",
      "Epoch 4/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.6498 - accuracy: 0.7860 - top@3_accuracy: 0.9475 - val_loss: 1.0653 - val_accuracy: 0.6718 - val_top@3_accuracy: 0.8922\n",
      "Epoch 5/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.5421 - accuracy: 0.8234 - top@3_accuracy: 0.9606 - val_loss: 0.6598 - val_accuracy: 0.7804 - val_top@3_accuracy: 0.9458\n",
      "Epoch 6/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.4391 - accuracy: 0.8564 - top@3_accuracy: 0.9720 - val_loss: 0.7735 - val_accuracy: 0.7712 - val_top@3_accuracy: 0.9424\n",
      "Epoch 7/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.3695 - accuracy: 0.8821 - top@3_accuracy: 0.9785 - val_loss: 0.8963 - val_accuracy: 0.7502 - val_top@3_accuracy: 0.9032\n",
      "Epoch 8/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.3301 - accuracy: 0.8970 - top@3_accuracy: 0.9818 - val_loss: 0.7436 - val_accuracy: 0.7868 - val_top@3_accuracy: 0.9444\n",
      "Epoch 9/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.2427 - accuracy: 0.9228 - top@3_accuracy: 0.9885 - val_loss: 0.8364 - val_accuracy: 0.7800 - val_top@3_accuracy: 0.9376\n",
      "Epoch 10/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.1871 - accuracy: 0.9409 - top@3_accuracy: 0.9919 - val_loss: 0.7597 - val_accuracy: 0.8020 - val_top@3_accuracy: 0.9506\n",
      "Epoch 11/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.4168 - accuracy: 0.8888 - top@3_accuracy: 0.9762 - val_loss: 0.7385 - val_accuracy: 0.7760 - val_top@3_accuracy: 0.9388\n",
      "Epoch 12/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.2625 - accuracy: 0.9190 - top@3_accuracy: 0.9864 - val_loss: 0.7146 - val_accuracy: 0.8084 - val_top@3_accuracy: 0.9570\n",
      "Epoch 13/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.1163 - accuracy: 0.9636 - top@3_accuracy: 0.9966 - val_loss: 0.8843 - val_accuracy: 0.8018 - val_top@3_accuracy: 0.9498\n",
      "Epoch 14/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.1123 - accuracy: 0.9657 - top@3_accuracy: 0.9963 - val_loss: 0.7433 - val_accuracy: 0.8196 - val_top@3_accuracy: 0.9582\n",
      "Epoch 15/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.1120 - accuracy: 0.9672 - top@3_accuracy: 0.9970 - val_loss: 0.9242 - val_accuracy: 0.7908 - val_top@3_accuracy: 0.9388\n",
      "313/313 [==============================] - 3s 7ms/step - loss: 0.6857 - accuracy: 0.7797 - top@3_accuracy: 0.9427\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4f91a2e3c646ad95582a34a6dac2f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.009 MB uploaded\\r'), FloatProgress(value=0.1161533754843973, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁▄▅▆▆▇▇▇▇█▇▇███</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▅▄▄▃▃▂▂▂▁▂▂▁▁▁</td></tr><tr><td>epoch/top@3_accuracy</td><td>▁▅▆▇▇▇▇███▇████</td></tr><tr><td>epoch/val_accuracy</td><td>▁▅▃▄▇▇▆▇▇█▇███▇</td></tr><tr><td>epoch/val_loss</td><td>█▄▇▅▁▂▄▂▃▂▂▂▃▂▄</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>▁▅▁▄▇▇▅▇▇█▇███▇</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.7804</td></tr><tr><td>best_val_loss</td><td>0.65981</td></tr><tr><td>epoch/accuracy</td><td>0.96718</td></tr><tr><td>epoch/epoch</td><td>14</td></tr><tr><td>epoch/learning_rate</td><td>0.00025</td></tr><tr><td>epoch/loss</td><td>0.11198</td></tr><tr><td>epoch/top@3_accuracy</td><td>0.99698</td></tr><tr><td>epoch/val_accuracy</td><td>0.7908</td></tr><tr><td>epoch/val_loss</td><td>0.92421</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>0.9388</td></tr><tr><td>test_acc</td><td>0.7797</td></tr><tr><td>test_loss</td><td>0.68567</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">golden-sweep-1</strong> at: <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/njsfs215' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/njsfs215</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_115657-njsfs215\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 73x9wrfn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ta_layers: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation: light\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearlystopping_patience: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: [3, 3]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00025118864\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnormalization: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpooling: max\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treg_alpha: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\NEURAL PROJE\\wandb\\run-20240103_120256-73x9wrfn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/73x9wrfn' target=\"_blank\">misunderstood-sweep-2</a></strong> to <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/s4w5tx8e' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/s4w5tx8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/s4w5tx8e' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/s4w5tx8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/73x9wrfn' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/73x9wrfn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  5/704 [..............................] - ETA: 22s - loss: 2.8585 - accuracy: 0.1562 - top@3_accuracy: 0.3594WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0120s vs `on_train_batch_end` time: 0.0273s). Check your callbacks.\n",
      "704/704 [==============================] - 24s 33ms/step - loss: 1.7191 - accuracy: 0.3797 - top@3_accuracy: 0.7291 - val_loss: 1.2887 - val_accuracy: 0.5336 - val_top@3_accuracy: 0.8508\n",
      "Epoch 2/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 1.2531 - accuracy: 0.5540 - top@3_accuracy: 0.8528 - val_loss: 1.2308 - val_accuracy: 0.5816 - val_top@3_accuracy: 0.8654\n",
      "Epoch 3/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 1.0372 - accuracy: 0.6399 - top@3_accuracy: 0.8907 - val_loss: 1.2089 - val_accuracy: 0.6182 - val_top@3_accuracy: 0.8412\n",
      "Epoch 4/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.9037 - accuracy: 0.6949 - top@3_accuracy: 0.9114 - val_loss: 0.8746 - val_accuracy: 0.6998 - val_top@3_accuracy: 0.9256\n",
      "Epoch 5/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.8173 - accuracy: 0.7276 - top@3_accuracy: 0.9248 - val_loss: 0.8146 - val_accuracy: 0.7250 - val_top@3_accuracy: 0.9298\n",
      "Epoch 6/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.7454 - accuracy: 0.7532 - top@3_accuracy: 0.9357 - val_loss: 0.6968 - val_accuracy: 0.7598 - val_top@3_accuracy: 0.9464\n",
      "Epoch 7/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.6896 - accuracy: 0.7739 - top@3_accuracy: 0.9434 - val_loss: 0.6021 - val_accuracy: 0.7994 - val_top@3_accuracy: 0.9496\n",
      "Epoch 8/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.6459 - accuracy: 0.7889 - top@3_accuracy: 0.9499 - val_loss: 0.6737 - val_accuracy: 0.7834 - val_top@3_accuracy: 0.9444\n",
      "Epoch 9/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.6082 - accuracy: 0.8017 - top@3_accuracy: 0.9534 - val_loss: 0.6471 - val_accuracy: 0.7868 - val_top@3_accuracy: 0.9546\n",
      "Epoch 10/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.5653 - accuracy: 0.8147 - top@3_accuracy: 0.9582 - val_loss: 0.5753 - val_accuracy: 0.8102 - val_top@3_accuracy: 0.9594\n",
      "Epoch 11/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.5995 - accuracy: 0.8077 - top@3_accuracy: 0.9556 - val_loss: 0.5684 - val_accuracy: 0.8164 - val_top@3_accuracy: 0.9518\n",
      "Epoch 12/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.5141 - accuracy: 0.8338 - top@3_accuracy: 0.9636 - val_loss: 0.5571 - val_accuracy: 0.8208 - val_top@3_accuracy: 0.9570\n",
      "Epoch 13/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.5043 - accuracy: 0.8354 - top@3_accuracy: 0.9644 - val_loss: 0.6651 - val_accuracy: 0.7830 - val_top@3_accuracy: 0.9490\n",
      "Epoch 14/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.5308 - accuracy: 0.8309 - top@3_accuracy: 0.9637 - val_loss: 0.5919 - val_accuracy: 0.8056 - val_top@3_accuracy: 0.9514\n",
      "Epoch 15/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.4752 - accuracy: 0.8451 - top@3_accuracy: 0.9687 - val_loss: 0.5365 - val_accuracy: 0.8216 - val_top@3_accuracy: 0.9638\n",
      "Epoch 16/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.4312 - accuracy: 0.8570 - top@3_accuracy: 0.9729 - val_loss: 0.4902 - val_accuracy: 0.8386 - val_top@3_accuracy: 0.9660\n",
      "Epoch 17/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.6331 - accuracy: 0.8073 - top@3_accuracy: 0.9547 - val_loss: 0.4973 - val_accuracy: 0.8350 - val_top@3_accuracy: 0.9676\n",
      "Epoch 18/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.4383 - accuracy: 0.8564 - top@3_accuracy: 0.9721 - val_loss: 0.6694 - val_accuracy: 0.7850 - val_top@3_accuracy: 0.9508\n",
      "Epoch 19/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.4539 - accuracy: 0.8543 - top@3_accuracy: 0.9715 - val_loss: 0.5535 - val_accuracy: 0.8156 - val_top@3_accuracy: 0.9628\n",
      "Epoch 20/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.4679 - accuracy: 0.8520 - top@3_accuracy: 0.9700 - val_loss: 0.4646 - val_accuracy: 0.8482 - val_top@3_accuracy: 0.9656\n",
      "Epoch 21/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.3769 - accuracy: 0.8752 - top@3_accuracy: 0.9776 - val_loss: 0.4270 - val_accuracy: 0.8658 - val_top@3_accuracy: 0.9736\n",
      "Epoch 22/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.3609 - accuracy: 0.8807 - top@3_accuracy: 0.9796 - val_loss: 0.3767 - val_accuracy: 0.8770 - val_top@3_accuracy: 0.9762\n",
      "Epoch 23/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.3395 - accuracy: 0.8910 - top@3_accuracy: 0.9821 - val_loss: 1.2468 - val_accuracy: 0.6922 - val_top@3_accuracy: 0.9204\n",
      "Epoch 24/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.4032 - accuracy: 0.8733 - top@3_accuracy: 0.9769 - val_loss: 0.6726 - val_accuracy: 0.7828 - val_top@3_accuracy: 0.9394\n",
      "Epoch 25/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.5091 - accuracy: 0.8460 - top@3_accuracy: 0.9687 - val_loss: 0.4604 - val_accuracy: 0.8494 - val_top@3_accuracy: 0.9690\n",
      "Epoch 26/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.3997 - accuracy: 0.8732 - top@3_accuracy: 0.9773 - val_loss: 0.4565 - val_accuracy: 0.8494 - val_top@3_accuracy: 0.9684\n",
      "Epoch 27/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.3417 - accuracy: 0.8880 - top@3_accuracy: 0.9816 - val_loss: 0.4353 - val_accuracy: 0.8584 - val_top@3_accuracy: 0.9668\n",
      "Epoch 28/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.2916 - accuracy: 0.9034 - top@3_accuracy: 0.9853 - val_loss: 0.3654 - val_accuracy: 0.8798 - val_top@3_accuracy: 0.9764\n",
      "Epoch 29/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.3255 - accuracy: 0.8947 - top@3_accuracy: 0.9839 - val_loss: 0.4027 - val_accuracy: 0.8664 - val_top@3_accuracy: 0.9736\n",
      "Epoch 30/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.3950 - accuracy: 0.8779 - top@3_accuracy: 0.9776 - val_loss: 0.4361 - val_accuracy: 0.8574 - val_top@3_accuracy: 0.9694\n",
      "Epoch 31/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.3521 - accuracy: 0.8882 - top@3_accuracy: 0.9828 - val_loss: 0.4071 - val_accuracy: 0.8642 - val_top@3_accuracy: 0.9694\n",
      "Epoch 32/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.3223 - accuracy: 0.8963 - top@3_accuracy: 0.9832 - val_loss: 0.3981 - val_accuracy: 0.8770 - val_top@3_accuracy: 0.9724\n",
      "Epoch 33/100\n",
      "704/704 [==============================] - 22s 32ms/step - loss: 0.2914 - accuracy: 0.9065 - top@3_accuracy: 0.9862 - val_loss: 0.3729 - val_accuracy: 0.8762 - val_top@3_accuracy: 0.9754\n",
      "Epoch 34/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.2835 - accuracy: 0.9066 - top@3_accuracy: 0.9864 - val_loss: 0.3718 - val_accuracy: 0.8750 - val_top@3_accuracy: 0.9766\n",
      "Epoch 35/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.2469 - accuracy: 0.9178 - top@3_accuracy: 0.9887 - val_loss: 0.3657 - val_accuracy: 0.8838 - val_top@3_accuracy: 0.9794\n",
      "Epoch 36/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.2328 - accuracy: 0.9233 - top@3_accuracy: 0.9898 - val_loss: 0.3956 - val_accuracy: 0.8812 - val_top@3_accuracy: 0.9778\n",
      "Epoch 37/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.3349 - accuracy: 0.8969 - top@3_accuracy: 0.9844 - val_loss: 0.4768 - val_accuracy: 0.8516 - val_top@3_accuracy: 0.9630\n",
      "Epoch 38/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.3187 - accuracy: 0.8984 - top@3_accuracy: 0.9843 - val_loss: 1.1167 - val_accuracy: 0.6228 - val_top@3_accuracy: 0.8702\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3987 - accuracy: 0.8742 - top@3_accuracy: 0.9755\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2773768105ea48139c80d039106ca608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.031 MB uploaded\\r'), FloatProgress(value=0.03498739469962492, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁▃▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇███▇████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▆▅▄▄▃▃▃▃▃▃▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁</td></tr><tr><td>epoch/top@3_accuracy</td><td>▁▄▅▆▆▇▇▇▇▇▇▇▇▇▇█▇██▇████▇█████████████</td></tr><tr><td>epoch/val_accuracy</td><td>▁▂▃▄▅▆▆▆▆▇▇▇▆▆▇▇▇▆▇▇██▄▆▇▇▇██▇██████▇▃</td></tr><tr><td>epoch/val_loss</td><td>██▇▅▄▄▃▃▃▃▃▂▃▃▂▂▂▃▂▂▁▁█▃▂▂▂▁▁▂▁▁▁▁▁▁▂▇</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>▁▂▁▅▅▆▆▆▇▇▇▇▆▇▇▇▇▇▇▇██▅▆▇▇▇██▇▇█████▇▂</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.8798</td></tr><tr><td>best_val_loss</td><td>0.36539</td></tr><tr><td>epoch/accuracy</td><td>0.89842</td></tr><tr><td>epoch/epoch</td><td>37</td></tr><tr><td>epoch/learning_rate</td><td>0.00025</td></tr><tr><td>epoch/loss</td><td>0.31872</td></tr><tr><td>epoch/top@3_accuracy</td><td>0.98427</td></tr><tr><td>epoch/val_accuracy</td><td>0.6228</td></tr><tr><td>epoch/val_loss</td><td>1.11672</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>0.8702</td></tr><tr><td>test_acc</td><td>0.8742</td></tr><tr><td>test_loss</td><td>0.39871</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">misunderstood-sweep-2</strong> at: <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/73x9wrfn' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/73x9wrfn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_120256-73x9wrfn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: i4ksnhzy with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ta_layers: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation: heavy\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearlystopping_patience: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: [3, 3]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00025118864\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnormalization: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpooling: max\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treg_alpha: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\NEURAL PROJE\\wandb\\run-20240103_121727-i4ksnhzy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/i4ksnhzy' target=\"_blank\">wise-sweep-3</a></strong> to <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/s4w5tx8e' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/s4w5tx8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/s4w5tx8e' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/s4w5tx8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/i4ksnhzy' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/i4ksnhzy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  5/704 [..............................] - ETA: 23s - loss: 3.0937 - accuracy: 0.1063 - top@3_accuracy: 0.3031WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0124s vs `on_train_batch_end` time: 0.0213s). Check your callbacks.\n",
      "704/704 [==============================] - 24s 33ms/step - loss: 1.8729 - accuracy: 0.3126 - top@3_accuracy: 0.6684 - val_loss: 1.6007 - val_accuracy: 0.4124 - val_top@3_accuracy: 0.7570\n",
      "Epoch 2/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 1.4870 - accuracy: 0.4588 - top@3_accuracy: 0.7964 - val_loss: 1.5486 - val_accuracy: 0.4586 - val_top@3_accuracy: 0.7800\n",
      "Epoch 3/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 1.3153 - accuracy: 0.5336 - top@3_accuracy: 0.8374 - val_loss: 1.3373 - val_accuracy: 0.5216 - val_top@3_accuracy: 0.8252\n",
      "Epoch 4/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 1.1802 - accuracy: 0.5894 - top@3_accuracy: 0.8654 - val_loss: 1.0877 - val_accuracy: 0.6194 - val_top@3_accuracy: 0.8780\n",
      "Epoch 5/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 1.0740 - accuracy: 0.6322 - top@3_accuracy: 0.8834 - val_loss: 1.1803 - val_accuracy: 0.6206 - val_top@3_accuracy: 0.8884\n",
      "Epoch 6/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.9978 - accuracy: 0.6633 - top@3_accuracy: 0.8967 - val_loss: 0.9043 - val_accuracy: 0.6966 - val_top@3_accuracy: 0.9054\n",
      "Epoch 7/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.9559 - accuracy: 0.6800 - top@3_accuracy: 0.9032 - val_loss: 0.9009 - val_accuracy: 0.6976 - val_top@3_accuracy: 0.8954\n",
      "Epoch 8/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.8871 - accuracy: 0.7053 - top@3_accuracy: 0.9130 - val_loss: 1.0244 - val_accuracy: 0.6890 - val_top@3_accuracy: 0.8978\n",
      "Epoch 9/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.8415 - accuracy: 0.7249 - top@3_accuracy: 0.9196 - val_loss: 0.9907 - val_accuracy: 0.6786 - val_top@3_accuracy: 0.8912\n",
      "Epoch 10/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.7942 - accuracy: 0.7409 - top@3_accuracy: 0.9287 - val_loss: 0.9115 - val_accuracy: 0.6844 - val_top@3_accuracy: 0.8944\n",
      "Epoch 11/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.7476 - accuracy: 0.7526 - top@3_accuracy: 0.9332 - val_loss: 0.6245 - val_accuracy: 0.7914 - val_top@3_accuracy: 0.9516\n",
      "Epoch 12/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.7227 - accuracy: 0.7636 - top@3_accuracy: 0.9351 - val_loss: 0.7346 - val_accuracy: 0.7542 - val_top@3_accuracy: 0.9436\n",
      "Epoch 13/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.7086 - accuracy: 0.7673 - top@3_accuracy: 0.9401 - val_loss: 0.9272 - val_accuracy: 0.7094 - val_top@3_accuracy: 0.9168\n",
      "Epoch 14/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.7108 - accuracy: 0.7719 - top@3_accuracy: 0.9406 - val_loss: 0.7587 - val_accuracy: 0.7426 - val_top@3_accuracy: 0.9254\n",
      "Epoch 15/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.7097 - accuracy: 0.7718 - top@3_accuracy: 0.9417 - val_loss: 0.6782 - val_accuracy: 0.7800 - val_top@3_accuracy: 0.9330\n",
      "Epoch 16/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.7208 - accuracy: 0.7732 - top@3_accuracy: 0.9384 - val_loss: 0.6611 - val_accuracy: 0.7792 - val_top@3_accuracy: 0.9466\n",
      "Epoch 17/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.6175 - accuracy: 0.7984 - top@3_accuracy: 0.9492 - val_loss: 0.6866 - val_accuracy: 0.7794 - val_top@3_accuracy: 0.9464\n",
      "Epoch 18/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.6795 - accuracy: 0.7827 - top@3_accuracy: 0.9442 - val_loss: 0.7020 - val_accuracy: 0.7624 - val_top@3_accuracy: 0.9418\n",
      "Epoch 19/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.5740 - accuracy: 0.8131 - top@3_accuracy: 0.9565 - val_loss: 0.6096 - val_accuracy: 0.7980 - val_top@3_accuracy: 0.9458\n",
      "Epoch 20/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.5936 - accuracy: 0.8100 - top@3_accuracy: 0.9545 - val_loss: 0.5995 - val_accuracy: 0.8024 - val_top@3_accuracy: 0.9484\n",
      "Epoch 21/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.6050 - accuracy: 0.8070 - top@3_accuracy: 0.9548 - val_loss: 0.6819 - val_accuracy: 0.7730 - val_top@3_accuracy: 0.9460\n",
      "Epoch 22/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.6048 - accuracy: 0.8038 - top@3_accuracy: 0.9539 - val_loss: 0.5611 - val_accuracy: 0.8122 - val_top@3_accuracy: 0.9542\n",
      "Epoch 23/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.5592 - accuracy: 0.8156 - top@3_accuracy: 0.9587 - val_loss: 0.5607 - val_accuracy: 0.8088 - val_top@3_accuracy: 0.9610\n",
      "Epoch 24/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.5148 - accuracy: 0.8312 - top@3_accuracy: 0.9632 - val_loss: 0.4787 - val_accuracy: 0.8388 - val_top@3_accuracy: 0.9648\n",
      "Epoch 25/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.6968 - accuracy: 0.7792 - top@3_accuracy: 0.9391 - val_loss: 0.5897 - val_accuracy: 0.8032 - val_top@3_accuracy: 0.9522\n",
      "Epoch 26/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.5628 - accuracy: 0.8136 - top@3_accuracy: 0.9592 - val_loss: 0.5063 - val_accuracy: 0.8326 - val_top@3_accuracy: 0.9634\n",
      "Epoch 27/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.5171 - accuracy: 0.8297 - top@3_accuracy: 0.9646 - val_loss: 0.4832 - val_accuracy: 0.8446 - val_top@3_accuracy: 0.9680\n",
      "Epoch 28/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.6383 - accuracy: 0.7939 - top@3_accuracy: 0.9488 - val_loss: 0.7084 - val_accuracy: 0.7644 - val_top@3_accuracy: 0.9316\n",
      "Epoch 29/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.5297 - accuracy: 0.8269 - top@3_accuracy: 0.9618 - val_loss: 0.4894 - val_accuracy: 0.8352 - val_top@3_accuracy: 0.9656\n",
      "Epoch 30/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.4674 - accuracy: 0.8443 - top@3_accuracy: 0.9677 - val_loss: 0.4954 - val_accuracy: 0.8340 - val_top@3_accuracy: 0.9636\n",
      "Epoch 31/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.4456 - accuracy: 0.8515 - top@3_accuracy: 0.9697 - val_loss: 0.4373 - val_accuracy: 0.8548 - val_top@3_accuracy: 0.9678\n",
      "Epoch 32/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.4949 - accuracy: 0.8409 - top@3_accuracy: 0.9655 - val_loss: 0.4034 - val_accuracy: 0.8618 - val_top@3_accuracy: 0.9734\n",
      "Epoch 33/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.4463 - accuracy: 0.8510 - top@3_accuracy: 0.9706 - val_loss: 0.5547 - val_accuracy: 0.8138 - val_top@3_accuracy: 0.9606\n",
      "Epoch 34/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.5430 - accuracy: 0.8241 - top@3_accuracy: 0.9637 - val_loss: 0.4888 - val_accuracy: 0.8362 - val_top@3_accuracy: 0.9692\n",
      "Epoch 35/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.5046 - accuracy: 0.8369 - top@3_accuracy: 0.9648 - val_loss: 1.2831 - val_accuracy: 0.6068 - val_top@3_accuracy: 0.7978\n",
      "Epoch 36/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.4961 - accuracy: 0.8350 - top@3_accuracy: 0.9655 - val_loss: 0.3945 - val_accuracy: 0.8698 - val_top@3_accuracy: 0.9720\n",
      "Epoch 37/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.4201 - accuracy: 0.8607 - top@3_accuracy: 0.9730 - val_loss: 0.3925 - val_accuracy: 0.8654 - val_top@3_accuracy: 0.9740\n",
      "Epoch 38/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.3959 - accuracy: 0.8690 - top@3_accuracy: 0.9759 - val_loss: 0.4293 - val_accuracy: 0.8556 - val_top@3_accuracy: 0.9708\n",
      "Epoch 39/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.5121 - accuracy: 0.8343 - top@3_accuracy: 0.9650 - val_loss: 0.4646 - val_accuracy: 0.8458 - val_top@3_accuracy: 0.9682\n",
      "Epoch 40/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.7130 - accuracy: 0.7724 - top@3_accuracy: 0.9440 - val_loss: 0.6942 - val_accuracy: 0.7660 - val_top@3_accuracy: 0.9336\n",
      "Epoch 41/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.4763 - accuracy: 0.8442 - top@3_accuracy: 0.9688 - val_loss: 0.4847 - val_accuracy: 0.8434 - val_top@3_accuracy: 0.9696\n",
      "Epoch 42/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.4155 - accuracy: 0.8607 - top@3_accuracy: 0.9739 - val_loss: 0.4792 - val_accuracy: 0.8380 - val_top@3_accuracy: 0.9658\n",
      "Epoch 43/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.4081 - accuracy: 0.8664 - top@3_accuracy: 0.9748 - val_loss: 0.3999 - val_accuracy: 0.8660 - val_top@3_accuracy: 0.9718\n",
      "Epoch 44/100\n",
      "704/704 [==============================] - 24s 33ms/step - loss: 0.3729 - accuracy: 0.8776 - top@3_accuracy: 0.9782 - val_loss: 0.4095 - val_accuracy: 0.8656 - val_top@3_accuracy: 0.9730\n",
      "Epoch 45/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.5482 - accuracy: 0.8226 - top@3_accuracy: 0.9609 - val_loss: 0.4403 - val_accuracy: 0.8592 - val_top@3_accuracy: 0.9706\n",
      "Epoch 46/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.4203 - accuracy: 0.8602 - top@3_accuracy: 0.9748 - val_loss: 0.4642 - val_accuracy: 0.8558 - val_top@3_accuracy: 0.9648\n",
      "Epoch 47/100\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.3903 - accuracy: 0.8704 - top@3_accuracy: 0.9766 - val_loss: 0.4614 - val_accuracy: 0.8526 - val_top@3_accuracy: 0.9716\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.4376 - accuracy: 0.8515 - top@3_accuracy: 0.9715\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d281addc54f461ea96aca0bff505b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁▃▄▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███▇▇▇██▇████▇█</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▆▅▅▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▃▂▂▂▁▁▂▂▂▂▁▁▂▁▁▁▁▂▁</td></tr><tr><td>epoch/top@3_accuracy</td><td>▁▄▅▅▆▆▇▇▇▇▇▇▇▇▇▇█▇▇██▇█▇████████████████</td></tr><tr><td>epoch/val_accuracy</td><td>▁▂▃▄▄▅▅▅▅▇▆▆▇▇▇▆▇▇▇▇█▇▇▆▇▇██▇▄██████████</td></tr><tr><td>epoch/val_loss</td><td>██▆▅▆▄▅▄▄▂▃▄▃▃▃▃▂▃▂▂▁▂▂▃▂▂▁▁▂▆▁▁▁▁▂▂▁▁▁▁</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>▁▂▃▅▅▆▆▅▅▇▇▆▇▇▇▇▇▇▇██▇█▇█████▂██████████</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.8654</td></tr><tr><td>best_val_loss</td><td>0.39255</td></tr><tr><td>epoch/accuracy</td><td>0.87038</td></tr><tr><td>epoch/epoch</td><td>46</td></tr><tr><td>epoch/learning_rate</td><td>0.00025</td></tr><tr><td>epoch/loss</td><td>0.39028</td></tr><tr><td>epoch/top@3_accuracy</td><td>0.97658</td></tr><tr><td>epoch/val_accuracy</td><td>0.8526</td></tr><tr><td>epoch/val_loss</td><td>0.46139</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>0.9716</td></tr><tr><td>test_acc</td><td>0.8515</td></tr><tr><td>test_loss</td><td>0.43758</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wise-sweep-3</strong> at: <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/i4ksnhzy' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/i4ksnhzy</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_121727-i4ksnhzy\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
