{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and Biases related imports\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (50000, 32, 32, 3)\n",
      "Train Labels Shape: (50000, 10)\n",
      "Test Data Shape: (10000, 32, 32, 3)\n",
      "Test Labels Shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def load_cifar10_batch(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        batch = pickle.load(file, encoding='bytes')\n",
    "    return batch\n",
    "\n",
    "def load_cifar10_data(folder_path):\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        batch_file = f\"{folder_path}/data_batch_{i}\"\n",
    "        batch = load_cifar10_batch(batch_file)\n",
    "        train_data.append(batch[b'data'])\n",
    "        train_labels.extend(batch[b'labels'])\n",
    "\n",
    "    test_batch_file = f\"{folder_path}/test_batch\"\n",
    "    test_batch = load_cifar10_batch(test_batch_file)\n",
    "    test_data = test_batch[b'data']\n",
    "    test_labels = test_batch[b'labels']\n",
    "\n",
    "    train_data = np.vstack(train_data)\n",
    "    train_labels = np.array(train_labels)\n",
    "    test_labels = np.array(test_labels)\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "def preprocess_data(train_data, train_labels, test_data, test_labels):\n",
    "    train_data = train_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "    test_data = test_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "\n",
    "    train_labels_onehot = to_categorical(train_labels)\n",
    "    test_labels_onehot = to_categorical(test_labels)\n",
    "\n",
    "    return train_data, train_labels_onehot, test_data, test_labels_onehot\n",
    "\n",
    "cifar10_folder = 'cifar-10-batches-py'\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_cifar10_data(cifar10_folder)\n",
    "\n",
    "x_train, y_train, x_test, y_test = preprocess_data(\n",
    "    train_data, train_labels, test_data, test_labels\n",
    ")\n",
    "\n",
    "print(\"Train Data Shape:\", x_train.shape)\n",
    "print(\"Train Labels Shape:\", y_train.shape)\n",
    "print(\"Test Data Shape:\", x_test.shape)\n",
    "print(\"Test Labels Shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlihem\u001b[0m (\u001b[33mtakim\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.environ['WANDB_NOTEBOOK_NAME'] = 'RUN_1'\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'val_loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    'normalization': {\n",
    "          'values': [False, True]\n",
    "        }\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "parameters_dict.update({\n",
    "    'earlystopping_patience': {\n",
    "        'value': 10},\n",
    "    'epochs': {\n",
    "        'value': 100},\n",
    "    'learning_rate': {\n",
    "        'value': 0.00025118864\n",
    "        },\n",
    "    'batch_size': {\n",
    "          'value': 64\n",
    "        },\n",
    "    'kernel_size': {\n",
    "        'value': (3, 3)\n",
    "        },\n",
    "    'dropout': {\n",
    "          'value': True\n",
    "        },\n",
    "    'pooling': {\n",
    "          'value': 'max'\n",
    "        },\n",
    "    'batchnorm': {\n",
    "          'value': True\n",
    "        },\n",
    "    'a_layers': {\n",
    "          'value': 16\n",
    "        },\n",
    "    'reg_alpha': {\n",
    "        'value': 0\n",
    "        },\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'grid',\n",
      " 'metric': {'goal': 'minimize', 'name': 'val_loss'},\n",
      " 'parameters': {'a_layers': {'value': 16},\n",
      "                'batch_size': {'value': 64},\n",
      "                'batchnorm': {'value': True},\n",
      "                'dropout': {'value': True},\n",
      "                'earlystopping_patience': {'value': 10},\n",
      "                'epochs': {'value': 100},\n",
      "                'kernel_size': {'value': (3, 3)},\n",
      "                'learning_rate': {'value': 0.00025118864},\n",
      "                'normalization': {'values': [False, True]},\n",
      "                'pooling': {'value': 'max'},\n",
      "                'reg_alpha': {'value': 0}}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 8anw4qlh\n",
      "Sweep URL: https://wandb.ai/takim/CIFAR-10_Classification/sweeps/8anw4qlh\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"CIFAR-10_Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_model(kernel_size, dropout, pooling, batchnorm, n_layers, reg_alpha):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size, activation='relu', padding='same', input_shape=(32, 32, 3), kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(256, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(256, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(256, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    if n_layers == 19:\n",
    "        model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "        model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "        model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "        if batchnorm:\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=4096, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if dropout:\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=4096, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if dropout:\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def train(config = None):\n",
    "    with wandb.init(config=config):\n",
    "\n",
    "        config = wandb.config\n",
    "\n",
    "        x_train_to_use = (x_train.astype('float32') / 255) if config['normalization'] else x_train\n",
    "        x_test_to_use = (x_test.astype('float32') / 255) if config['normalization'] else x_test\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        model = create_model(config[\"kernel_size\"], config[\"dropout\"], config[\"pooling\"], config[\"batchnorm\"], config[\"a_layers\"], config[\"reg_alpha\"])\n",
    "        model.compile(\n",
    "            optimizer = Adam(learning_rate=config[\"learning_rate\"]),\n",
    "            loss = \"categorical_crossentropy\",\n",
    "            metrics = [\"accuracy\", tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top@3_accuracy')]\n",
    "        )\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                    patience=config[\"earlystopping_patience\"],\n",
    "                                    restore_best_weights=True)\n",
    "\n",
    "        history = model.fit(x_train_to_use, y_train,\n",
    "                                    epochs=config[\"epochs\"],\n",
    "                                    batch_size=config[\"batch_size\"],\n",
    "                                    validation_split=0.1,\n",
    "                                    callbacks=[\n",
    "                                        WandbMetricsLogger(log_freq='epoch'),\n",
    "                                        early_stopping\n",
    "                                    ], verbose=1\n",
    "                                    )\n",
    "        \n",
    "        test_stats = model.evaluate(x_test_to_use, y_test)\n",
    "        wandb.log({\"test_loss\": test_stats[0]})\n",
    "        wandb.log({\"test_acc\": test_stats[1]})\n",
    "\n",
    "        val_loss_history = history.history['val_loss']\n",
    "        val_acc_history = history.history['val_accuracy']\n",
    "\n",
    "        best_epoch_num = -1 if (len(val_loss_history) == 100 or len(val_loss_history) <= 10) else (len(val_loss_history) - 11)\n",
    "\n",
    "        wandb.log({\"best_val_loss\": val_loss_history[best_epoch_num]})\n",
    "        wandb.log({\"best_val_acc\": val_acc_history[best_epoch_num]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5q4gtz4g with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ta_layers: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearlystopping_patience: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: [3, 3]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00025118864\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnormalization: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpooling: max\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treg_alpha: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\NEURAL PROJE\\wandb\\run-20240103_112021-5q4gtz4g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/5q4gtz4g' target=\"_blank\">frosty-sweep-1</a></strong> to <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/8anw4qlh' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/8anw4qlh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/8anw4qlh' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/8anw4qlh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/5q4gtz4g' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/5q4gtz4g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  5/704 [..............................] - ETA: 23s - loss: 2.9774 - accuracy: 0.1406 - top@3_accuracy: 0.3125  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0124s vs `on_train_batch_end` time: 0.0173s). Check your callbacks.\n",
      "704/704 [==============================] - 29s 34ms/step - loss: 1.5714 - accuracy: 0.4293 - top@3_accuracy: 0.7640 - val_loss: 1.4028 - val_accuracy: 0.4978 - val_top@3_accuracy: 0.8052\n",
      "Epoch 2/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 1.0313 - accuracy: 0.6394 - top@3_accuracy: 0.8912 - val_loss: 1.3662 - val_accuracy: 0.5898 - val_top@3_accuracy: 0.8366\n",
      "Epoch 3/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.7968 - accuracy: 0.7314 - top@3_accuracy: 0.9268 - val_loss: 0.8170 - val_accuracy: 0.7236 - val_top@3_accuracy: 0.9272\n",
      "Epoch 4/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.6548 - accuracy: 0.7834 - top@3_accuracy: 0.9464 - val_loss: 0.8100 - val_accuracy: 0.7398 - val_top@3_accuracy: 0.9312\n",
      "Epoch 5/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.5246 - accuracy: 0.8287 - top@3_accuracy: 0.9617 - val_loss: 0.7562 - val_accuracy: 0.7482 - val_top@3_accuracy: 0.9416\n",
      "Epoch 6/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.4379 - accuracy: 0.8563 - top@3_accuracy: 0.9712 - val_loss: 0.6150 - val_accuracy: 0.7996 - val_top@3_accuracy: 0.9474\n",
      "Epoch 7/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.3444 - accuracy: 0.8903 - top@3_accuracy: 0.9807 - val_loss: 0.6574 - val_accuracy: 0.7996 - val_top@3_accuracy: 0.9520\n",
      "Epoch 8/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.2888 - accuracy: 0.9083 - top@3_accuracy: 0.9861 - val_loss: 0.6986 - val_accuracy: 0.8008 - val_top@3_accuracy: 0.9408\n",
      "Epoch 9/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.2340 - accuracy: 0.9269 - top@3_accuracy: 0.9895 - val_loss: 0.6754 - val_accuracy: 0.8032 - val_top@3_accuracy: 0.9464\n",
      "Epoch 10/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.1954 - accuracy: 0.9393 - top@3_accuracy: 0.9920 - val_loss: 0.7506 - val_accuracy: 0.8038 - val_top@3_accuracy: 0.9524\n",
      "Epoch 11/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.1500 - accuracy: 0.9516 - top@3_accuracy: 0.9949 - val_loss: 0.7596 - val_accuracy: 0.8096 - val_top@3_accuracy: 0.9448\n",
      "Epoch 12/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.1669 - accuracy: 0.9493 - top@3_accuracy: 0.9946 - val_loss: 0.7500 - val_accuracy: 0.8112 - val_top@3_accuracy: 0.9526\n",
      "Epoch 13/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.3361 - accuracy: 0.9064 - top@3_accuracy: 0.9846 - val_loss: 0.6745 - val_accuracy: 0.8118 - val_top@3_accuracy: 0.9478\n",
      "Epoch 14/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.1193 - accuracy: 0.9631 - top@3_accuracy: 0.9962 - val_loss: 0.7713 - val_accuracy: 0.8284 - val_top@3_accuracy: 0.9566\n",
      "Epoch 15/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.0667 - accuracy: 0.9798 - top@3_accuracy: 0.9986 - val_loss: 0.8028 - val_accuracy: 0.8214 - val_top@3_accuracy: 0.9518\n",
      "Epoch 16/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.0748 - accuracy: 0.9772 - top@3_accuracy: 0.9986 - val_loss: 0.8404 - val_accuracy: 0.8220 - val_top@3_accuracy: 0.9458\n",
      "313/313 [==============================] - 3s 7ms/step - loss: 0.6494 - accuracy: 0.7933 - top@3_accuracy: 0.9413\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404ce83a77de475c8ef724cd30acb826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁▄▅▆▆▆▇▇▇▇██▇███</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▅▄▄▃▃▂▂▂▂▁▁▂▁▁▁</td></tr><tr><td>epoch/top@3_accuracy</td><td>▁▅▆▆▇▇▇█████████</td></tr><tr><td>epoch/val_accuracy</td><td>▁▃▆▆▆▇▇▇▇▇██████</td></tr><tr><td>epoch/val_loss</td><td>██▃▃▂▁▁▂▂▂▂▂▂▂▃▃</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>▁▂▇▇▇██▇██▇█████</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.7996</td></tr><tr><td>best_val_loss</td><td>0.61499</td></tr><tr><td>epoch/accuracy</td><td>0.97724</td></tr><tr><td>epoch/epoch</td><td>15</td></tr><tr><td>epoch/learning_rate</td><td>0.00025</td></tr><tr><td>epoch/loss</td><td>0.07481</td></tr><tr><td>epoch/top@3_accuracy</td><td>0.99858</td></tr><tr><td>epoch/val_accuracy</td><td>0.822</td></tr><tr><td>epoch/val_loss</td><td>0.84041</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>0.9458</td></tr><tr><td>test_acc</td><td>0.7933</td></tr><tr><td>test_loss</td><td>0.64944</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">frosty-sweep-1</strong> at: <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/5q4gtz4g' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/5q4gtz4g</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_112021-5q4gtz4g\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3m47tai5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ta_layers: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearlystopping_patience: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: [3, 3]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00025118864\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnormalization: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpooling: max\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treg_alpha: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\NEURAL PROJE\\wandb\\run-20240103_112706-3m47tai5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/3m47tai5' target=\"_blank\">hopeful-sweep-2</a></strong> to <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/8anw4qlh' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/8anw4qlh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/8anw4qlh' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/8anw4qlh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/3m47tai5' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/3m47tai5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  5/704 [..............................] - ETA: 24s - loss: 3.4304 - accuracy: 0.1187 - top@3_accuracy: 0.3000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0130s vs `on_train_batch_end` time: 0.0215s). Check your callbacks.\n",
      "704/704 [==============================] - 24s 33ms/step - loss: 1.5322 - accuracy: 0.4461 - top@3_accuracy: 0.7750 - val_loss: 1.2495 - val_accuracy: 0.5674 - val_top@3_accuracy: 0.8312\n",
      "Epoch 2/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 1.0140 - accuracy: 0.6456 - top@3_accuracy: 0.8943 - val_loss: 1.2967 - val_accuracy: 0.5744 - val_top@3_accuracy: 0.8396\n",
      "Epoch 3/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.8005 - accuracy: 0.7291 - top@3_accuracy: 0.9292 - val_loss: 0.8822 - val_accuracy: 0.7016 - val_top@3_accuracy: 0.9344\n",
      "Epoch 4/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.6407 - accuracy: 0.7862 - top@3_accuracy: 0.9494 - val_loss: 0.7812 - val_accuracy: 0.7476 - val_top@3_accuracy: 0.9354\n",
      "Epoch 5/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.5188 - accuracy: 0.8294 - top@3_accuracy: 0.9633 - val_loss: 0.7963 - val_accuracy: 0.7412 - val_top@3_accuracy: 0.9462\n",
      "Epoch 6/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.4290 - accuracy: 0.8605 - top@3_accuracy: 0.9725 - val_loss: 0.7325 - val_accuracy: 0.7604 - val_top@3_accuracy: 0.9370\n",
      "Epoch 7/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.3486 - accuracy: 0.8870 - top@3_accuracy: 0.9811 - val_loss: 0.7048 - val_accuracy: 0.7864 - val_top@3_accuracy: 0.9356\n",
      "Epoch 8/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.2677 - accuracy: 0.9130 - top@3_accuracy: 0.9868 - val_loss: 0.7153 - val_accuracy: 0.7970 - val_top@3_accuracy: 0.9454\n",
      "Epoch 9/100\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.2693 - accuracy: 0.9163 - top@3_accuracy: 0.9875 - val_loss: 0.8033 - val_accuracy: 0.7878 - val_top@3_accuracy: 0.9456\n",
      "Epoch 10/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.1972 - accuracy: 0.9393 - top@3_accuracy: 0.9919 - val_loss: 0.7076 - val_accuracy: 0.8234 - val_top@3_accuracy: 0.9564\n",
      "Epoch 11/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.2782 - accuracy: 0.9188 - top@3_accuracy: 0.9869 - val_loss: 0.6861 - val_accuracy: 0.7856 - val_top@3_accuracy: 0.9478\n",
      "Epoch 12/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.1776 - accuracy: 0.9441 - top@3_accuracy: 0.9938 - val_loss: 0.8522 - val_accuracy: 0.7920 - val_top@3_accuracy: 0.9480\n",
      "Epoch 13/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.1148 - accuracy: 0.9648 - top@3_accuracy: 0.9966 - val_loss: 0.7573 - val_accuracy: 0.8260 - val_top@3_accuracy: 0.9566\n",
      "Epoch 14/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.1085 - accuracy: 0.9668 - top@3_accuracy: 0.9971 - val_loss: 0.8429 - val_accuracy: 0.8076 - val_top@3_accuracy: 0.9428\n",
      "Epoch 15/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.1900 - accuracy: 0.9464 - top@3_accuracy: 0.9938 - val_loss: 2.5092 - val_accuracy: 0.5192 - val_top@3_accuracy: 0.7762\n",
      "Epoch 16/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.4438 - accuracy: 0.8716 - top@3_accuracy: 0.9722 - val_loss: 0.7531 - val_accuracy: 0.8204 - val_top@3_accuracy: 0.9560\n",
      "Epoch 17/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.0684 - accuracy: 0.9781 - top@3_accuracy: 0.9988 - val_loss: 0.8678 - val_accuracy: 0.8300 - val_top@3_accuracy: 0.9550\n",
      "Epoch 18/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.0481 - accuracy: 0.9850 - top@3_accuracy: 0.9994 - val_loss: 0.8839 - val_accuracy: 0.8248 - val_top@3_accuracy: 0.9568\n",
      "Epoch 19/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.1953 - accuracy: 0.9449 - top@3_accuracy: 0.9926 - val_loss: 0.8320 - val_accuracy: 0.8210 - val_top@3_accuracy: 0.9566\n",
      "Epoch 20/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.0565 - accuracy: 0.9823 - top@3_accuracy: 0.9992 - val_loss: 0.9018 - val_accuracy: 0.8262 - val_top@3_accuracy: 0.9492\n",
      "Epoch 21/100\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.0499 - accuracy: 0.9845 - top@3_accuracy: 0.9993 - val_loss: 0.9015 - val_accuracy: 0.8216 - val_top@3_accuracy: 0.9544\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.7401 - accuracy: 0.7752 - top@3_accuracy: 0.9365\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c316e7cd084bcf810cfd235cfb3750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁▄▅▅▆▆▇▇▇▇▇▇██▇▇██▇██</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▆▅▄▃▃▂▂▂▂▂▂▁▁▂▃▁▁▂▁▁</td></tr><tr><td>epoch/top@3_accuracy</td><td>▁▅▆▆▇▇▇████████▇█████</td></tr><tr><td>epoch/val_accuracy</td><td>▂▂▅▆▆▆▇▇▇█▇▇█▇▁██████</td></tr><tr><td>epoch/val_loss</td><td>▃▃▂▁▁▁▁▁▁▁▁▂▁▂█▁▂▂▂▂▂</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>▃▃▇▇█▇▇██████▇▁██████</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.7856</td></tr><tr><td>best_val_loss</td><td>0.6861</td></tr><tr><td>epoch/accuracy</td><td>0.98449</td></tr><tr><td>epoch/epoch</td><td>20</td></tr><tr><td>epoch/learning_rate</td><td>0.00025</td></tr><tr><td>epoch/loss</td><td>0.04993</td></tr><tr><td>epoch/top@3_accuracy</td><td>0.99929</td></tr><tr><td>epoch/val_accuracy</td><td>0.8216</td></tr><tr><td>epoch/val_loss</td><td>0.90154</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>0.9544</td></tr><tr><td>test_acc</td><td>0.7752</td></tr><tr><td>test_loss</td><td>0.74008</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hopeful-sweep-2</strong> at: <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/3m47tai5' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/3m47tai5</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_112706-3m47tai5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
