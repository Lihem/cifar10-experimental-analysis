{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and Biases related imports\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (50000, 32, 32, 3)\n",
      "Train Labels Shape: (50000, 10)\n",
      "Test Data Shape: (10000, 32, 32, 3)\n",
      "Test Labels Shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def load_cifar10_batch(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        batch = pickle.load(file, encoding='bytes')\n",
    "    return batch\n",
    "\n",
    "def load_cifar10_data(folder_path):\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        batch_file = f\"{folder_path}/data_batch_{i}\"\n",
    "        batch = load_cifar10_batch(batch_file)\n",
    "        train_data.append(batch[b'data'])\n",
    "        train_labels.extend(batch[b'labels'])\n",
    "\n",
    "    test_batch_file = f\"{folder_path}/test_batch\"\n",
    "    test_batch = load_cifar10_batch(test_batch_file)\n",
    "    test_data = test_batch[b'data']\n",
    "    test_labels = test_batch[b'labels']\n",
    "\n",
    "    train_data = np.vstack(train_data)\n",
    "    train_labels = np.array(train_labels)\n",
    "    test_labels = np.array(test_labels)\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "def preprocess_data(train_data, train_labels, test_data, test_labels):\n",
    "    train_data = train_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "    test_data = test_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "\n",
    "    train_labels_onehot = to_categorical(train_labels)\n",
    "    test_labels_onehot = to_categorical(test_labels)\n",
    "\n",
    "    return train_data, train_labels_onehot, test_data, test_labels_onehot\n",
    "\n",
    "cifar10_folder = 'cifar-10-batches-py'\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_cifar10_data(cifar10_folder)\n",
    "\n",
    "x_train, y_train, x_test, y_test = preprocess_data(\n",
    "    train_data, train_labels, test_data, test_labels\n",
    ")\n",
    "\n",
    "print(\"Train Data Shape:\", x_train.shape)\n",
    "print(\"Train Labels Shape:\", y_train.shape)\n",
    "print(\"Test Data Shape:\", x_test.shape)\n",
    "print(\"Test Labels Shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlihem\u001b[0m (\u001b[33mtakim\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.environ['WANDB_NOTEBOOK_NAME'] = 'RUN_1'\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'val_loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    'augmentation': {\n",
    "          'values': ['none', 'light', 'heavy']\n",
    "        }\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "parameters_dict.update({\n",
    "    'earlystopping_patience': {\n",
    "        'value': 10},\n",
    "    'epochs': {\n",
    "        'value': 100},\n",
    "    'learning_rate': {\n",
    "        'value': 0.000063\n",
    "        },\n",
    "    'batch_size': {\n",
    "          'value': 64\n",
    "        },\n",
    "    'dropout': {\n",
    "          'value': True\n",
    "        },\n",
    "    'batchnorm': {\n",
    "          'value': True\n",
    "        },\n",
    "    'regularization': {\n",
    "          'value': False\n",
    "        },\n",
    "    'normalization': {\n",
    "        'value': True}\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'grid',\n",
      " 'metric': {'goal': 'minimize', 'name': 'val_loss'},\n",
      " 'parameters': {'augmentation': {'values': ['none', 'light', 'heavy']},\n",
      "                'batch_size': {'value': 64},\n",
      "                'batchnorm': {'value': True},\n",
      "                'dropout': {'value': True},\n",
      "                'earlystopping_patience': {'value': 10},\n",
      "                'epochs': {'value': 100},\n",
      "                'learning_rate': {'value': 6.3e-05},\n",
      "                'normalization': {'value': True},\n",
      "                'regularization': {'value': False}}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ncvawp6t\n",
      "Sweep URL: https://wandb.ai/takim/CIFAR-10_Classification/sweeps/ncvawp6t\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"CIFAR-10_Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the AlexNet architecture\n",
    "def create_model(dropout, batchnorm, regularization):\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    if regularization:\n",
    "        model.add(tf.keras.layers.Conv2D(filters=96, kernel_size=(11, 11), strides=(2, 2), activation='relu', input_shape=(32, 32, 3), kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Conv2D(filters=96, kernel_size=(11, 11), strides=(2, 2), activation='relu', input_shape=(32, 32, 3)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(1, 1)))\n",
    "\n",
    "\n",
    "    if regularization:\n",
    "        model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding=\"same\"))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(1, 1)))\n",
    "\n",
    "\n",
    "    if regularization:\n",
    "        model.add(tf.keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "        model.add(tf.keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "        model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"same\"))\n",
    "        model.add(tf.keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"same\"))\n",
    "        model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"same\"))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(1, 1)))\n",
    "\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=4096, activation='relu'))\n",
    "    if dropout:\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=4096, activation='relu'))\n",
    "    if dropout:\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train(config = None):\n",
    "    with wandb.init(config=config):\n",
    "\n",
    "        config = wandb.config\n",
    "\n",
    "        do_normalization = config['normalization']\n",
    "        do_augmentation = config['augmentation'] != 'none'\n",
    "\n",
    "        x_train_to_use = (x_train.astype('float32') / 255) if do_normalization else x_train\n",
    "        x_test_to_use = (x_test.astype('float32') / 255) if do_normalization else x_test\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        model = create_model(config[\"dropout\"], config[\"batchnorm\"], config[\"regularization\"])\n",
    "        model.compile(\n",
    "            optimizer = Adam(learning_rate=config[\"learning_rate\"]),\n",
    "            loss = \"categorical_crossentropy\",\n",
    "            metrics = [\"accuracy\", tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top@3_accuracy')]\n",
    "        )\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                    patience=config[\"earlystopping_patience\"],\n",
    "                                    restore_best_weights=True)\n",
    "\n",
    "        if not do_augmentation:\n",
    "            history = model.fit(x_train_to_use, y_train,\n",
    "                                epochs=config[\"epochs\"],\n",
    "                                batch_size=config[\"batch_size\"],\n",
    "                                validation_split=0.1,\n",
    "                                callbacks=[\n",
    "                                    WandbMetricsLogger(log_freq='epoch'),\n",
    "                                    early_stopping\n",
    "                                ], verbose=1\n",
    "                                )\n",
    "        else:\n",
    "            if config['augmentation'] == 'light':\n",
    "                datagen = ImageDataGenerator(\n",
    "                    rotation_range=20,\n",
    "                    horizontal_flip=True,\n",
    "                    width_shift_range=0.1,\n",
    "                    height_shift_range=0.1,\n",
    "                    fill_mode='nearest'\n",
    "                )\n",
    "            else:\n",
    "                datagen = ImageDataGenerator(\n",
    "                    rotation_range=40,\n",
    "                    horizontal_flip=True,\n",
    "                    width_shift_range=0.2,\n",
    "                    height_shift_range=0.2,\n",
    "                    shear_range=0.1,\n",
    "                    zoom_range=0.1,\n",
    "                    fill_mode='nearest'\n",
    "                )\n",
    "\n",
    "            x_tr, x_vl, y_tr, y_vl = train_test_split(x_train_to_use, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "            train_datagen = datagen.flow(x_tr, y_tr, batch_size=config[\"batch_size\"])\n",
    "            history = model.fit(train_datagen,\n",
    "                                epochs=config[\"epochs\"],\n",
    "                                batch_size=config[\"batch_size\"],\n",
    "                                validation_data=(x_vl, y_vl),\n",
    "                                callbacks=[\n",
    "                                    WandbMetricsLogger(log_freq='epoch'),\n",
    "                                    early_stopping\n",
    "                                ], verbose=1\n",
    "                                )\n",
    "            \n",
    "        \n",
    "        test_stats = model.evaluate(x_test_to_use, y_test)\n",
    "        wandb.log({\"test_loss\": test_stats[0]})\n",
    "        wandb.log({\"test_acc\": test_stats[1]})\n",
    "\n",
    "        val_loss_history = history.history['val_loss']\n",
    "        val_acc_history = history.history['val_accuracy']\n",
    "\n",
    "        best_epoch_num = -1 if (len(val_loss_history) == 100 or len(val_loss_history) <= 10) else (len(val_loss_history) - 11)\n",
    "\n",
    "        wandb.log({\"best_val_loss\": val_loss_history[best_epoch_num]})\n",
    "        wandb.log({\"best_val_acc\": val_acc_history[best_epoch_num]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: aw54os4k with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation: none\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearlystopping_patience: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 6.3e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnormalization: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tregularization: False\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\NEURAL PROJE\\wandb\\run-20240103_131836-aw54os4k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/aw54os4k' target=\"_blank\">unique-sweep-1</a></strong> to <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/ncvawp6t' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/ncvawp6t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/ncvawp6t' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/ncvawp6t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/aw54os4k' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/aw54os4k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  4/704 [..............................] - ETA: 16s - loss: 4.7714 - accuracy: 0.0781 - top@3_accuracy: 0.3320  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_train_batch_end` time: 0.0128s). Check your callbacks.\n",
      "704/704 [==============================] - 19s 22ms/step - loss: 1.8350 - accuracy: 0.3769 - top@3_accuracy: 0.7144 - val_loss: 1.7117 - val_accuracy: 0.3618 - val_top@3_accuracy: 0.7016\n",
      "Epoch 2/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.3972 - accuracy: 0.5050 - top@3_accuracy: 0.8169 - val_loss: 1.5593 - val_accuracy: 0.4516 - val_top@3_accuracy: 0.7562\n",
      "Epoch 3/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.2501 - accuracy: 0.5613 - top@3_accuracy: 0.8484 - val_loss: 1.3611 - val_accuracy: 0.5016 - val_top@3_accuracy: 0.8292\n",
      "Epoch 4/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.1358 - accuracy: 0.6039 - top@3_accuracy: 0.8706 - val_loss: 1.3198 - val_accuracy: 0.5336 - val_top@3_accuracy: 0.8110\n",
      "Epoch 5/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.0408 - accuracy: 0.6335 - top@3_accuracy: 0.8866 - val_loss: 1.2255 - val_accuracy: 0.5776 - val_top@3_accuracy: 0.8548\n",
      "Epoch 6/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.9562 - accuracy: 0.6648 - top@3_accuracy: 0.9041 - val_loss: 1.0858 - val_accuracy: 0.6210 - val_top@3_accuracy: 0.8830\n",
      "Epoch 7/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.8684 - accuracy: 0.6957 - top@3_accuracy: 0.9196 - val_loss: 1.1681 - val_accuracy: 0.6008 - val_top@3_accuracy: 0.8702\n",
      "Epoch 8/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.7985 - accuracy: 0.7245 - top@3_accuracy: 0.9289 - val_loss: 1.0240 - val_accuracy: 0.6488 - val_top@3_accuracy: 0.8940\n",
      "Epoch 9/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.7333 - accuracy: 0.7436 - top@3_accuracy: 0.9390 - val_loss: 1.0633 - val_accuracy: 0.6400 - val_top@3_accuracy: 0.8832\n",
      "Epoch 10/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.6522 - accuracy: 0.7710 - top@3_accuracy: 0.9490 - val_loss: 1.0145 - val_accuracy: 0.6440 - val_top@3_accuracy: 0.8940\n",
      "Epoch 11/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.5842 - accuracy: 0.7954 - top@3_accuracy: 0.9578 - val_loss: 1.0939 - val_accuracy: 0.6298 - val_top@3_accuracy: 0.8692\n",
      "Epoch 12/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.5237 - accuracy: 0.8176 - top@3_accuracy: 0.9648 - val_loss: 0.9909 - val_accuracy: 0.6594 - val_top@3_accuracy: 0.9000\n",
      "Epoch 13/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.4625 - accuracy: 0.8369 - top@3_accuracy: 0.9720 - val_loss: 1.0170 - val_accuracy: 0.6706 - val_top@3_accuracy: 0.8972\n",
      "Epoch 14/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.4055 - accuracy: 0.8582 - top@3_accuracy: 0.9780 - val_loss: 1.0940 - val_accuracy: 0.6666 - val_top@3_accuracy: 0.9018\n",
      "Epoch 15/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.3594 - accuracy: 0.8730 - top@3_accuracy: 0.9827 - val_loss: 1.1115 - val_accuracy: 0.6596 - val_top@3_accuracy: 0.8962\n",
      "Epoch 16/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.3061 - accuracy: 0.8937 - top@3_accuracy: 0.9874 - val_loss: 1.1429 - val_accuracy: 0.6676 - val_top@3_accuracy: 0.8946\n",
      "Epoch 17/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.2781 - accuracy: 0.9035 - top@3_accuracy: 0.9890 - val_loss: 1.1986 - val_accuracy: 0.6608 - val_top@3_accuracy: 0.8972\n",
      "Epoch 18/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.2443 - accuracy: 0.9156 - top@3_accuracy: 0.9916 - val_loss: 1.4010 - val_accuracy: 0.6258 - val_top@3_accuracy: 0.8798\n",
      "Epoch 19/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.2149 - accuracy: 0.9262 - top@3_accuracy: 0.9935 - val_loss: 1.4852 - val_accuracy: 0.6158 - val_top@3_accuracy: 0.8726\n",
      "Epoch 20/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.2003 - accuracy: 0.9312 - top@3_accuracy: 0.9945 - val_loss: 1.3580 - val_accuracy: 0.6590 - val_top@3_accuracy: 0.8914\n",
      "Epoch 21/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.1816 - accuracy: 0.9384 - top@3_accuracy: 0.9953 - val_loss: 1.2752 - val_accuracy: 0.6898 - val_top@3_accuracy: 0.9074\n",
      "Epoch 22/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.1736 - accuracy: 0.9421 - top@3_accuracy: 0.9958 - val_loss: 1.2935 - val_accuracy: 0.6506 - val_top@3_accuracy: 0.8938\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 1.0303 - accuracy: 0.6465 - top@3_accuracy: 0.8905\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06acf4be982747428789cdbecac12fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁▃▃▄▄▅▅▅▆▆▆▆▇▇▇▇██████</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▆▆▅▅▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>epoch/top@3_accuracy</td><td>▁▄▄▅▅▆▆▆▇▇▇▇▇█████████</td></tr><tr><td>epoch/val_accuracy</td><td>▁▃▄▅▆▇▆▇▇▇▇▇██▇█▇▇▆▇█▇</td></tr><tr><td>epoch/val_loss</td><td>█▇▅▄▃▂▃▁▂▁▂▁▁▂▂▂▃▅▆▅▄▄</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>▁▃▅▅▆▇▇█▇█▇██████▇▇▇██</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.6594</td></tr><tr><td>best_val_loss</td><td>0.99094</td></tr><tr><td>epoch/accuracy</td><td>0.94209</td></tr><tr><td>epoch/epoch</td><td>21</td></tr><tr><td>epoch/learning_rate</td><td>6e-05</td></tr><tr><td>epoch/loss</td><td>0.17357</td></tr><tr><td>epoch/top@3_accuracy</td><td>0.99578</td></tr><tr><td>epoch/val_accuracy</td><td>0.6506</td></tr><tr><td>epoch/val_loss</td><td>1.29352</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>0.8938</td></tr><tr><td>test_acc</td><td>0.6465</td></tr><tr><td>test_loss</td><td>1.03025</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unique-sweep-1</strong> at: <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/aw54os4k' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/aw54os4k</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_131836-aw54os4k\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cdmnvfvn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation: light\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearlystopping_patience: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 6.3e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnormalization: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tregularization: False\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\NEURAL PROJE\\wandb\\run-20240103_132426-cdmnvfvn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/cdmnvfvn' target=\"_blank\">drawn-sweep-2</a></strong> to <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/ncvawp6t' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/ncvawp6t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/ncvawp6t' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/ncvawp6t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/cdmnvfvn' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/cdmnvfvn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  4/704 [..............................] - ETA: 15s - loss: 4.1307 - accuracy: 0.1250 - top@3_accuracy: 0.3828 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0070s vs `on_train_batch_end` time: 0.0327s). Check your callbacks.\n",
      "704/704 [==============================] - 16s 22ms/step - loss: 1.9306 - accuracy: 0.3406 - top@3_accuracy: 0.6843 - val_loss: 1.6391 - val_accuracy: 0.3936 - val_top@3_accuracy: 0.7406\n",
      "Epoch 2/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.5383 - accuracy: 0.4528 - top@3_accuracy: 0.7771 - val_loss: 1.4548 - val_accuracy: 0.4810 - val_top@3_accuracy: 0.7900\n",
      "Epoch 3/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.4214 - accuracy: 0.4943 - top@3_accuracy: 0.8106 - val_loss: 1.3322 - val_accuracy: 0.5210 - val_top@3_accuracy: 0.8292\n",
      "Epoch 4/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.3379 - accuracy: 0.5272 - top@3_accuracy: 0.8294 - val_loss: 1.5363 - val_accuracy: 0.4784 - val_top@3_accuracy: 0.7468\n",
      "Epoch 5/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.2773 - accuracy: 0.5507 - top@3_accuracy: 0.8421 - val_loss: 1.3744 - val_accuracy: 0.4994 - val_top@3_accuracy: 0.8324\n",
      "Epoch 6/100\n",
      "704/704 [==============================] - 15s 22ms/step - loss: 1.2227 - accuracy: 0.5709 - top@3_accuracy: 0.8544 - val_loss: 1.2292 - val_accuracy: 0.5594 - val_top@3_accuracy: 0.8472\n",
      "Epoch 7/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.1671 - accuracy: 0.5935 - top@3_accuracy: 0.8673 - val_loss: 1.2118 - val_accuracy: 0.5716 - val_top@3_accuracy: 0.8594\n",
      "Epoch 8/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.1234 - accuracy: 0.6100 - top@3_accuracy: 0.8751 - val_loss: 1.1420 - val_accuracy: 0.6018 - val_top@3_accuracy: 0.8828\n",
      "Epoch 9/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.0829 - accuracy: 0.6234 - top@3_accuracy: 0.8832 - val_loss: 1.1672 - val_accuracy: 0.5930 - val_top@3_accuracy: 0.8780\n",
      "Epoch 10/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.0493 - accuracy: 0.6361 - top@3_accuracy: 0.8880 - val_loss: 1.1187 - val_accuracy: 0.6184 - val_top@3_accuracy: 0.8732\n",
      "Epoch 11/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.0055 - accuracy: 0.6519 - top@3_accuracy: 0.8970 - val_loss: 1.1783 - val_accuracy: 0.6064 - val_top@3_accuracy: 0.8678\n",
      "Epoch 12/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.9729 - accuracy: 0.6624 - top@3_accuracy: 0.9018 - val_loss: 1.1097 - val_accuracy: 0.6170 - val_top@3_accuracy: 0.8652\n",
      "Epoch 13/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.9420 - accuracy: 0.6755 - top@3_accuracy: 0.9067 - val_loss: 1.1239 - val_accuracy: 0.6264 - val_top@3_accuracy: 0.8638\n",
      "Epoch 14/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.9119 - accuracy: 0.6853 - top@3_accuracy: 0.9123 - val_loss: 1.0897 - val_accuracy: 0.6358 - val_top@3_accuracy: 0.8826\n",
      "Epoch 15/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.8898 - accuracy: 0.6928 - top@3_accuracy: 0.9163 - val_loss: 1.0805 - val_accuracy: 0.6158 - val_top@3_accuracy: 0.8996\n",
      "Epoch 16/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.8692 - accuracy: 0.7022 - top@3_accuracy: 0.9189 - val_loss: 1.0242 - val_accuracy: 0.6598 - val_top@3_accuracy: 0.8946\n",
      "Epoch 17/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.8429 - accuracy: 0.7066 - top@3_accuracy: 0.9230 - val_loss: 1.0545 - val_accuracy: 0.6444 - val_top@3_accuracy: 0.8996\n",
      "Epoch 18/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.8135 - accuracy: 0.7202 - top@3_accuracy: 0.9283 - val_loss: 1.0688 - val_accuracy: 0.6312 - val_top@3_accuracy: 0.8636\n",
      "Epoch 19/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.7956 - accuracy: 0.7239 - top@3_accuracy: 0.9302 - val_loss: 0.9650 - val_accuracy: 0.6714 - val_top@3_accuracy: 0.9108\n",
      "Epoch 20/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.7792 - accuracy: 0.7322 - top@3_accuracy: 0.9337 - val_loss: 0.9382 - val_accuracy: 0.6866 - val_top@3_accuracy: 0.8954\n",
      "Epoch 21/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.7544 - accuracy: 0.7395 - top@3_accuracy: 0.9369 - val_loss: 0.9097 - val_accuracy: 0.6876 - val_top@3_accuracy: 0.9160\n",
      "Epoch 22/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.7373 - accuracy: 0.7455 - top@3_accuracy: 0.9385 - val_loss: 0.8972 - val_accuracy: 0.6936 - val_top@3_accuracy: 0.9156\n",
      "Epoch 23/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.7184 - accuracy: 0.7536 - top@3_accuracy: 0.9406 - val_loss: 0.8267 - val_accuracy: 0.7186 - val_top@3_accuracy: 0.9226\n",
      "Epoch 24/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.7014 - accuracy: 0.7564 - top@3_accuracy: 0.9442 - val_loss: 0.8917 - val_accuracy: 0.6966 - val_top@3_accuracy: 0.9136\n",
      "Epoch 25/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.6813 - accuracy: 0.7647 - top@3_accuracy: 0.9454 - val_loss: 0.9503 - val_accuracy: 0.6612 - val_top@3_accuracy: 0.9118\n",
      "Epoch 26/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.6757 - accuracy: 0.7658 - top@3_accuracy: 0.9468 - val_loss: 0.7999 - val_accuracy: 0.7226 - val_top@3_accuracy: 0.9242\n",
      "Epoch 27/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.6607 - accuracy: 0.7725 - top@3_accuracy: 0.9491 - val_loss: 0.9884 - val_accuracy: 0.6642 - val_top@3_accuracy: 0.8926\n",
      "Epoch 28/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.6476 - accuracy: 0.7764 - top@3_accuracy: 0.9490 - val_loss: 0.7384 - val_accuracy: 0.7554 - val_top@3_accuracy: 0.9402\n",
      "Epoch 29/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.6316 - accuracy: 0.7817 - top@3_accuracy: 0.9531 - val_loss: 0.7891 - val_accuracy: 0.7254 - val_top@3_accuracy: 0.9294\n",
      "Epoch 30/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.6178 - accuracy: 0.7880 - top@3_accuracy: 0.9544 - val_loss: 0.8896 - val_accuracy: 0.6988 - val_top@3_accuracy: 0.9140\n",
      "Epoch 31/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.5981 - accuracy: 0.7921 - top@3_accuracy: 0.9563 - val_loss: 0.9931 - val_accuracy: 0.6652 - val_top@3_accuracy: 0.8892\n",
      "Epoch 32/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.5817 - accuracy: 0.7996 - top@3_accuracy: 0.9597 - val_loss: 0.7547 - val_accuracy: 0.7430 - val_top@3_accuracy: 0.9368\n",
      "Epoch 33/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.5753 - accuracy: 0.8018 - top@3_accuracy: 0.9602 - val_loss: 0.9285 - val_accuracy: 0.6920 - val_top@3_accuracy: 0.8982\n",
      "Epoch 34/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.5766 - accuracy: 0.8013 - top@3_accuracy: 0.9585 - val_loss: 0.9036 - val_accuracy: 0.6828 - val_top@3_accuracy: 0.9102\n",
      "Epoch 35/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.5463 - accuracy: 0.8094 - top@3_accuracy: 0.9633 - val_loss: 0.8382 - val_accuracy: 0.7166 - val_top@3_accuracy: 0.9166\n",
      "Epoch 36/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.5406 - accuracy: 0.8128 - top@3_accuracy: 0.9636 - val_loss: 0.7767 - val_accuracy: 0.7358 - val_top@3_accuracy: 0.9294\n",
      "Epoch 37/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.5268 - accuracy: 0.8187 - top@3_accuracy: 0.9673 - val_loss: 0.7563 - val_accuracy: 0.7464 - val_top@3_accuracy: 0.9372\n",
      "Epoch 38/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.5205 - accuracy: 0.8191 - top@3_accuracy: 0.9663 - val_loss: 0.7183 - val_accuracy: 0.7572 - val_top@3_accuracy: 0.9404\n",
      "Epoch 39/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.5091 - accuracy: 0.8222 - top@3_accuracy: 0.9685 - val_loss: 0.7237 - val_accuracy: 0.7576 - val_top@3_accuracy: 0.9360\n",
      "Epoch 40/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.4978 - accuracy: 0.8272 - top@3_accuracy: 0.9697 - val_loss: 0.8153 - val_accuracy: 0.7164 - val_top@3_accuracy: 0.9298\n",
      "Epoch 41/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.4875 - accuracy: 0.8306 - top@3_accuracy: 0.9704 - val_loss: 0.7310 - val_accuracy: 0.7564 - val_top@3_accuracy: 0.9384\n",
      "Epoch 42/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.4749 - accuracy: 0.8375 - top@3_accuracy: 0.9713 - val_loss: 0.7843 - val_accuracy: 0.7504 - val_top@3_accuracy: 0.9248\n",
      "Epoch 43/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.4671 - accuracy: 0.8376 - top@3_accuracy: 0.9725 - val_loss: 0.9318 - val_accuracy: 0.7066 - val_top@3_accuracy: 0.8992\n",
      "Epoch 44/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.4586 - accuracy: 0.8410 - top@3_accuracy: 0.9742 - val_loss: 0.6979 - val_accuracy: 0.7744 - val_top@3_accuracy: 0.9426\n",
      "Epoch 45/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.4467 - accuracy: 0.8452 - top@3_accuracy: 0.9748 - val_loss: 0.7085 - val_accuracy: 0.7686 - val_top@3_accuracy: 0.9370\n",
      "Epoch 46/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.4383 - accuracy: 0.8502 - top@3_accuracy: 0.9745 - val_loss: 0.7755 - val_accuracy: 0.7428 - val_top@3_accuracy: 0.9318\n",
      "Epoch 47/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.4260 - accuracy: 0.8525 - top@3_accuracy: 0.9763 - val_loss: 0.8698 - val_accuracy: 0.7222 - val_top@3_accuracy: 0.9202\n",
      "Epoch 48/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.4285 - accuracy: 0.8514 - top@3_accuracy: 0.9771 - val_loss: 0.7441 - val_accuracy: 0.7514 - val_top@3_accuracy: 0.9370\n",
      "Epoch 49/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.4089 - accuracy: 0.8587 - top@3_accuracy: 0.9779 - val_loss: 0.7231 - val_accuracy: 0.7638 - val_top@3_accuracy: 0.9360\n",
      "Epoch 50/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.4072 - accuracy: 0.8600 - top@3_accuracy: 0.9777 - val_loss: 0.7188 - val_accuracy: 0.7686 - val_top@3_accuracy: 0.9374\n",
      "Epoch 51/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.4010 - accuracy: 0.8623 - top@3_accuracy: 0.9797 - val_loss: 0.6720 - val_accuracy: 0.7782 - val_top@3_accuracy: 0.9436\n",
      "Epoch 52/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.3958 - accuracy: 0.8629 - top@3_accuracy: 0.9792 - val_loss: 0.7203 - val_accuracy: 0.7626 - val_top@3_accuracy: 0.9414\n",
      "Epoch 53/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.3830 - accuracy: 0.8670 - top@3_accuracy: 0.9809 - val_loss: 0.7338 - val_accuracy: 0.7714 - val_top@3_accuracy: 0.9394\n",
      "Epoch 54/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.3802 - accuracy: 0.8688 - top@3_accuracy: 0.9816 - val_loss: 0.7397 - val_accuracy: 0.7622 - val_top@3_accuracy: 0.9334\n",
      "Epoch 55/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.3823 - accuracy: 0.8666 - top@3_accuracy: 0.9810 - val_loss: 0.9313 - val_accuracy: 0.7180 - val_top@3_accuracy: 0.9066\n",
      "Epoch 56/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.3653 - accuracy: 0.8747 - top@3_accuracy: 0.9825 - val_loss: 0.7843 - val_accuracy: 0.7476 - val_top@3_accuracy: 0.9362\n",
      "Epoch 57/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.3635 - accuracy: 0.8750 - top@3_accuracy: 0.9830 - val_loss: 0.7871 - val_accuracy: 0.7518 - val_top@3_accuracy: 0.9322\n",
      "Epoch 58/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.3552 - accuracy: 0.8775 - top@3_accuracy: 0.9832 - val_loss: 0.7209 - val_accuracy: 0.7628 - val_top@3_accuracy: 0.9442\n",
      "Epoch 59/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.3421 - accuracy: 0.8815 - top@3_accuracy: 0.9844 - val_loss: 0.7732 - val_accuracy: 0.7506 - val_top@3_accuracy: 0.9366\n",
      "Epoch 60/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.3411 - accuracy: 0.8826 - top@3_accuracy: 0.9852 - val_loss: 0.8164 - val_accuracy: 0.7452 - val_top@3_accuracy: 0.9286\n",
      "Epoch 61/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.3480 - accuracy: 0.8802 - top@3_accuracy: 0.9839 - val_loss: 0.7273 - val_accuracy: 0.7736 - val_top@3_accuracy: 0.9448\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.6947 - accuracy: 0.7685 - top@3_accuracy: 0.9415\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "787548d9724b472d8528f6661a4f1a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.031 MB uploaded\\r'), FloatProgress(value=0.0347799511002445, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁▂▃▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇███████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/top@3_accuracy</td><td>▁▃▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>epoch/val_accuracy</td><td>▁▃▃▃▄▅▅▅▅▅▆▆▆▆▆▇▆▆█▇▆▆▆▇▇██▇██▇█████▇█▇█</td></tr><tr><td>epoch/val_loss</td><td>█▇▇▆▅▄▄▅▄▄▄▄▃▃▃▃▃▃▁▃▃▃▃▂▂▁▁▂▁▁▂▂▁▁▁▁▂▂▂▁</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>▁▃▁▄▅▆▆▅▅▆▆▆▇▇▇▇▇▆█▇▆▆▇▇███▇██▇█████████</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.7782</td></tr><tr><td>best_val_loss</td><td>0.67197</td></tr><tr><td>epoch/accuracy</td><td>0.8802</td></tr><tr><td>epoch/epoch</td><td>60</td></tr><tr><td>epoch/learning_rate</td><td>6e-05</td></tr><tr><td>epoch/loss</td><td>0.34802</td></tr><tr><td>epoch/top@3_accuracy</td><td>0.98393</td></tr><tr><td>epoch/val_accuracy</td><td>0.7736</td></tr><tr><td>epoch/val_loss</td><td>0.72727</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>0.9448</td></tr><tr><td>test_acc</td><td>0.7685</td></tr><tr><td>test_loss</td><td>0.69469</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">drawn-sweep-2</strong> at: <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/cdmnvfvn' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/cdmnvfvn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_132426-cdmnvfvn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gbumg0q4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation: heavy\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearlystopping_patience: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 6.3e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnormalization: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tregularization: False\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\NEURAL PROJE\\wandb\\run-20240103_134006-gbumg0q4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/gbumg0q4' target=\"_blank\">unique-sweep-3</a></strong> to <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/ncvawp6t' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/ncvawp6t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/takim/CIFAR-10_Classification' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/takim/CIFAR-10_Classification/sweeps/ncvawp6t' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/sweeps/ncvawp6t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/gbumg0q4' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/gbumg0q4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  4/704 [..............................] - ETA: 14s - loss: 4.3718 - accuracy: 0.1172 - top@3_accuracy: 0.3008 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0062s vs `on_train_batch_end` time: 0.0408s). Check your callbacks.\n",
      "704/704 [==============================] - 16s 21ms/step - loss: 2.1008 - accuracy: 0.2754 - top@3_accuracy: 0.6084 - val_loss: 1.7293 - val_accuracy: 0.3798 - val_top@3_accuracy: 0.7308\n",
      "Epoch 2/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.7687 - accuracy: 0.3570 - top@3_accuracy: 0.7080 - val_loss: 1.6267 - val_accuracy: 0.4032 - val_top@3_accuracy: 0.7444\n",
      "Epoch 3/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.6625 - accuracy: 0.3972 - top@3_accuracy: 0.7422 - val_loss: 1.6857 - val_accuracy: 0.4018 - val_top@3_accuracy: 0.6948\n",
      "Epoch 4/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.5959 - accuracy: 0.4286 - top@3_accuracy: 0.7628 - val_loss: 1.8187 - val_accuracy: 0.3504 - val_top@3_accuracy: 0.6800\n",
      "Epoch 5/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.5370 - accuracy: 0.4535 - top@3_accuracy: 0.7782 - val_loss: 1.4558 - val_accuracy: 0.4782 - val_top@3_accuracy: 0.8022\n",
      "Epoch 6/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.4886 - accuracy: 0.4670 - top@3_accuracy: 0.7917 - val_loss: 1.9108 - val_accuracy: 0.3436 - val_top@3_accuracy: 0.6644\n",
      "Epoch 7/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.4385 - accuracy: 0.4888 - top@3_accuracy: 0.8038 - val_loss: 1.6627 - val_accuracy: 0.4374 - val_top@3_accuracy: 0.7144\n",
      "Epoch 8/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.4024 - accuracy: 0.5010 - top@3_accuracy: 0.8131 - val_loss: 1.3950 - val_accuracy: 0.4880 - val_top@3_accuracy: 0.8264\n",
      "Epoch 9/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.3671 - accuracy: 0.5153 - top@3_accuracy: 0.8240 - val_loss: 1.3678 - val_accuracy: 0.5126 - val_top@3_accuracy: 0.8300\n",
      "Epoch 10/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.3319 - accuracy: 0.5271 - top@3_accuracy: 0.8309 - val_loss: 1.3044 - val_accuracy: 0.5646 - val_top@3_accuracy: 0.8498\n",
      "Epoch 11/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.3101 - accuracy: 0.5404 - top@3_accuracy: 0.8362 - val_loss: 1.2697 - val_accuracy: 0.5562 - val_top@3_accuracy: 0.8534\n",
      "Epoch 12/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.2717 - accuracy: 0.5524 - top@3_accuracy: 0.8447 - val_loss: 1.2644 - val_accuracy: 0.6012 - val_top@3_accuracy: 0.8674\n",
      "Epoch 13/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.2477 - accuracy: 0.5612 - top@3_accuracy: 0.8508 - val_loss: 1.3283 - val_accuracy: 0.5154 - val_top@3_accuracy: 0.8302\n",
      "Epoch 14/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.2242 - accuracy: 0.5715 - top@3_accuracy: 0.8548 - val_loss: 1.3004 - val_accuracy: 0.5640 - val_top@3_accuracy: 0.8298\n",
      "Epoch 15/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.1939 - accuracy: 0.5825 - top@3_accuracy: 0.8603 - val_loss: 1.2056 - val_accuracy: 0.5914 - val_top@3_accuracy: 0.8738\n",
      "Epoch 16/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.1732 - accuracy: 0.5907 - top@3_accuracy: 0.8634 - val_loss: 1.2199 - val_accuracy: 0.6058 - val_top@3_accuracy: 0.8664\n",
      "Epoch 17/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.1570 - accuracy: 0.5991 - top@3_accuracy: 0.8683 - val_loss: 1.2224 - val_accuracy: 0.5664 - val_top@3_accuracy: 0.8590\n",
      "Epoch 18/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.1388 - accuracy: 0.6038 - top@3_accuracy: 0.8678 - val_loss: 1.2646 - val_accuracy: 0.5656 - val_top@3_accuracy: 0.8534\n",
      "Epoch 19/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.1184 - accuracy: 0.6128 - top@3_accuracy: 0.8752 - val_loss: 1.1601 - val_accuracy: 0.6246 - val_top@3_accuracy: 0.8744\n",
      "Epoch 20/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.0986 - accuracy: 0.6180 - top@3_accuracy: 0.8781 - val_loss: 1.1545 - val_accuracy: 0.6248 - val_top@3_accuracy: 0.8912\n",
      "Epoch 21/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.0776 - accuracy: 0.6309 - top@3_accuracy: 0.8834 - val_loss: 1.1528 - val_accuracy: 0.6100 - val_top@3_accuracy: 0.8698\n",
      "Epoch 22/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.0649 - accuracy: 0.6311 - top@3_accuracy: 0.8837 - val_loss: 1.1268 - val_accuracy: 0.6110 - val_top@3_accuracy: 0.8690\n",
      "Epoch 23/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.0389 - accuracy: 0.6395 - top@3_accuracy: 0.8899 - val_loss: 1.1228 - val_accuracy: 0.6224 - val_top@3_accuracy: 0.8698\n",
      "Epoch 24/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.0279 - accuracy: 0.6457 - top@3_accuracy: 0.8905 - val_loss: 1.0412 - val_accuracy: 0.6372 - val_top@3_accuracy: 0.8926\n",
      "Epoch 25/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 1.0157 - accuracy: 0.6485 - top@3_accuracy: 0.8948 - val_loss: 1.0141 - val_accuracy: 0.6616 - val_top@3_accuracy: 0.9062\n",
      "Epoch 26/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.9984 - accuracy: 0.6518 - top@3_accuracy: 0.8978 - val_loss: 1.0505 - val_accuracy: 0.6466 - val_top@3_accuracy: 0.8908\n",
      "Epoch 27/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.9856 - accuracy: 0.6577 - top@3_accuracy: 0.8996 - val_loss: 1.0825 - val_accuracy: 0.6266 - val_top@3_accuracy: 0.8894\n",
      "Epoch 28/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.9792 - accuracy: 0.6607 - top@3_accuracy: 0.9014 - val_loss: 1.1305 - val_accuracy: 0.6168 - val_top@3_accuracy: 0.8658\n",
      "Epoch 29/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.9593 - accuracy: 0.6680 - top@3_accuracy: 0.9028 - val_loss: 1.0309 - val_accuracy: 0.6506 - val_top@3_accuracy: 0.9052\n",
      "Epoch 30/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.9475 - accuracy: 0.6739 - top@3_accuracy: 0.9054 - val_loss: 0.9657 - val_accuracy: 0.6818 - val_top@3_accuracy: 0.9120\n",
      "Epoch 31/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.9433 - accuracy: 0.6755 - top@3_accuracy: 0.9069 - val_loss: 0.9414 - val_accuracy: 0.6822 - val_top@3_accuracy: 0.9210\n",
      "Epoch 32/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.9247 - accuracy: 0.6805 - top@3_accuracy: 0.9075 - val_loss: 0.9159 - val_accuracy: 0.6936 - val_top@3_accuracy: 0.9184\n",
      "Epoch 33/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.9216 - accuracy: 0.6807 - top@3_accuracy: 0.9104 - val_loss: 1.0089 - val_accuracy: 0.6602 - val_top@3_accuracy: 0.8998\n",
      "Epoch 34/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.9116 - accuracy: 0.6859 - top@3_accuracy: 0.9104 - val_loss: 0.9409 - val_accuracy: 0.6802 - val_top@3_accuracy: 0.9194\n",
      "Epoch 35/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.8994 - accuracy: 0.6881 - top@3_accuracy: 0.9139 - val_loss: 1.1073 - val_accuracy: 0.6206 - val_top@3_accuracy: 0.8782\n",
      "Epoch 36/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.8872 - accuracy: 0.6934 - top@3_accuracy: 0.9157 - val_loss: 1.0435 - val_accuracy: 0.6336 - val_top@3_accuracy: 0.8864\n",
      "Epoch 37/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.8738 - accuracy: 0.6997 - top@3_accuracy: 0.9173 - val_loss: 0.9303 - val_accuracy: 0.6740 - val_top@3_accuracy: 0.9138\n",
      "Epoch 38/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.8681 - accuracy: 0.6993 - top@3_accuracy: 0.9195 - val_loss: 0.9334 - val_accuracy: 0.6752 - val_top@3_accuracy: 0.9166\n",
      "Epoch 39/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.8527 - accuracy: 0.7068 - top@3_accuracy: 0.9212 - val_loss: 0.8724 - val_accuracy: 0.7092 - val_top@3_accuracy: 0.9186\n",
      "Epoch 40/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.8448 - accuracy: 0.7073 - top@3_accuracy: 0.9223 - val_loss: 1.0214 - val_accuracy: 0.6562 - val_top@3_accuracy: 0.8886\n",
      "Epoch 41/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.8355 - accuracy: 0.7097 - top@3_accuracy: 0.9230 - val_loss: 0.8609 - val_accuracy: 0.7022 - val_top@3_accuracy: 0.9242\n",
      "Epoch 42/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.8347 - accuracy: 0.7111 - top@3_accuracy: 0.9251 - val_loss: 0.9181 - val_accuracy: 0.6820 - val_top@3_accuracy: 0.9196\n",
      "Epoch 43/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.8185 - accuracy: 0.7185 - top@3_accuracy: 0.9256 - val_loss: 0.8694 - val_accuracy: 0.6998 - val_top@3_accuracy: 0.9230\n",
      "Epoch 44/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.8228 - accuracy: 0.7170 - top@3_accuracy: 0.9261 - val_loss: 0.8659 - val_accuracy: 0.7092 - val_top@3_accuracy: 0.9254\n",
      "Epoch 45/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.8121 - accuracy: 0.7188 - top@3_accuracy: 0.9264 - val_loss: 0.8014 - val_accuracy: 0.7342 - val_top@3_accuracy: 0.9382\n",
      "Epoch 46/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.8011 - accuracy: 0.7226 - top@3_accuracy: 0.9283 - val_loss: 0.8365 - val_accuracy: 0.7258 - val_top@3_accuracy: 0.9294\n",
      "Epoch 47/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.8015 - accuracy: 0.7240 - top@3_accuracy: 0.9281 - val_loss: 0.8797 - val_accuracy: 0.7034 - val_top@3_accuracy: 0.9252\n",
      "Epoch 48/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.7822 - accuracy: 0.7271 - top@3_accuracy: 0.9316 - val_loss: 0.8463 - val_accuracy: 0.7208 - val_top@3_accuracy: 0.9248\n",
      "Epoch 49/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.7812 - accuracy: 0.7312 - top@3_accuracy: 0.9311 - val_loss: 0.8920 - val_accuracy: 0.6948 - val_top@3_accuracy: 0.9156\n",
      "Epoch 50/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.7756 - accuracy: 0.7328 - top@3_accuracy: 0.9337 - val_loss: 0.8061 - val_accuracy: 0.7238 - val_top@3_accuracy: 0.9302\n",
      "Epoch 51/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.7623 - accuracy: 0.7363 - top@3_accuracy: 0.9348 - val_loss: 0.9137 - val_accuracy: 0.6818 - val_top@3_accuracy: 0.9100\n",
      "Epoch 52/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.7545 - accuracy: 0.7390 - top@3_accuracy: 0.9364 - val_loss: 0.8259 - val_accuracy: 0.7194 - val_top@3_accuracy: 0.9254\n",
      "Epoch 53/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.7500 - accuracy: 0.7420 - top@3_accuracy: 0.9370 - val_loss: 0.8882 - val_accuracy: 0.7056 - val_top@3_accuracy: 0.9130\n",
      "Epoch 54/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.7501 - accuracy: 0.7415 - top@3_accuracy: 0.9365 - val_loss: 0.8184 - val_accuracy: 0.7254 - val_top@3_accuracy: 0.9266\n",
      "Epoch 55/100\n",
      "704/704 [==============================] - 15s 21ms/step - loss: 0.7355 - accuracy: 0.7454 - top@3_accuracy: 0.9384 - val_loss: 0.8170 - val_accuracy: 0.7194 - val_top@3_accuracy: 0.9314\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.8144 - accuracy: 0.7294 - top@3_accuracy: 0.9316\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e707a664df054d2a9ef683f1a52f1608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▁▂▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/top@3_accuracy</td><td>▁▃▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>epoch/val_accuracy</td><td>▂▂▂▃▁▃▄▅▆▄▅▆▅▆▆▆▆▆▇▆▆▇▇▇▇▆▇▇█▇▇▇███▇██▇█</td></tr><tr><td>epoch/val_loss</td><td>▇▆▇▅█▆▅▄▄▄▄▄▄▃▃▃▃▃▂▃▃▂▂▂▂▃▂▂▁▁▂▁▁▁▁▂▁▁▂▁</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>▃▃▂▅▁▂▅▆▆▅▅▆▆▆▇▆▆▇▇▇▆▇█▇█▆▇▇▇██████▇██▇█</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.7342</td></tr><tr><td>best_val_loss</td><td>0.80138</td></tr><tr><td>epoch/accuracy</td><td>0.7454</td></tr><tr><td>epoch/epoch</td><td>54</td></tr><tr><td>epoch/learning_rate</td><td>6e-05</td></tr><tr><td>epoch/loss</td><td>0.73548</td></tr><tr><td>epoch/top@3_accuracy</td><td>0.93842</td></tr><tr><td>epoch/val_accuracy</td><td>0.7194</td></tr><tr><td>epoch/val_loss</td><td>0.81703</td></tr><tr><td>epoch/val_top@3_accuracy</td><td>0.9314</td></tr><tr><td>test_acc</td><td>0.7294</td></tr><tr><td>test_loss</td><td>0.81442</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unique-sweep-3</strong> at: <a href='https://wandb.ai/takim/CIFAR-10_Classification/runs/gbumg0q4' target=\"_blank\">https://wandb.ai/takim/CIFAR-10_Classification/runs/gbumg0q4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_134006-gbumg0q4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
